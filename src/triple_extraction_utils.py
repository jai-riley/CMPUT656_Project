from sklearn.metrics import  precision_score, recall_score, f1_score
from langchain_google_genai import ChatGoogleGenerativeAI
import os
from openai import OpenAI
import copy
import requests
import re


API_BASE_URL = "https://api.cloudflare.com/client/v4/accounts/375121881c6ec963b21944159717e15e/ai/run/"


def gpt_get_completion(prompt, gpt_llm, model="gpt-3.5-turbo-0125"):
    """
    Generate completions using the OpenAI GPT language model.

    Parameters:
        prompt (str): The input prompt for the model.
        gpt_llm: An instance of the OpenAI API with the appropriate API key.
        model (str): The name of the GPT model to use for completion.
                     Default is "gpt-3.5-turbo-0125".

    Returns:
        str: The completion generated by the model.
    """
    completion = gpt_llm.chat.completions.create(
    model= model,
      messages=[
        {"role": "system", "content": "Act as a perfect triple extractor model"},
        {"role": "user", "content": prompt}
      ]
    )
    return(completion.choices[0].message.content)


def llama_prompt_run(model, prompt, headers):
  """
    Send a prompt to the LLaMA  API for processing.

    Parameters:
        model (str): The model endpoint to send the prompt to.
        prompt (str): The input prompt for the model.
        headers: HTTP headers required for authorization.

    Returns:
        dict: A JSON response containing the output generated by the model.
  """
  input = {
    "messages": [
      { "role": "system", "content": "You are an expert at extracting event from given sentences for event extraction" },
      { "role": "user", "content": prompt }
    ]
  }
  response = requests.post(f"{API_BASE_URL}{model}", headers=headers, json=input)
  return response.json()


def relation_list_extractor(dataset):
    list_of_relation = []
    for item in dataset:
        list_of_relation.append(item['relation'])
    list_of_relation = list(set(list_of_relation))
    return list_of_relation
        
def extract_triples(text):
    triples = []
    pattern = r'\[([^\[\]]+)\]'
    matches = re.findall(pattern, text)
    for match in matches:
        triple = [elem.strip().strip("'") for elem in match.split(',')]
        triples.append(triple)
    return triples


def TE_prompt_runner(prompt_base, dataset, list_of_relations, Num_samples=20, CoT = False, model=None, API = None, sub_model = 'gpt-3.5-turbo-0125'):
    """
    Run the prompt-based event extraction process.

    Parameters:
        prompt_base (str): The base prompt for event extraction.
        dataset (list): A list of dictionaries representing the dataset containing sentences.
        list_of_events (list): A list of event types to be extracted.
        Num_samples (int): The number of samples to process. Default is 20.
        CoT (bool): Whether to use "Chain of Thought" reasoning. Default is False.
        model (str): The name of the model to use for event extraction.
        API: The API key required for model access.
        sub_model (str): The specific GPT sub-model to use. Default is 'gpt-3.5-turbo-0125'.

    Returns:
        tuple: A tuple containing the output list and, if CoT is enabled, the reasoning list.
    """
    if model == 'Gemini':
      
      gemini_api_key = API
      os.environ["GOOGLE_API_KEY"] = gemini_api_key
      gemini_llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0)

    elif model == 'GPT':
      gpt_api_key = API
      gpt_llm = OpenAI(api_key=gpt_api_key)

    elif model == 'LLaMA':
      headers = {"Authorization": "Bearer {}".format(API)}


    counter = 0
    output_list = []
    reasoning_list = []
    for idx, sample in enumerate(dataset):
        prompt = prompt_base.format(list_of_relations, sample['tokens'])

        try:
            # answer contain the model response to our prompt
            if model == 'GPT':
                answer = gpt_get_completion(prompt, gpt_llm, model = sub_model )
                
            elif model == 'LLaMA':
                prediction = llama_prompt_run("@hf/thebloke/llama-2-13b-chat-awq", prompt, headers=headers)
                answer = prediction['result']['response']
            else:
                prediction = gemini_llm.invoke(prompt)
                answer = prediction.content

            if CoT == 'CoT': # Collect the entire response of LLM for CoT 
                reasoning_list.append(answer)

            # print(answer)
            
            output_list.append( {"id" : sample["id"], 'Triples': extract_triples(answer)})
            counter += 1
        except Exception as e:
            # Some sentence may be blocked by Gemini for some safty settings
            print('error for sentence: ', sample['tokens'])
        if counter == Num_samples:
            break
    if  CoT == 'CoT':
        return output_list, reasoning_list
    return output_list, None


def TE_output_processing(org_output):
  output = copy.deepcopy(org_output)
  # remove empty list samples
  output = [item for item in output if len(item.keys())>= 2]
  return output

def gold_samples_extractor(dataset):

  gold_samples = {}
  for sample in dataset:
    # gold_sample = {}
    gold_samples[sample['id']] = [sample['h']['name'], sample['relation'],sample['t']['name']]
    # gold_samples.append(gold_sample)
  return gold_samples

def binary_evaluation(gold_samples, processed_output, orderless_relations = []):
  labels = []
  predictions = []

  for item in processed_output:
    gold_sample_triple = gold_samples[item['id']]

    relation = gold_sample_triple[1]
    labels.append(relation)
    flag = 0
    for triple in item['Triples']:
      # print("{} <-> {}\n{} <-> {}\n{} <-> {}\n *** \n".format(triple[0], gold_sample_triple[0], triple[1], gold_sample_triple[1], triple[2], gold_sample_triple[2]))
      if triple[0] == gold_sample_triple[0] and triple[1] == gold_sample_triple[1] and triple[2] == gold_sample_triple[2]:
        flag = 1
      if relation in orderless_relations and triple[2] == gold_sample_triple[0] and triple[1] == gold_sample_triple[1] and triple[0] == gold_sample_triple[2]:
        flag = 1
    if flag ==1:
      predictions.append(relation)
    else:
      predictions.append('wrong')

  return labels, predictions

def nonbinary_evaluation(gold_samples, processed_output, orderless_relations = []):
  labels = []
  predictions = []

  for item in processed_output:
    gold_sample_triple = gold_samples[item['id']]
    relation = gold_sample_triple[1]
    triple_label = []
    predicts_label = []

    for triple in item['Triples']:
      flag = 0
      if triple[0] == gold_sample_triple[0] and triple[1] == gold_sample_triple[1] and triple[2] == gold_sample_triple[2]:
        triple_label.append(relation)
        predicts_label.append(relation)
        flag = 1

      elif relation in orderless_relations and triple[2] == gold_sample_triple[0] and triple[1] == gold_sample_triple[1] and triple[0] == gold_sample_triple[2]:
        triple_label.append(relation)
        predicts_label.append(relation)
        flag = 1

      if flag == 0:
        triple_label.append("unknown")
        predicts_label.append("wrong")

    if len(triple_label) == 0:
      triple_label.append("unknown")
      predicts_label.append("wrong")

    if not relation in triple_label:
      triple_label[0] = relation

    labels.extend(triple_label)
    predictions.extend(predicts_label)

  return labels, predictions

def TE_metric_calculation(labels, predictions, event_labels):

  # when predict an event which is not in gournd truth, we had wrong prediction which decreases our P.
  # when we did not predict any event and there is an even in groundtruth, it deacreses our R.
  # Thats why we put zeor both in ground truth and pred list to handle this inconsitency of number of extracted events.
  micro_p = precision_score(labels,predictions, average='micro')*100.0
  micro_r = recall_score(labels,predictions, labels= event_labels, average='micro')*100.0
  if micro_p + micro_r != 0:
    micro_f1 = (2 * micro_p * micro_r)/(micro_p + micro_r)
  else:
    micro_f1 = 0.0
  # f1_score(labels,predictions, average='micro')*100.0


  print("Micro_F1:", round(micro_f1,2))
  print("Micro_Precision:", round(micro_p,2))
  print("Micro_Recall:", round(micro_r,2))

  return micro_p, micro_r, micro_f1

def print_output(output, dataset, dataset_gold_samples, nb_predictions, cot_reasoning_list = None):
  index = 0
  abstained_counter = 0
  sample_num = 0
  for item in output:
    if len(item['Triples']) == 0:
      abstained_counter += 1
  print("Rate of unstructured or abstained output: {}%".format(abstained_counter/len(output)))
  print()
  for i, item in enumerate(output):
    # sample_num += 1
    if len(item['Triples']) == 0:
      abstained_counter += 1
    for smple in dataset:
      if smple['id'] == item['id']:
        print("Sentence: ", smple['tokens'])

    print("Gold -----> ", dataset_gold_samples[item['id']] )
    print()
    if cot_reasoning_list:
      print("Reasoning ---> ", cot_reasoning_list[i])
      print()
    for pred in item['Triples']:
      print("{:90}".format(str(pred)), end = " ")
      if nb_predictions[index] != "wrong":
        print("*** Correct Prediction ****", nb_predictions[index])
      else:
        print()
      index += 1

    print("\n***\n")

def TE_write_output(output, dataset, dataset_name, dataset_gold_samples, nb_predictions,  prompt_type, target_prompt, resutls, model, cot_reasoning_list=None):
    target_dir = './{}_Experiments_Log/{}'.format(dataset_name, model)
    try:
      create_directory(target_dir)
    except Exception as e:
      print(e)
    
    index = 0
    abstained_counter = 0
    sample_num = 0
    with open('{}/{}.txt'.format(target_dir, prompt_type), "w") as file:
        for item in output:
            if len(item['Triples']) == 0:
                abstained_counter += 1
        file.write("Prompt: \n''' " + target_prompt + "''' \n")
        micro_p, micro_r, micro_f1  = resutls
        file.write("micro_p = {}\nmicro_r = {}\nmicro_f1 = {}\n\n".format(micro_p, micro_r, micro_f1 ))
        file.write("Rate of unstructured or abstained output: {}%\n\n".format(abstained_counter / len(output)))
        for i, item in enumerate(output):
            if len(item['Triples']) == 0:
                abstained_counter += 1
            for smple in dataset:
                if smple['id'] == item['id']:
                    file.write("Sentence: {}\n".format(smple['tokens']))
            file.write("Gold -----> {}\n\n".format(dataset_gold_samples[item['id']]))
            if cot_reasoning_list:
                file.write("Reasoning ---> {}\n\n".format(cot_reasoning_list[i]))
            for pred in item['Triples']:
                file.write("{:90}".format(str(pred)))
                if nb_predictions[index] != "wrong":
                    file.write("*** Correct Prediction **** {}\n".format(nb_predictions[index]))
                else:
                    file.write("\n")
                index += 1
            file.write("\n***\n\n")


def create_directory(directory_name):
    try:
        os.mkdir(directory_name)
        print("Directory '{}' created successfully.".format(directory_name))
    except FileExistsError:
        print("Directory '{}' already exists.".format(directory_name))
    except Exception as e:
        print("An error occurred:", e)
