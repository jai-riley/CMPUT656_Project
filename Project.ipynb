{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dnGEtzKeBiR5",
        "wfBtcEKJrO2L",
        "egTRG4rjmnzD"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T1NX3D0_8vps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7e7684-8d43-4270-a8c9-1ff224c2e44e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m737.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# %pip install --upgrade pillow\n",
        "%pip install --upgrade --quiet  langchain-google-genai pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import google.generativeai as genai\n",
        "import bigframes.dataframe\n",
        "# Replace with your model\n",
        "from vertexai import generative_models\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# from vertexai.preview.generative_models import (\n",
        "#     HarmCategory,\n",
        "#     HarmBlockThreshold )\n",
        "from google.cloud.aiplatform_v1beta1.types.content import SafetySetting\n",
        "import ast\n",
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "9yBXTzTo89s3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "\n",
        "#     \" #getpass.getpass(\"Provide your Google API Key\")\n",
        "#     # os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAEwNMlP_mg5oDc4Ut06CkUc_v7Ow_bAg4\" #getpass.getpass(\"Provide your Google API Key\")\n",
        "\n",
        "# safety_settings : list[str] = [{\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "#                     {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "#                     {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "#                     {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"}]\n",
        "\n",
        "# safety_config = {\n",
        "#         generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
        "#         generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
        "#         generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH:  generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
        "#         generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_NONE\n",
        "#     }\n",
        "# llm = genai.GenerativeModel('gemini-pro', safety_settings = safety_settings)"
      ],
      "metadata": {
        "id": "WcRDLK7L89p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini Loading"
      ],
      "metadata": {
        "id": "NzyRX34yaJaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.cloud.aiplatform_v1beta1.types.content import SafetySetting\n",
        "from vertexai.preview.generative_models import HarmCategory, HarmBlockThreshold\n",
        "gemini_api_key = ''\n",
        "os.environ[\"GOOGLE_API_KEY\"] = gemini_api_key\n",
        "safety_settings = [\n",
        "    {\n",
        "        \"category\": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,        \"threshold\": HarmBlockThreshold.BLOCK_NONE,    },    {\n",
        "        \"category\": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,        \"threshold\": HarmBlockThreshold.BLOCK_NONE,    },    {\n",
        "        \"category\": HarmCategory.HARM_CATEGORY_HATE_SPEECH,        \"threshold\": HarmBlockThreshold.BLOCK_NONE,    },    {\n",
        "        \"category\": HarmCategory.HARM_CATEGORY_HARASSMENT,        \"threshold\": HarmBlockThreshold.BLOCK_NONE,    },]\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n"
      ],
      "metadata": {
        "id": "t7H-TE5F1NBJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT 4 loading"
      ],
      "metadata": {
        "id": "wzdjaPPXaMC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "t1OLKvZmazJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "XMPCDI8EatFH"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_api_key = ''\n",
        "llm = OpenAI(api_key=gpt_api_key)"
      ],
      "metadata": {
        "id": "cSNh-hLoa5Gf"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_get_completion(prompt, model=\"gpt-4\"):\n",
        "    completion = llm.chat.completions.create(\n",
        "    model= model,\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Act as a text-to-SQL model\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "    )\n",
        "    return(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "7u2DW1CTa7nd"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Os-z6rBib18X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAVEN"
      ],
      "metadata": {
        "id": "ziPhAGsGjrQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAVEN_dataset = []"
      ],
      "metadata": {
        "id": "JzhknM9IbPzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_events = ['Action', 'Arriving', 'Robbery', 'Preventing_or_letting', 'Kidnapping', 'Bodily_harm', 'Committing_crime', 'Temporary_stay', 'Writing', 'Commerce_sell', 'Recovering', 'Death', 'Cause_change_of_position_on_a_scale', 'Having_or_lacking_access', 'Participation', 'Reveal_secret', 'Competition', 'Extradition', 'Research', 'Wearing', 'Damaging', 'Hold', 'Carry_goods', 'Violence', 'Becoming', 'Labeling', 'Process_start', 'Justifying', 'Risk', 'Attack', 'Change_event_time', 'Bearing_arms', 'Conquering', 'Expressing_publicly', 'Employment', 'Presence', 'Prison', 'Quarreling', 'Emptying', 'Name_conferral', 'Change_of_leadership', 'Confronting_problem', 'Coming_to_believe', 'Expansion', 'Assistance', 'Placing', 'Arranging', 'Choosing', 'Traveling', 'Control', 'Testing', 'Military_operation', 'Bringing', 'Cost', 'Cause_to_amalgamate', 'Releasing', 'Protest', 'Aiming', 'Award', 'Imposing_obligation', 'Request', 'Connect', 'Judgment_communication', 'Destroying', 'Agree_or_refuse_to_act', 'Causation', 'Institutionalization', 'Adducing', 'Check', 'Response', 'Submitting_documents', 'Resolve_problem', 'Removing', 'Rewards_and_punishments', 'Dispersal', 'Terrorism', 'Scouring', 'Motion_directional', 'Patrolling', 'Scrutiny', 'Getting', 'Revenge', 'Ratification', 'Cure', 'Hindering', 'Cause_to_make_progress', 'Breathing', 'Commerce_pay', 'Supporting', 'Hiding_objects', 'Commitment', 'Telling', 'Change_sentiment', 'Emergency', 'Legality', 'Cause_change_of_strength', 'Forming_relationships', 'Coming_to_be', 'GetReady', 'Hostile_encounter', 'Lighting', 'Being_in_operation', 'Receiving', 'Building', 'Change_tool', 'Warning', 'Departing', 'Perception_active', 'Criminal_investigation', 'Communication', 'Suspicion', 'Exchange', 'Change', 'Education_teaching', 'Giving', 'Using', 'Manufacturing', 'Creating', 'Legal_rulings', 'Collaboration', 'Use_firearm', 'Publishing', 'Motion', 'Know', 'GiveUp', 'Escaping', 'Filling', 'Reporting', 'Create_artwork', 'Rescuing', 'Influence', 'Convincing', 'Reforming_a_system', 'Defending', 'Body_movement', 'Limiting', 'Cause_to_be_included', 'Practice', 'Catastrophe', 'Containing', 'Supply', 'Expend_resource', 'Surrendering', 'Sending', 'Vocalizations', 'Come_together', 'Earnings_and_losses', 'Preserving', 'Killing', 'Renting', 'Self_motion', 'Surrounding', 'Social_event', 'Arrest', 'Rite', 'Openness', 'Incident', 'Ingestion', 'Besieging', 'Process_end', 'Theft', 'Commerce_buy', 'Sign_agreement', 'Statement', 'Achieve', 'Becoming_a_member', 'Deciding', 'Recording']"
      ],
      "metadata": {
        "id": "s8BBoobrbNib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40 samples from MAVEN"
      ],
      "metadata": {
        "id": "SzVuEJF2b0zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAVEN_dataset_40samples = [\n",
        "{'sentence': \"French troops were sent into the area, as Général d'armée Maxime Weygand attempted to build up a defence in depth on the south bank of the Somme and make bigger attacks to eliminate the German bridgeheads.\", 'events': [{'Event_type': 'Sending', 'trigger_word': 'sent'}, {'Event_type': 'Building', 'trigger_word': 'build'}, {'Event_type': 'Creating', 'trigger_word': 'make'}, {'Event_type': 'Destroying', 'trigger_word': 'eliminate'}], 'id': 0},\n",
        "{'sentence': 'From the end of June to early September, over 3,000 forest fires were recorded across the nation.', 'events': [{'Event_type': 'Recording', 'trigger_word': 'recorded'}], 'id': 1},\n",
        "{'sentence': 'The invasion of Kuwait on 2 August 1990 was a two-day operation conducted by Iraq against the neighboring State of Kuwait, which resulted in the seven-month-long Iraqi occupation of the country.', 'events': [{'Event_type': 'Action', 'trigger_word': 'conducted'}, {'Event_type': 'Causation', 'trigger_word': 'resulted in'}], 'id': 2},\n",
        "{'sentence': 'Market Garden consisted of two sub operations: The attack was the largest airborne operation up to that point in World War II.', 'events': [{'Event_type': 'Attack', 'trigger_word': 'attack'}], 'id': 3},\n",
        "{'sentence': 'The hurricane continued to strengthen while developing a well-defined eye, and peaked as a Category 4 hurricane on September 11.', 'events': [{'Event_type': 'Cause_change_of_strength', 'trigger_word': 'strengthen'}, {'Event_type': 'Catastrophe', 'trigger_word': 'hurricane'}, {'Event_type': 'Self_motion', 'trigger_word': 'peaked'}], 'id': 4},\n",
        "{'sentence': 'A groundbreaking of the permanent memorial occurred in June 2006.', 'events': [{'Event_type': 'Social_event', 'trigger_word': 'memorial'}, {'Event_type': 'Presence', 'trigger_word': 'occurred'}], 'id': 5},\n",
        "{'sentence': 'As winners, Chelsea took part in the 2012 UEFA Super Cup, losing 4–1 to Atlético Madrid, the winners of the 2011–12 UEFA Europa League.', 'events': [{'Event_type': 'Earnings_and_losses', 'trigger_word': 'losing'}], 'id': 6},\n",
        "{'sentence': 'Two people died in the incident: the pilot, Pete Barnes, 50, and a pedestrian, Matthew Wood, 39, from Sutton in south London.', 'events': [{'Event_type': 'Death', 'trigger_word': 'died'}], 'id': 7},\n",
        "{'sentence': 'A series of meetings with the Conservatives began shortly after the hung parliament was announced, and continued over the weekend after the election.', 'events': [{'Event_type': 'Process_start', 'trigger_word': 'began'}, {'Event_type': 'Change_of_leadership', 'trigger_word': 'election'}, {'Event_type': 'Expressing_publicly', 'trigger_word': 'announced'}, {'Event_type': 'Social_event', 'trigger_word': 'meetings'}], 'id': 8},\n",
        "{'sentence': \"The festival's acts come from a wide range of genres, such as: electro, rock, drum and bass, pop, R&B, reggae, house, punk, hardcore, metal, hip-hop, indie, techno, and more.\", 'events': [{'Event_type': 'Social_event', 'trigger_word': 'festival'}, {'Event_type': 'Social_event', 'trigger_word': 'pop'}], 'id': 9},\n",
        "{'sentence': 'The chase was short, as \"Duguay Trouin\" was a poor sailor with many of the crew sick and unable to report for duty.', 'events': [{'Event_type': 'Self_motion', 'trigger_word': 'chase'}], 'id': 10},\n",
        "{'sentence': 'The ICTY convicted two JNA officers in connection with the massacre, and also tried former Serbian President Slobodan Milošević for a number of war crimes, including those committed at Vukovar.', 'events': [{'Event_type': 'Legal_rulings', 'trigger_word': 'convicted'}], 'id': 11},\n",
        "{'sentence': 'Weakening as it drifted inland, Winifred persisted as a tropical depression for another five days after landfall before finally dissipating on 5 February.', 'events': [{'Event_type': 'Motion', 'trigger_word': 'drifted'}, {'Event_type': 'Cause_change_of_strength', 'trigger_word': 'Weakening'}, {'Event_type': 'Arriving', 'trigger_word': 'landfall'}, {'Event_type': 'Wearing', 'trigger_word': 'persisted'}, {'Event_type': 'Removing', 'trigger_word': 'dissipating'}], 'id': 12},\n",
        "{'sentence': 'The whole community greets the Sun as they listen to Tibetan chants and guest musicians on the grassy hill.', 'events': [{'Event_type': 'Perception_active', 'trigger_word': 'listen'}, {'Event_type': 'Traveling', 'trigger_word': 'guest'}], 'id': 13},\n",
        "{'sentence': 'In Libya the Islamic State of Iraq and the Levant (ISIL) has been able to control some limited territory in the ongoing civil war since 2014, amid allegations of local collaboration between the otherwise rivalling AQIM and ISIL.', 'events': [{'Event_type': 'Control', 'trigger_word': 'control'}, {'Event_type': 'Limiting', 'trigger_word': 'limited'}, {'Event_type': 'Hostile_encounter', 'trigger_word': 'war'}], 'id': 14},\n",
        "{'sentence': 'The Inter-Provincial Series has been funded at least partly by the ICC via their TAPP programme.', 'events': [{'Event_type': 'Supply', 'trigger_word': 'funded'}], 'id': 15},\n",
        "{'sentence': 'The \"Struma\" disaster joined that of SS \"Patria\" – sunk after Haganah sabotage while laden with Jewish refugees 15 months earlier – as rallying points for the Irgun and Lehi revisionist Zionist clandestine movements, encouraging their violent revolt against the British presence in Palestine.', 'events': [{'Event_type': 'Change_of_leadership', 'trigger_word': 'revolt'}, {'Event_type': 'Becoming_a_member', 'trigger_word': 'joined'}, {'Event_type': 'Self_motion', 'trigger_word': 'sunk'}, {'Event_type': 'Bringing', 'trigger_word': 'laden'}], 'id': 16},\n",
        "{'sentence': 'On Friday, 28 February 1986, at 23:21 CET (22:21 UTC), Olof Palme, Prime Minister of Sweden, was fatally wounded by a single gunshot while walking home from a cinema with his wife Lisbet Palme on the central Stockholm street Sveavägen.', 'events': [{'Event_type': 'Bodily_harm', 'trigger_word': 'wounded'}, {'Event_type': 'Use_firearm', 'trigger_word': 'gunshot'}, {'Event_type': 'Self_motion', 'trigger_word': 'walking'}], 'id': 17},\n",
        "{'sentence': 'The Department of Social Security (DSS) sent employees to receive claims for damage, requests for financial aid, and filings for unemployment benefits.', 'events': [{'Event_type': 'Sending', 'trigger_word': 'sent'}, {'Event_type': 'Request', 'trigger_word': 'requests'}, {'Event_type': 'Receiving', 'trigger_word': 'receive'}, {'Event_type': 'Assistance', 'trigger_word': 'aid'}, {'Event_type': 'Employment', 'trigger_word': 'employees'}, {'Event_type': 'Earnings_and_losses', 'trigger_word': 'benefits'}, {'Event_type': 'Damaging', 'trigger_word': 'damage'}], 'id': 18},\n",
        "{'sentence': 'Air Algérie Flight 6289 (AH6289), was a domestic passenger flight which crashed on 6 March 2003, at the Aguenar – Hadj Bey Akhamok Airport in Algeria, killing all but one of the 103 people on board.', 'events': [{'Event_type': 'Catastrophe', 'trigger_word': 'crashed'}, {'Event_type': 'Killing', 'trigger_word': 'killing'}], 'id': 19},\n",
        "{'sentence': \"The accident is the RAF's worst peacetime disaster.\", 'events': [{'Event_type': 'Catastrophe', 'trigger_word': 'accident'}], 'id': 20},\n",
        "{'sentence': 'The governing Labour administration led by Gordon Brown was defeated in the election and lost its overall majority after 13 years in office.', 'events': [{'Event_type': 'Change_of_leadership', 'trigger_word': 'election'}, {'Event_type': 'Earnings_and_losses', 'trigger_word': 'lost'}, {'Event_type': 'Hostile_encounter', 'trigger_word': 'defeated'}], 'id': 21},\n",
        "{'sentence': 'The battle is mentioned in the Babylonian Chronicles, now housed in the British Museum.', 'events': [{'Event_type': 'Hostile_encounter', 'trigger_word': 'battle'}, {'Event_type': 'Statement', 'trigger_word': 'mentioned'}, {'Event_type': 'Containing', 'trigger_word': 'housed'}], 'id': 22},\n",
        "{'sentence': \"Nicolas Anelka of France scored the first goal in Club World Cup history, while Brazilian champions Corinthians' goalkeeper Dida posted the first official clean sheet in the tournament.\", 'events': [{'Event_type': 'Getting', 'trigger_word': 'scored'}, {'Event_type': 'Competition', 'trigger_word': 'champions'}, {'Event_type': 'Competition', 'trigger_word': 'tournament'}, {'Event_type': 'Sending', 'trigger_word': 'posted'}], 'id': 23},\n",
        "{'sentence': 'Hawthorn, who were competing in their inaugural VFL Grand Final despite being in the competition since 1925, came into the game as minor premiers and favourites.', 'events': [{'Event_type': 'Competition', 'trigger_word': 'competing'}, {'Event_type': 'Motion', 'trigger_word': 'came'}], 'id': 24},\n",
        "{'sentence': 'At the same time, General Joaquín Fanjul, commander of the military garrison based in Montaña barracks in Madrid, was preparing to launch the military rebellion in the city.', 'events': [{'Event_type': 'Process_start', 'trigger_word': 'launch'}, {'Event_type': 'GetReady', 'trigger_word': 'preparing'}], 'id': 25},\n",
        "{'sentence': 'The Engineers were said to have missed their best back, Lieut.', 'events': [{'Event_type': 'Communication', 'trigger_word': 'said'}, {'Event_type': 'Earnings_and_losses', 'trigger_word': 'missed'}], 'id': 26},\n",
        "{'sentence': 'They were organised on the initiative of deaf Frenchman Eugène Rubens-Alcais, who, just after the Games, co-founded the Comité International des Sports des Sourds with other \"deaf sporting leaders\".', 'events': [{'Event_type': 'Arranging', 'trigger_word': 'organised'}, {'Event_type': 'Collaboration', 'trigger_word': 'co-founded'}], 'id': 27},\n",
        "{'sentence': 'The impeachment trial was formally opened on November 20, with twenty-one senators taking their oaths as judges, and Supreme Court Chief Justice Hilario Davide, Jr. presiding.', 'events': [{'Event_type': 'Openness', 'trigger_word': 'opened'}], 'id': 28},\n",
        "{'sentence': 'The defense of Sihang Warehouse took place from October 26 to November 1, 1937, and marked the beginning of the end of the three-month Battle of Shanghai in the opening phase of the Second Sino-Japanese War.', 'events': [{'Event_type': 'Defending', 'trigger_word': 'defense'}, {'Event_type': 'Process_start', 'trigger_word': 'beginning'}, {'Event_type': 'Process_start', 'trigger_word': 'took place'}, {'Event_type': 'Recording', 'trigger_word': 'marked'}, {'Event_type': 'Process_end', 'trigger_word': 'end'}, {'Event_type': 'Hostile_encounter', 'trigger_word': 'Battle'}, {'Event_type': 'Hostile_encounter', 'trigger_word': 'War'}, {'Event_type': 'Openness', 'trigger_word': 'opening'}], 'id': 29},\n",
        "{'sentence': 'The attack failed, and a small raid that evening inflicted little damage.', 'events': [{'Event_type': 'Attack', 'trigger_word': 'attack'}, {'Event_type': 'Agree_or_refuse_to_act', 'trigger_word': 'failed'}, {'Event_type': 'Causation', 'trigger_word': 'inflicted'}], 'id': 30},\n",
        "{'sentence': \"The report cites research that shows people's feelings about a police encounter depend significantly on whether they feel the officer displays respect and courtesy.\", 'events': [{'Event_type': 'Change_sentiment', 'trigger_word': 'feel'}, {'Event_type': 'Adducing', 'trigger_word': 'cites'}, {'Event_type': 'Influence', 'trigger_word': 'depend'}], 'id': 31},\n",
        "{'sentence': 'Fighting continued for the next two days.', 'events': [{'Event_type': 'Hostile_encounter', 'trigger_word': 'Fighting'}], 'id': 32},\n",
        "{'sentence': 'On 19 March, he opened fire at the Ozar Hatorah Jewish day school in Toulouse, killing a rabbi and three children, also wounding four others.', 'events': [{'Event_type': 'Openness', 'trigger_word': 'opened'}, {'Event_type': 'Killing', 'trigger_word': 'killing'}], 'id': 33},\n",
        "{'sentence': 'Peace negotiations and foreign involvement led to a ceasefire in 1995 that was broken the next year before a final peace agreement and new national elections were held in 1997.', 'events': [{'Event_type': 'Communication', 'trigger_word': 'negotiations'}, {'Event_type': 'Participation', 'trigger_word': 'involvement'}, {'Event_type': 'Process_end', 'trigger_word': 'ceasefire'}, {'Event_type': 'Sign_agreement', 'trigger_word': 'agreement'}, {'Event_type': 'Change_of_leadership', 'trigger_word': 'elections'}, {'Event_type': 'Bodily_harm', 'trigger_word': 'broken'}, {'Event_type': 'Causation', 'trigger_word': 'led to'}], 'id': 34},\n",
        "{'sentence': 'On 2 August 1990 the Iraqi Army invaded and occupied Kuwait, which was met with international condemnation and brought immediate economic sanctions against Iraq by members of the UN Security Council.', 'events': [{'Event_type': 'Bringing', 'trigger_word': 'brought'}, {'Event_type': 'Attack', 'trigger_word': 'invaded'}, {'Event_type': 'Control', 'trigger_word': 'occupied'}, {'Event_type': 'Come_together', 'trigger_word': 'met with'}], 'id': 35},\n",
        "{'sentence': 'The Fall of Philadelphia marked the fall of Philadelphia, the last independent Christian Greek settlement in western Asia Minor, to the Muslim Turks of the Ottoman Sultanate.', 'events': [{'Event_type': 'Recording', 'trigger_word': 'marked'}, {'Event_type': 'Motion_directional', 'trigger_word': 'fall'}], 'id': 36},\n",
        "{'sentence': \"They see the Council's decision as part of a wider 'cultural war' against 'Britishness' in Northern Ireland.\", 'events': [{'Event_type': 'Deciding', 'trigger_word': 'decision'}, {'Event_type': 'Military_operation', 'trigger_word': 'war'}], 'id': 37},\n",
        "{'sentence': 'In the wake of the Polish advance eastward, the Soviets sued for peace and the war ended with a cease-fire in October 1920.', 'events': [{'Event_type': 'Process_end', 'trigger_word': 'ended'}, {'Event_type': 'Cause_to_make_progress', 'trigger_word': 'advance'}, {'Event_type': 'Hostile_encounter', 'trigger_word': 'war'}, {'Event_type': 'Request', 'trigger_word': 'sued'}], 'id': 38},\n",
        "{'sentence': 'Western fears of Soviet troops arriving at the German frontiers increased the interest of Western powers in the war.', 'events': [{'Event_type': 'Cause_change_of_position_on_a_scale', 'trigger_word': 'increased'}, {'Event_type': 'Arriving', 'trigger_word': 'arriving at'}, {'Event_type': 'Hostile_encounter', 'trigger_word': 'war'}], 'id': 39},\n",
        "]"
      ],
      "metadata": {
        "id": "eDEwGweEaUcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_list_of_events = []\n",
        "for item in MAVEN_dataset_40samples:\n",
        "  for event in item['events']:\n",
        "    sample_list_of_events.append(event['Event_type'])\n",
        "sample_list_of_events = list(set(sample_list_of_events))\n"
      ],
      "metadata": {
        "id": "dsBHsc8Calmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Event Extraction"
      ],
      "metadata": {
        "id": "dnGEtzKeBiR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Setup"
      ],
      "metadata": {
        "id": "9s_WF5AKvwWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EE_prompt_runner(prompt_base, dataset, list_of_events):\n",
        "  counter = 0\n",
        "  output_list = []\n",
        "  for idx, sample in enumerate(dataset):\n",
        "    prompt = prompt_base.format(list_of_events, sample['sentence'])\n",
        "    # print(prompt)\n",
        "    # output_list.append( {\"id\" : sample[\"id\"]})\n",
        "    try:\n",
        "      prediction = llm.invoke(prompt)\n",
        "      # print(output_list, idx)\n",
        "      answer =  prediction.content\n",
        "      # print(\"\\n **** \\n {} \\n **** \\n\".format(answer))\n",
        "      if \"Event type: \" not in  answer:\n",
        "          answer = \"Event type: \" + answer\n",
        "      # print(prompt)\n",
        "      output_list.append( {\"id\" : sample[\"id\"], 'prediction':answer })\n",
        "      # output_list[idx]['prediction'] = answer\n",
        "      counter += 1\n",
        "    except Exception as e:\n",
        "      # print(e)\n",
        "      print('error for sentence: ', sample['sentence'])\n",
        "      # output_list[idx]['prediction'] =  'error'\n",
        "    if counter == 20:\n",
        "      break\n",
        "\n",
        "  return output_list"
      ],
      "metadata": {
        "id": "j4h_1rMau48x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EE_output_processing(org_output):\n",
        "  output = copy.deepcopy(org_output)\n",
        "  # remove empty list samples\n",
        "  output = [item for item in output if len(item.keys())>= 2]\n",
        "  # for sample in output:\n",
        "  #   if len(sample.keys()) < 2:\n",
        "  #     output.remove(sample)\n",
        "  # print(output)\n",
        "\n",
        "  for sample in output:\n",
        "    sample['prediction'] = [ item.strip() for item in sample['prediction'].split(\"Event type: \")[1].strip().split(',')]\n",
        "  return output"
      ],
      "metadata": {
        "id": "uXFrJRMzn6Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EE_gold_samples_generator(MAVAN_dataset):\n",
        "  gold_samples = {}\n",
        "  for sample in MAVAN_dataset:\n",
        "    # gold_sample = {}\n",
        "    gold_samples[sample['id']] = [event['Event_type'] for event in  sample['events']]\n",
        "    # gold_samples.append(gold_sample)\n",
        "  return gold_samples"
      ],
      "metadata": {
        "id": "uvke66wpn6Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_lists(list1, list2):\n",
        "    # Combine the input lists and find unique elements\n",
        "    merged_list = list1 + list2\n",
        "    unique_elements = list(set(merged_list))\n",
        "\n",
        "    # Create placeholders for elements not present in each list\n",
        "    reordered_list1 = []\n",
        "    reordered_list2 = []\n",
        "    for element in unique_elements:\n",
        "        if element in list1 and element in list2:\n",
        "            reordered_list1.append(element)\n",
        "            reordered_list2.append(element)\n",
        "        elif element in list1:\n",
        "            reordered_list1.append(element)\n",
        "            reordered_list2.append(\"0\")\n",
        "        elif element in list2:\n",
        "            reordered_list1.append(\"0\")\n",
        "            reordered_list2.append(element)\n",
        "\n",
        "    return reordered_list1, reordered_list2"
      ],
      "metadata": {
        "id": "NueuRNxkn6EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EE_evaluation(gold_samples, processed_output):\n",
        "  labels = []\n",
        "  predictions = []\n",
        "\n",
        "  for item in processed_output:\n",
        "    # print(item['prediction'])\n",
        "    # print(gold_samples[item['id']])\n",
        "    # print(\"\\n *** \\n \")\n",
        "    gold, pred = reorder_lists(gold_samples[item['id']] ,item['prediction'])\n",
        "    labels.extend(gold)\n",
        "    predictions.extend(pred)\n",
        "\n",
        "  return labels, predictions"
      ],
      "metadata": {
        "id": "g5XwBQcqoUga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EE_metric_calculation(labels, predictions, event_labels):\n",
        "\n",
        "  # When predicting an event not present in the ground truth, our predictions are incorrect, resulting in a decrease in our Precision (P).\n",
        "  # If we fail to predict any event when one exists in the ground truth, it decreases our Recall (R).\n",
        "  #This is why we include zeros in both the ground truth and prediction lists, to address the inconsistency in the number of extracted events.\"\n",
        "\n",
        "  micro_p = precision_score(labels,predictions, labels = event_labels,average='micro')*100.0\n",
        "  micro_r = recall_score(labels,predictions, labels = event_labels, average='micro')*100.0\n",
        "  micro_f1 = f1_score(labels,predictions, labels = event_labels, average='micro')*100.0\n",
        "\n",
        "\n",
        "  print(\"Micro_F1:\", round(micro_f1,2))\n",
        "  print(\"Micro_Precision:\", round(micro_p,2))\n",
        "  print(\"Micro_Recall:\", round(micro_r,2))\n",
        "\n",
        "\n",
        "  # macro_p=precision_score(labels,predictions,labels = event_labels,average='macro')*100.0\n",
        "  # macro_r=recall_score(labels,predictions,labels = event_labels,average='macro')*100.0\n",
        "  # macro_f1=f1_score(labels,predictions,labels = event_labels,average='macro')*100.0\n",
        "\n",
        "\n",
        "  # print(\"Macro_F1:\",macro_f1)\n",
        "  # print(\"Macro_Precision:\",macro_p)\n",
        "  # print(\"Macro_Recall:\",macro_r)\n"
      ],
      "metadata": {
        "id": "fRfn3Hqwp25T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "5dnBv0jvom1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gold_samples = EE_gold_samples_generator(MAVEN_dataset_40samples)"
      ],
      "metadata": {
        "id": "JVzcV0_3sHqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Prompts"
      ],
      "metadata": {
        "id": "_9H-9OOspSf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EE_zero_shot_paper = '''\n",
        "The list of event types: {}\n",
        "Give a sentence: French troops were sent into the area, as Général d'armée Maxime Weygand attempted to build up a defence in depth on the south bank of the Somme and make bigger attacks to eliminate the German bridgeheads.\n",
        "What types of events are included in this sentence? Please return the most likely answer according to the list of event types above. Require the answer in the form: Event type\n",
        "Give a sentence: {}\n",
        "Event type:\n",
        "'''\n",
        "\n",
        "EE_one_shot_paper = '''\n",
        "The list of event types: {}\n",
        "What types of events are included in the following sentence? Please return the most likely answer according to the list of event types above. Require the answer in the form: Event type.Ensure that the extracted event is included in the prepared list.\n",
        "Example:\n",
        "\n",
        "Give a sentence: French troops were sent into the area, as Général d'armée Maxime Weygand attempted to build up a defence in depth on the south bank of the Somme and make bigger attacks to eliminate the German bridgeheads.\n",
        "Event type: Sending, Building, Creating, Destroying\n",
        "\n",
        "Give a sentence: {}\n",
        "Event type:\n",
        "'''\n"
      ],
      "metadata": {
        "id": "bwn4aZCEpREL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replicating Paper results"
      ],
      "metadata": {
        "id": "BBexxuEOpiRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero_Shot\n",
        "output = EE_prompt_runner(EE_zero_shot_paper, MAVEN_dataset_40samples, sample_list_of_events)\n",
        "processed_output = EE_output_processing(output)\n",
        "labels, predictions = EE_evaluation(gold_samples, processed_output)\n",
        "EE_metric_calculation(labels, predictions, sample_list_of_events)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbb6KtLmYRgC",
        "outputId": "ac6c50a7-54f8-4c51-f58f-4a97ae8f0d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for sentence:  Two people died in the incident: the pilot, Pete Barnes, 50, and a pedestrian, Matthew Wood, 39, from Sutton in south London.\n",
            "error for sentence:  The ICTY convicted two JNA officers in connection with the massacre, and also tried former Serbian President Slobodan Milošević for a number of war crimes, including those committed at Vukovar.\n",
            "error for sentence:  The \"Struma\" disaster joined that of SS \"Patria\" – sunk after Haganah sabotage while laden with Jewish refugees 15 months earlier – as rallying points for the Irgun and Lehi revisionist Zionist clandestine movements, encouraging their violent revolt against the British presence in Palestine.\n",
            "error for sentence:  On Friday, 28 February 1986, at 23:21 CET (22:21 UTC), Olof Palme, Prime Minister of Sweden, was fatally wounded by a single gunshot while walking home from a cinema with his wife Lisbet Palme on the central Stockholm street Sveavägen.\n",
            "Micro_F1: 8.421052631578947\n",
            "Micro_Precision: 8.88888888888889\n",
            "Micro_Recall: 8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One_Shot\n",
        "output = EE_prompt_runner(EE_one_shot_paper, MAVEN_dataset_40samples, sample_list_of_events)\n",
        "processed_output = EE_output_processing(output)\n",
        "labels, predictions = EE_evaluation(gold_samples, processed_output)\n",
        "EE_metric_calculation(labels, predictions, sample_list_of_events)"
      ],
      "metadata": {
        "id": "ext2gVFoxVzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec19df49-26fd-48f2-a32a-93dfb5d062f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for sentence:  French troops were sent into the area, as Général d'armée Maxime Weygand attempted to build up a defence in depth on the south bank of the Somme and make bigger attacks to eliminate the German bridgeheads.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for sentence:  On Friday, 28 February 1986, at 23:21 CET (22:21 UTC), Olof Palme, Prime Minister of Sweden, was fatally wounded by a single gunshot while walking home from a cinema with his wife Lisbet Palme on the central Stockholm street Sveavägen.\n",
            "Micro_F1: 24.615384615384617\n",
            "Micro_Precision: 47.05882352941176\n",
            "Micro_Recall: 16.666666666666664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying designed CoT prompt"
      ],
      "metadata": {
        "id": "PvFE2XQ_tf3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EE_cot_one_shot1 = '''\n",
        "\n",
        "Analyze the provided sentence step by step to extract the event types:\n",
        "\n",
        "Step 1: Recognize Trigger Words: Trigger words are terms or phrases indicating the start or occurrence of events within a text or context.\n",
        "Step 2: Match each trigger word with the most relevant event type from the list of events. The list of event types: {}\n",
        "output: Format the output by listing the event types separated by commas after the keyword \"Event type: \"\n",
        "\n",
        "example:\n",
        "\n",
        "provided sentence: French troops were sent into the area, as Général d'armée Maxime Weygand attempted to build up a defence in depth on the south bank of the Somme and make bigger attacks to eliminate the German bridgeheads.\n",
        "\n",
        "Step1: triger words: <sent>, <build>, <make>, <eliminate>\n",
        "\n",
        "Step2:\n",
        "For the trigger word <sent>, the most related event among events in the Event list is <Sending>.\n",
        "For the trigger word <build>, the most related event among events in the Event list is <Building>.\n",
        "For the trigger word <make>, the most related event among events in the Event list is <Creating>.\n",
        "For the trigger word <eliminate>, the most related event among events in the Event list is <Destroying>.\n",
        "\n",
        "Event type: Sending, Building, Creating, Destroying\n",
        "\n",
        "provided sentence: {}\n",
        "'''"
      ],
      "metadata": {
        "id": "3B1E9L7qtlrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = EE_prompt_runner(EE_cot_one_shot1, MAVEN_dataset_40samples, sample_list_of_events)\n",
        "processed_output = EE_output_processing(output)\n",
        "labels, predictions = EE_evaluation(gold_samples, processed_output)\n",
        "EE_metric_calculation(labels, predictions, sample_list_of_events)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpUYkQRetWmS",
        "outputId": "04f415ff-700c-4a43-d798-2b0497fa09ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 55.55555555555556\n",
            "Micro_Precision: 60.97560975609756\n",
            "Micro_Recall: 51.02040816326531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Triple Extraction"
      ],
      "metadata": {
        "id": "8GNZixamV9Rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RE-TACRED Dataset"
      ],
      "metadata": {
        "id": "wfBtcEKJrO2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicates = ['per:siblings', 'per:title', 'per:stateorprovince_of_death', 'per:country_of_death', 'org:founded_by', 'per:date_of_birth', 'org:member_of', 'per:country_of_birth', 'org:shareholders', 'org:top_members/employees', 'org:founded', 'per:stateorprovinces_of_residence', 'per:religion', 'per:cause_of_death', 'per:stateorprovince_of_birth', 'org:number_of_employees/members', 'org:city_of_branch', 'org:members', 'per:charges', 'per:identity', 'per:city_of_death', 'per:employee_of', 'org:alternate_names', 'org:stateorprovince_of_branch', 'per:schools_attended', 'per:children', 'per:countries_of_residence', 'per:other_family', 'org:political/religious_affiliation', 'no_relation', 'per:parents', 'org:website', 'per:date_of_death', 'per:spouse', 'per:origin', 'per:age', 'per:cities_of_residence', 'per:city_of_birth', 'org:dissolved', 'org:country_of_branch']\n",
        "len(predicates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m7A_lYexHMY",
        "outputId": "ce0e0518-eb6c-421d-a11f-d60d00f77618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RE_TACRED_dataset_40samples = [\n",
        "{\"relation\": \"no_relation\", \"tokens\": \"Chen Tao unleashed the corner , a Botswana defender cleared it out of the box , but Shen was again picked by the loose ball and drove it in with a tight angle to make the breakthrough for China in the 40 minute .\", \"h\": {\"name\": \"Chen Tao\", \"pos\": [0, 2]}, \"subj_type\": \"PERSON\", \"t\": {\"name\": \"Shen\", \"pos\": [17, 18]}, \"obj_type\": \"PERSON\", \"id\":0},\n",
        "{\"relation\": \"no_relation\", \"tokens\": \"Her recovery from the attack -- the chimp bit and clawed off her face and hands -- was presented to the world via an episode of the `` Oprah Winfrey Show '' in November .\", \"h\": {\"name\": \"her\", \"pos\": [12, 13]}, \"subj_type\": \"PERSON\", \"t\": {\"name\": \"attack\", \"pos\": [4, 5]}, \"obj_type\": \"CAUSE_OF_DEATH\", \"id\":1},\n",
        "{\"relation\": \"no_relation\", \"tokens\": \"Two women who filed sexual harassment claims : Worked for NRA , where alleged behavior took place .\", \"h\": {\"name\": \"NRA\", \"pos\": [10, 11]}, \"subj_type\": \"ORGANIZATION\", \"t\": {\"name\": \"Two\", \"pos\": [0, 1]}, \"obj_type\": \"NUMBER\",\"id\":2},\n",
        "{\"relation\": \"per:identity\", \"tokens\": \"American Amanda Knox accuses police of forcing false statements out of her after she was arrested over the 2007 sex-murder of her British housemate in central Italy as she takes the stand for the first time .\", \"h\": {\"name\": \"her\", \"pos\": [21, 22]}, \"subj_type\": \"PERSON\", \"t\": {\"name\": \"Amanda\", \"pos\": [1, 2]}, \"obj_type\": \"PERSON\", \"id\":3},\n",
        "{\"relation\": \"org:member_of\", \"tokens\": \"AIG sold ALICO to MetLife in March .\", \"h\": {\"name\": \"ALICO\", \"pos\": [2, 3]}, \"subj_type\": \"ORGANIZATION\", \"t\": {\"name\": \"AIG\", \"pos\": [0, 1]}, \"obj_type\": \"ORGANIZATION\", \"id\":4},\n",
        "{\"relation\": \"per:parents\", \"tokens\": \"Knox 's mother Edda Mellas was shaking with grief as her stepmother Cassandra Knox moved to comfort her .\", \"h\": {\"name\": \"Knox\", \"pos\": [0, 1]}, \"subj_type\": \"PERSON\", \"t\": {\"name\": \"Cassandra\", \"pos\": [12, 13]}, \"obj_type\": \"PERSON\", \"id\":5}\n",
        "]"
      ],
      "metadata": {
        "id": "ijQhe-xc2SaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# samples in prompts\n",
        "'''\n",
        "The given sentence is : Piedra reported to the IRS that his practice gave $ 107,862 to Scientology groups in 2003 .\n",
        "Triples: [his, per:identity, Piedra]\n",
        "'''"
      ],
      "metadata": {
        "id": "TTAU3tBEP4wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RE_TACRED_list_of_relation = []\n",
        "for item in RE_TACRED_dataset_40samples:\n",
        "  RE_TACRED_list_of_relation.append(item['relation'])\n",
        "\n",
        "RE_TACRED_list_of_relation = list(set(RE_TACRED_list_of_relation))"
      ],
      "metadata": {
        "id": "qxdtkZSzFmiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RE_TACRED_list_of_relation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ6bKygCF4ut",
        "outputId": "f58e531e-2e8b-48ae-9b6d-2af4a9b470d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['per:identity', 'per:parents', 'org:member_of', 'no_relation']"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SciERC Daatset"
      ],
      "metadata": {
        "id": "fmhnFsRF2OIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SciERC_dataset_40samples = [\n",
        "{'id': 0, 'relation': 'USED-FOR', 'tokens': 'We present a novel method for discovering parallel sentences in comparable , non-parallel corpora .', 'h': {'name': 'method', 'pos': [4, 5]}, 't': {'name': 'discovering parallel sentences', 'pos': [6, 9]}},\n",
        "{'id': 1, 'relation': 'USED-FOR', 'tokens': 'During normal tracking conditions when the object is visible from frame to frame , local optimization is used to track the local mode of the similarity measure in a parameter space of translation , rotation and scale .', 'h': {'name': 'parameter space of translation , rotation and scale', 'pos': [29, 37]}, 't': {'name': 'local mode of the similarity measure', 'pos': [21, 27]}},\n",
        "{'id': 2, 'relation': 'USED-FOR', 'tokens': 'This paper describes a method for incorporating priming into an incremental probabilistic parser .', 'h': {'name': 'priming', 'pos': [7, 8]}, 't': {'name': 'incremental probabilistic parser', 'pos': [10, 13]}},\n",
        "{'id': 5, 'relation': 'USED-FOR', 'tokens': 'Our technique is based on an improved , dynamic-programming , stereo algorithm for efficient novel-view generation .', 'h': {'name': 'dynamic-programming , stereo algorithm', 'pos': [8, 12]}, 't': {'name': 'technique', 'pos': [1, 2]}},\n",
        "{'id': 6, 'relation': 'USED-FOR', 'tokens': 'A demonstration -LRB- in UNIX -RRB- for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis -LRB- ICALL -RRB- , including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples .', 'h': {'name': 'lemmatized indexing', 'pos': [32, 34]}, 't': {'name': 'aligned bilingual corpus', 'pos': [36, 39]}},\n",
        "{'id': 7, 'relation': 'USED-FOR', 'tokens': 'We propose a multi-task end-to-end Joint Classification-Regression Recurrent Neural Network to better explore the action type and temporal localiza-tion information .', 'h': {'name': 'multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'pos': [3, 10]}, 't': {'name': 'action type', 'pos': [14, 16]}},\n",
        "{'id': 4, 'relation': 'HYPONYM-OF', 'tokens': 'We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .', 'h': {'name': 'Head Grammars', 'pos': [14, 16]}, 't': {'name': 'grammatical formalisms', 'pos': [7, 9]}},\n",
        "{'id': 43, 'relation': 'HYPONYM-OF', 'tokens': 'This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .', 'h': {'name': 'dependency grammars', 'pos': [20, 22]}, 't': {'name': 'grammar formalisms', 'pos': [12, 14]}},\n",
        "{'id': 57, 'relation': 'HYPONYM-OF', 'tokens': 'We have implemented a restricted domain parser called Plume .', 'h': {'name': 'Plume', 'pos': [8, 9]}, 't': {'name': 'restricted domain parser', 'pos': [4, 7]}},\n",
        "{'id': 59, 'relation': 'HYPONYM-OF', 'tokens': 'There are four language pairs currently supported by GLOSSER : English-Bulgarian , English-Estonian , English-Hungarian and French-Dutch .', 'h': {'name': 'English-Estonian', 'pos': [12, 13]}, 't': {'name': 'language pairs', 'pos': [3, 5]}},\n",
        "{'id': 88, 'relation': 'HYPONYM-OF', 'tokens': 'Turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words .', 'h': {'name': 'Turkish', 'pos': [0, 1]}, 't': {'name': 'agglutinative language', 'pos': [3, 5]}},\n",
        "{'id': 8, 'relation': 'CONJUNCTION', 'tokens': 'Thus , in this paper , we study the problem of robust PCA with side information , where both prior structure and features of entities are exploited for recovery .', 'h': {'name': 'prior structure', 'pos': [19, 21]}, 't': {'name': 'features of entities', 'pos': [22, 25]}},\n",
        "{'id': 16, 'relation': 'CONJUNCTION', 'tokens': 'Multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and reconstruction , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .', 'h': {'name': 'Multi-view constraints', 'pos': [0, 2]}, 't': {'name': 'normalized representation', 'pos': [11, 13]}},\n",
        "{'id': 22, 'relation': 'CONJUNCTION', 'tokens': 'Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of Chinese text available in corpora and on the Web .', 'h': {'name': 'corpora', 'pos': [30, 31]}, 't': {'name': 'Web', 'pos': [34, 35]}},\n",
        "{'id': 36, 'relation': 'CONJUNCTION', 'tokens': 'Furthermore , this paper presents a novel algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts -LRB- flicker -RRB- ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .', 'h': {'name': 'cost aggregation algorithm', 'pos': [32, 35]}, 't': {'name': 'algorithm', 'pos': [7, 8]}},\n",
        "{'id': 54, 'relation': 'CONJUNCTION', 'tokens': 'This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , dags , graphs , and products of them .', 'h': {'name': 'strings', 'pos': [14, 15]}, 't': {'name': 'trees', 'pos': [16, 17]}},\n",
        "{'id': 67, 'relation': 'CONJUNCTION', 'tokens': 'The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases , referring expressions , and interruptions .', 'h': {'name': 'referring expressions', 'pos': [20, 22]}, 't': {'name': 'interruptions', 'pos': [24, 25]}},\n",
        "{'id': 18, 'relation': 'PART-OF', 'tokens': 'Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing -LRB- -LSB- 1 -RSB- -LSB- 2 -RSB- -RRB- .', 'h': {'name': 'Recognition of proper nouns', 'pos': [0, 4]}, 't': {'name': 'morphological analysis', 'pos': [19, 21]}},\n",
        "{'id': 33, 'relation': 'PART-OF', 'tokens': 'In this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances -LRB- called the linguistic structure -RRB- , a structure of purposes -LRB- called the intentional structure -RRB- , and the state of focus of attention -LRB- called the attentional state -RRB- .', 'h': {'name': 'intentional structure', 'pos': [36, 38]}, 't': {'name': 'components', 'pos': [13, 14]}},\n",
        "{'id': 38, 'relation': 'PART-OF', 'tokens': 'We analyzed eye gaze , head nods and attentional focus in the context of a direction-giving task .', 'h': {'name': 'attentional focus', 'pos': [8, 10]}, 't': {'name': 'direction-giving task', 'pos': [15, 17]}},\n",
        "{'id': 40, 'relation': 'PART-OF', 'tokens': 'Amorph recognizes NE items in two stages : dictionary lookup and rule application .', 'h': {'name': 'dictionary lookup', 'pos': [8, 10]}, 't': {'name': 'Amorph', 'pos': [0, 1]}},\n",
        "{'id': 51, 'relation': 'PART-OF', 'tokens': 'In the Object Recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .', 'h': {'name': 'estimating object pose', 'pos': [16, 19]}, 't': {'name': 'Object Recognition task', 'pos': [2, 5]}},\n",
        "{'id': 79, 'relation': 'PART-OF', 'tokens': 'Full digital resolution is maintained even with low-resolution analog-to-digital conversion , owing to random statistics in the analog summation of binary products .', 'h': {'name': 'random statistics', 'pos': [13, 15]}, 't': {'name': 'analog summation of binary products', 'pos': [17, 22]}},\n",
        "{'id': 19, 'relation': 'EVALUATE-FOR', 'tokens': 'Techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches .', 'h': {'name': 'utterances', 'pos': [24, 25]}, 't': {'name': 'trainable components', 'pos': [27, 29]}},\n",
        "{'id': 29, 'relation': 'EVALUATE-FOR', 'tokens': 'The experimental results show that the proposed histogram-based interest point detectors perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of repeatability and distinctiveness .', 'h': {'name': 'repeatability', 'pos': [30, 31]}, 't': {'name': 'histogram-based interest point detectors', 'pos': [7, 11]}},\n",
        "{'id': 34, 'relation': 'EVALUATE-FOR', 'tokens': 'An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings .', 'h': {'name': 'summarization quality', 'pos': [4, 6]}, 't': {'name': 'automatic parse-based evaluation', 'pos': [12, 15]}},\n",
        "{'id': 44, 'relation': 'EVALUATE-FOR', 'tokens': 'Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods .', 'h': {'name': 'ACE corpora', 'pos': [3, 5]}, 't': {'name': 'spectral clustering based approach', 'pos': [8, 12]}},\n",
        "{'id': 45, 'relation': 'EVALUATE-FOR', 'tokens': \"In particular there are three areas of novelty : -LRB- i -RRB- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .\", 'h': {'name': 'robustness', 'pos': [89, 90]}, 't': {'name': \"video sequence '' reillumination '' algorithm\", 'pos': [81, 87]}},\n",
        "{'id': 56, 'relation': 'FEATURE-OF', 'tokens': 'The results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our Horn clauses can be learnt automatically from these features .', 'h': {'name': 'features', 'pos': [5, 6]}, 't': {'name': 'heuristic principles', 'pos': [13, 15]}},\n",
        "{'id': 63, 'relation': 'FEATURE-OF', 'tokens': 'We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses .', 'h': {'name': 'probabilistic Horn clauses', 'pos': [15, 18]}, 't': {'name': 'heuristic principles', 'pos': [5, 7]}},\n",
        "{'id': 71, 'relation': 'FEATURE-OF', 'tokens': 'A separation method is proposed that is nearly statistically efficient -LRB- approaching the corresponding Cramér-Rao lower bound -RRB- , if the separated signals obey the assumed model .', 'h': {'name': 'Cramér-Rao lower bound -RRB-', 'pos': [14, 18]}, 't': {'name': 'separation method', 'pos': [1, 3]}},\n",
        "{'id': 74, 'relation': 'FEATURE-OF', 'tokens': 'We introduce a method to accelerate the evaluation of object detection cascades with the help of a divide-and-conquer procedure in the space of candidate regions .', 'h': {'name': 'space of candidate regions', 'pos': [21, 25]}, 't': {'name': 'divide-and-conquer procedure', 'pos': [17, 19]}},\n",
        "{'id': 84, 'relation': 'FEATURE-OF', 'tokens': 'Experimental results from a real telephone application on a natural number recognition task show an 50 % reduction in recognition errors with a moderate 12 % rejection rate of correct utterances and a low 1.5 % rate of false acceptance .', 'h': {'name': 'natural number recognition task', 'pos': [9, 13]}, 't': {'name': 'telephone application', 'pos': [5, 7]}},\n",
        "{'id': 92, 'relation': 'FEATURE-OF', 'tokens': 'Here , we leverage a logistic stick-breaking representation and recent innovations in Pólya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods , enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead .', 'h': {'name': 'minimal overhead', 'pos': [45, 47]}, 't': {'name': 'Gaussian models', 'pos': [42, 44]}},\n",
        "{'id': 122, 'relation': 'COMPARE', 'tokens': 'Our experiments on real data sets show that the resulting detector is more robust to the choice of training examples , and substantially improves both linear and kernel SVM when trained on 10 positive and 10 negative examples .', 'h': {'name': 'detector', 'pos': [10, 11]}, 't': {'name': 'linear and kernel SVM', 'pos': [25, 29]}},\n",
        "{'id': 164, 'relation': 'COMPARE', 'tokens': 'We consider the problem of computing the Kullback-Leibler distance , also called the relative entropy , between a probabilistic context-free grammar and a probabilistic finite automaton .', 'h': {'name': 'probabilistic context-free grammar', 'pos': [18, 21]}, 't': {'name': 'probabilistic finite automaton', 'pos': [23, 26]}},\n",
        "{'id': 192, 'relation': 'COMPARE', 'tokens': 'Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments .', 'h': {'name': 'technique', 'pos': [1, 2]}, 't': {'name': 'models', 'pos': [15, 16]}},\n",
        "{'id': 232, 'relation': 'COMPARE', 'tokens': 'We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representations .', 'h': {'name': 'this', 'pos': [34, 35]}, 't': {'name': 'object category representations', 'pos': [37, 40]}},\n",
        "{'id': 244, 'relation': 'COMPARE', 'tokens': 'We show that the trainable sentence planner performs better than the rule-based systems and the baselines , and as well as the hand-crafted system .', 'h': {'name': 'trainable sentence planner', 'pos': [4, 7]}, 't': {'name': 'baselines', 'pos': [15, 16]}},\n",
        "{'id': 290, 'relation': 'COMPARE', 'tokens': 'Compared to the exhaustive procedure that thus far is the state-of-the-art for cascade evaluation , the proposed method requires fewer evaluations of the classifier functions , thereby speeding up the search .', 'h': {'name': 'exhaustive procedure', 'pos': [3, 5]}, 't': {'name': 'method', 'pos': [17, 18]}},\n",
        "]"
      ],
      "metadata": {
        "id": "88CQuKi4WA9q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "samples in prompts:\n",
        "\n",
        "Example0:\n",
        "The given sentence is :  We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing.\n",
        "Triples: [lexical similarity , FEATURE-OF , discourse segments]\n",
        "-------------------------------------------------------------------------\n",
        "Example1:\n",
        "The given sentence: Experimental results on our dataset and the public G3D dataset both demonstrate very promising performance of our scheme.\n",
        "Triples: [dataset, CONJUNCTION, G3D dataset]\n",
        "\n",
        "Example2:\n",
        "The given sentence:  We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing.\n",
        "Triples: [lexical similarity , FEATURE-OF , discourse segments]\n",
        "\n",
        "Example3:\n",
        "The given sentence: We propose a novel probabilistic framework for learning visual models of 3D object categories by combining appearance information and geometric constraints.\n",
        "Triples: [geometric constraints, USED-FOR, probabilistic framework]\n",
        "\n",
        "Example4:\n",
        "the given sentence: Unlike existing interest point detectors , which measure pixel-wise differences in image intensity , our detectors incorporate histogram-based representations , and thus can find image regions that present a distinct distribution in the neighborhood.\n",
        "Triples:  [pixel-wise differences in image intensity, EVALUATE-FOR, interest point detectors]\n",
        "\n",
        "Example5:\n",
        "the given sentence: 'We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : SLIPPER , a rule-based learning algorithm , and TiMBL , a memory-based system.\n",
        "Triples:  [rule-based learning algorithm, COMPARE, memory-based system]\n",
        "\n",
        "Example6:\n",
        "the given sentence: However , when the object becomes partially or totally occluded , such local tracking is prone to failure , especially when common prediction techniques like the Kalman filter do not provide a good estimate of object parameters in future frames.\n",
        "Triples:  [Kalman filter, PART-OF, prediction techniques]\n",
        "\n",
        "Example7:\n",
        "the given sentence: With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the Prolog subset of logic.\n",
        "Triples:  [extraposition grammars, HYPONYM-OF, logic-based grammar formalism]\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "n2bU1zE1PtXR",
        "outputId": "997c940f-1d45-4132-996d-d6410d4bc3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nsamples in prompts:\\n\\nExample0:\\nThe given sentence is :  We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing.\\nTriples: [lexical similarity , FEATURE-OF , discourse segments]\\n-------------------------------------------------------------------------\\nExample1:\\nThe given sentence: Experimental results on our dataset and the public G3D dataset both demonstrate very promising performance of our scheme.\\nTriples: [dataset, CONJUNCTION, G3D dataset]\\n\\nExample2:\\nThe given sentence:  We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing.\\nTriples: [lexical similarity , FEATURE-OF , discourse segments]\\n\\nExample3:\\nThe given sentence: We propose a novel probabilistic framework for learning visual models of 3D object categories by combining appearance information and geometric constraints.\\nTriples: [geometric constraints, USED-FOR, probabilistic framework]\\n\\nExample4:\\nthe given sentence: Unlike existing interest point detectors , which measure pixel-wise differences in image intensity , our detectors incorporate histogram-based representations , and thus can find image regions that present a distinct distribution in the neighborhood.\\nTriples:  [pixel-wise differences in image intensity, EVALUATE-FOR, interest point detectors]\\n\\nExample5:\\nthe given sentence: 'We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : SLIPPER , a rule-based learning algorithm , and TiMBL , a memory-based system.\\nTriples:  [rule-based learning algorithm, COMPARE, memory-based system]\\n\\nExample6:\\nthe given sentence: However , when the object becomes partially or totally occluded , such local tracking is prone to failure , especially when common prediction techniques like the Kalman filter do not provide a good estimate of object parameters in future frames.\\nTriples:  [Kalman filter, PART-OF, prediction techniques]\\n\\nExample7:\\nthe given sentence: With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the Prolog subset of logic.\\nTriples:  [extraposition grammars, HYPONYM-OF, logic-based grammar formalism]\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SciERC_list_of_relation = []\n",
        "for item in SciERC_dataset_40samples:\n",
        "  SciERC_list_of_relation.append(item['relation'])\n",
        "\n",
        "SciERC_list_of_relation = list(set(SciERC_list_of_relation))"
      ],
      "metadata": {
        "id": "JGjM_Mvv4T3N"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Setups"
      ],
      "metadata": {
        "id": "xOuee0HF4Xd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_triples(text):\n",
        "    triples = []\n",
        "    pattern = r'\\[([^\\[\\]]+)\\]'\n",
        "    matches = re.findall(pattern, text)\n",
        "    for match in matches:\n",
        "        triple = [elem.strip().strip(\"'\") for elem in match.split(',')]\n",
        "        triples.append(triple)\n",
        "    return triples"
      ],
      "metadata": {
        "id": "stAzmzZvd8C8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TE_prompt_runner(prompt_base, dataset, list_of_relation, num_samples, CoT = False, model = None):\n",
        "  counter = 0\n",
        "  output_list = []\n",
        "  cot_reasoning_list = []\n",
        "  for idx, sample in enumerate(dataset):\n",
        "    prompt = prompt_base.format(list_of_relation, sample['tokens'])\n",
        "    # print(prompt)\n",
        "\n",
        "    try:\n",
        "      if model == 'GPT':\n",
        "        answer = gpt_get_completion(prompt)\n",
        "      else:\n",
        "        prediction = llm.invoke(prompt)\n",
        "        answer =  prediction.content\n",
        "\n",
        "      cot_reasoning_list.append(answer)\n",
        "      output_list.append( {\"id\" : sample[\"id\"], 'Triples': extract_triples(answer)})\n",
        "      counter += 1\n",
        "    except Exception as e:\n",
        "      # print(e)\n",
        "      print('error for sentence: ', sample['tokens'])\n",
        "    if counter == num_samples:\n",
        "      break\n",
        "  if CoT:\n",
        "    return output_list, cot_reasoning_list\n",
        "  return output_list, _"
      ],
      "metadata": {
        "id": "-GS6tBkx3s-N"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EE_output_processing(org_output):\n",
        "  output = copy.deepcopy(org_output)\n",
        "  # remove empty list samples\n",
        "  output = [item for item in output if len(item.keys())>= 2]\n",
        "  return output"
      ],
      "metadata": {
        "id": "204sddU-l01n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gold_samples_extractor(dataset):\n",
        "\n",
        "  gold_samples = {}\n",
        "  for sample in dataset:\n",
        "    # gold_sample = {}\n",
        "    gold_samples[sample['id']] = [sample['h']['name'], sample['relation'],sample['t']['name']]\n",
        "    # gold_samples.append(gold_sample)\n",
        "  return gold_samples"
      ],
      "metadata": {
        "id": "4kofQ-h0fI8A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_evaluation(gold_samples, processed_output, orderless_relations = []):\n",
        "  labels = []\n",
        "  predictions = []\n",
        "\n",
        "  for item in processed_output:\n",
        "    gold_sample_triple = gold_samples[item['id']]\n",
        "\n",
        "    relation = gold_sample_triple[1]\n",
        "    labels.append(relation)\n",
        "    flag = 0\n",
        "    for triple in item['Triples']:\n",
        "      # print(\"{} <-> {}\\n{} <-> {}\\n{} <-> {}\\n *** \\n\".format(triple[0], gold_sample_triple[0], triple[1], gold_sample_triple[1], triple[2], gold_sample_triple[2]))\n",
        "      if triple[0] == gold_sample_triple[0] and triple[1] == gold_sample_triple[1] and triple[2] == gold_sample_triple[2]:\n",
        "        flag = 1\n",
        "      if relation in orderless_relations and triple[2] == gold_sample_triple[0] and triple[1] == gold_sample_triple[1] and triple[0] == gold_sample_triple[2]:\n",
        "        flag = 1\n",
        "    if flag ==1:\n",
        "      predictions.append(relation)\n",
        "    else:\n",
        "      predictions.append('wrong')\n",
        "\n",
        "  return labels, predictions"
      ],
      "metadata": {
        "id": "p2exHFpvhKeD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nonbinary_evaluation(gold_samples, processed_output, orderless_relations = []):\n",
        "  labels = []\n",
        "  predictions = []\n",
        "\n",
        "  for item in processed_output:\n",
        "    gold_sample_triple = gold_samples[item['id']]\n",
        "    relation = gold_sample_triple[1]\n",
        "    triple_label = []\n",
        "    predicts_label = []\n",
        "\n",
        "    for triple in item['Triples']:\n",
        "      flag = 0\n",
        "      if triple[0] == gold_sample_triple[0] and triple[1] == gold_sample_triple[1] and triple[2] == gold_sample_triple[2]:\n",
        "        triple_label.append(relation)\n",
        "        predicts_label.append(relation)\n",
        "        flag = 1\n",
        "\n",
        "      elif relation in orderless_relations and triple[2] == gold_sample_triple[0] and triple[1] == gold_sample_triple[1] and triple[0] == gold_sample_triple[2]:\n",
        "        triple_label.append(relation)\n",
        "        predicts_label.append(relation)\n",
        "        flag = 1\n",
        "\n",
        "      if flag == 0:\n",
        "        triple_label.append(\"unknown\")\n",
        "        predicts_label.append(\"wrong\")\n",
        "\n",
        "    if len(triple_label) == 0:\n",
        "      triple_label.append(\"unknown\")\n",
        "      predicts_label.append(\"wrong\")\n",
        "\n",
        "    if not relation in triple_label:\n",
        "      triple_label[0] = relation\n",
        "\n",
        "    labels.extend(triple_label)\n",
        "    predictions.extend(predicts_label)\n",
        "\n",
        "  return labels, predictions"
      ],
      "metadata": {
        "id": "mUOBjzvdrY8t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TE_metric_calculation(labels, predictions, event_labels):\n",
        "\n",
        "  # when predict an event which is not in gournd truth, we had wrong prediction which decreases our P.\n",
        "  # when we did not predict any event and there is an even in groundtruth, it deacreses our R.\n",
        "  # Thats why we put zeor both in ground truth and pred list to handle this inconsitency of number of extracted events.\n",
        "  micro_p = precision_score(labels,predictions, average='micro')*100.0\n",
        "  micro_r = recall_score(labels,predictions, labels= event_labels, average='micro')*100.0\n",
        "  if micro_p + micro_r != 0:\n",
        "    micro_f1 = (2 * micro_p * micro_r)/(micro_p + micro_r)\n",
        "  else:\n",
        "    micro_f1 = 0.0\n",
        "  # f1_score(labels,predictions, average='micro')*100.0\n",
        "\n",
        "\n",
        "  print(\"Micro_F1:\", round(micro_f1,2))\n",
        "  print(\"Micro_Precision:\", round(micro_p,2))\n",
        "  print(\"Micro_Recall:\", round(micro_r,2))"
      ],
      "metadata": {
        "id": "4TXaozMmn0GU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_output(output, dataset, dataset_gold_samples, nb_predictions, cot_reasoning_list = None):\n",
        "  index = 0\n",
        "  abstained_counter = 0\n",
        "  sample_num = 0\n",
        "  for item in output:\n",
        "    if len(item['Triples']) == 0:\n",
        "      abstained_counter += 1\n",
        "  print(\"Rate of unstructured or abstained output: {}%\".format(abstained_counter/len(output)))\n",
        "  print()\n",
        "  for i, item in enumerate(output):\n",
        "    # sample_num += 1\n",
        "    if len(item['Triples']) == 0:\n",
        "      abstained_counter += 1\n",
        "    for smple in dataset:\n",
        "      if smple['id'] == item['id']:\n",
        "        print(\"Sentence: \", smple['tokens'])\n",
        "\n",
        "    print(\"Gold -----> \", dataset_gold_samples[item['id']] )\n",
        "    print()\n",
        "    if cot_reasoning_list:\n",
        "      print(\"Reasoning ---> \", cot_reasoning_list[i])\n",
        "      print()\n",
        "    for pred in item['Triples']:\n",
        "      print(\"{:90}\".format(str(pred)), end = \" \")\n",
        "      if nb_predictions[index] != \"wrong\":\n",
        "        print(\"*** Correct Prediction ****\", nb_predictions[index])\n",
        "      else:\n",
        "        print()\n",
        "      index += 1\n",
        "\n",
        "    print(\"\\n***\\n\")"
      ],
      "metadata": {
        "id": "jR4tNNiV12Va"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RE_TACRED Experiments"
      ],
      "metadata": {
        "id": "egTRG4rjmnzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RE_TACRED_gold_samples = gold_samples_extractor(RE_TACRED_dataset_40samples)"
      ],
      "metadata": {
        "id": "3htwAMDbFG_e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "67edfa8d-3d0f-4da5-fcfd-462448c41150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RE_TACRED_dataset_40samples' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f53a5a74314e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRE_TACRED_gold_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgold_samples_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRE_TACRED_dataset_40samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'RE_TACRED_dataset_40samples' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Prompt"
      ],
      "metadata": {
        "id": "nAnr8wGvHvlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paper_zero_shot_prompt = '''\n",
        "The list of predicates: {}\n",
        "What Subject-Predicate-Object triples are included in the following sentence? Please return the possible answers according to the list above. Require the answer only in the form: [subject, predicate, object]\n",
        "The given sentence is : Lange was part of one of them , the Maxima experiment , which was led by his former professor Paul Richards of the University of California , Berkeley .\n",
        "Triples: '''"
      ],
      "metadata": {
        "id": "wBEw2goTHye5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = TE_prompt_runner(paper_zero_shot_prompt, RE_TACRED_dataset_40samples, RE_TACRED_list_of_relation, num_samples= 20)"
      ],
      "metadata": {
        "id": "0YL2pMRzIbAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-binary Evaluation\n",
        "nb_labels, nb_predictions = nonbinary_evaluation(RE_TACRED_gold_samples, output)\n",
        "TE_metric_calculation(nb_labels, nb_predictions, RE_TACRED_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "20848267-34f3-4f38-cf0f-e5948fb1d476",
        "id": "ZJjnzLK3IbAV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RE_TACRED_gold_samples' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-eafebde0ba7c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Non-binary Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnb_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonbinary_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRE_TACRED_gold_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mTE_metric_calculation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRE_TACRED_list_of_relation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RE_TACRED_gold_samples' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Evaluation\n",
        "labels, predictions = binary_evaluation(RE_TACRED_gold_samples, output)\n",
        "TE_metric_calculation(labels, predictions, RE_TACRED_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b86174a-3d99-4bed-c648-8344e4c88dbc",
        "id": "6anIonD-IbAV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 0.0\n",
            "Micro_Precision: 0.0\n",
            "Micro_Recall: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_output(output, RE_TACRED_dataset_40samples, RE_TACRED_gold_samples, nb_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d5e511-577c-419f-9112-68af1724506b",
        "id": "OBzAY26eIbAV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate of unstructured or abstained output: 0.0%\n",
            "\n",
            "Sentence:  Chen Tao unleashed the corner , a Botswana defender cleared it out of the box , but Shen was again picked by the loose ball and drove it in with a tight angle to make the breakthrough for China in the 40 minute .\n",
            "Gold ----->  ['Chen Tao', 'no_relation', 'Shen']\n",
            "\n",
            "['Lange', 'per:member_of', 'Maxima experiment']                                            \n",
            "['Maxima experiment', 'per:parents', 'Paul Richards']                                      \n",
            "['Paul Richards', 'org:member_of', 'University of California', 'Berkeley']                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Her recovery from the attack -- the chimp bit and clawed off her face and hands -- was presented to the world via an episode of the `` Oprah Winfrey Show '' in November .\n",
            "Gold ----->  ['her', 'no_relation', 'attack']\n",
            "\n",
            "['Lange', 'per:member_of', 'Maxima experiment']                                            \n",
            "['Maxima experiment', 'per:parents', 'Paul Richards']                                      \n",
            "['Paul Richards', 'org:member_of', 'University of California', 'Berkeley']                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Two women who filed sexual harassment claims : Worked for NRA , where alleged behavior took place .\n",
            "Gold ----->  ['NRA', 'no_relation', 'Two']\n",
            "\n",
            "['Lange', 'per:member_of', 'Maxima experiment']                                            \n",
            "['Maxima experiment', 'per:parents', 'Paul Richards']                                      \n",
            "['Paul Richards', 'org:member_of', 'University of California', 'Berkeley']                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  American Amanda Knox accuses police of forcing false statements out of her after she was arrested over the 2007 sex-murder of her British housemate in central Italy as she takes the stand for the first time .\n",
            "Gold ----->  ['her', 'per:identity', 'Amanda']\n",
            "\n",
            "['Lange', 'per:member_of', 'Maxima experiment']                                            \n",
            "['Maxima experiment', 'per:parents', 'Paul Richards']                                      \n",
            "['Paul Richards', 'org:member_of', 'University of California', 'Berkeley']                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  AIG sold ALICO to MetLife in March .\n",
            "Gold ----->  ['ALICO', 'org:member_of', 'AIG']\n",
            "\n",
            "['Lange', 'per:member_of', 'Maxima experiment']                                            \n",
            "['Maxima experiment', 'per:parents', 'Paul Richards']                                      \n",
            "['Paul Richards', 'org:member_of', 'University of California', 'Berkeley']                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Knox 's mother Edda Mellas was shaking with grief as her stepmother Cassandra Knox moved to comfort her .\n",
            "Gold ----->  ['Knox', 'per:parents', 'Cassandra']\n",
            "\n",
            "['Lange', 'per:member_of', 'Maxima experiment']                                            \n",
            "['Maxima experiment', 'per:parents', 'Paul Richards']                                      \n",
            "['Paul Richards', 'org:member_of', 'University of California', 'Berkeley']                 \n",
            "\n",
            "***\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Designed Prompt"
      ],
      "metadata": {
        "id": "-eWRUtJdM4mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paper_one_shot_prompt = '''\n",
        "The list of predicates : {}\n",
        "What Subject-Predicate-Object triples are included in the following sentence? Please return the answers only in the form: [subject, predicate, object]\n",
        "\n",
        "Example:\n",
        "The given sentence is : Piedra reported to the IRS that his practice gave $ 107,862 to Scientology groups in 2003 .\n",
        "Triples: [his, per:identity, Piedra]\n",
        "\n",
        "The given sentence is : Lange was part of one of them , the Maxima experiment , which was led by his former professor Paul Richards of the University of California , Berkeley .\n",
        "Triples: '''"
      ],
      "metadata": {
        "id": "c_RFS2cRHydV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = TE_prompt_runner(paper_one_shot_prompt, RE_TACRED_dataset_40samples, RE_TACRED_list_of_relation, num_samples= 20)"
      ],
      "metadata": {
        "id": "6ktMMyqVIe1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-binary Evaluation\n",
        "nb_labels, nb_predictions = nonbinary_evaluation(RE_TACRED_gold_samples, output)\n",
        "TE_metric_calculation(nb_labels, nb_predictions, RE_TACRED_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db243aee-a074-4429-cb13-8565eb3e2da1",
        "id": "lcFMlRrDIe1j"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 0.0\n",
            "Micro_Precision: 0.0\n",
            "Micro_Recall: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Evaluation\n",
        "labels, predictions = binary_evaluation(RE_TACRED_gold_samples, output)\n",
        "TE_metric_calculation(labels, predictions, RE_TACRED_list_of_relation)"
      ],
      "metadata": {
        "id": "VyB9Ex8aIe1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_output(output, RE_TACRED_dataset_40samples, RE_TACRED_gold_samples, nb_predictions)"
      ],
      "metadata": {
        "id": "9Tjl4nAgIe1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4JbUuOMvIeJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QRhO3h1RKnk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SciERC Experiments"
      ],
      "metadata": {
        "id": "PJcVyuufHbgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SciERC_gold_samples = gold_samples_extractor(SciERC_dataset_40samples)"
      ],
      "metadata": {
        "id": "4I1bljGQHqQQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SciERC_orderless_list = [\"COMPARE\", \"CONJUNCTION\", \"HYPONYM-OF\"]"
      ],
      "metadata": {
        "id": "akRgu1T_MPnp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Prompt"
      ],
      "metadata": {
        "id": "PIc6bioMmqbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paper_zero_shot_prompt = '''\n",
        "The list of predicates: {}\n",
        "What Subject-Predicate-Object triples are included in the following sentence? Please return the possible answers according to the list above. Require the answer only in the form: [subject, predicate, object]\n",
        "The given sentence is : {}\n",
        "Triples: '''"
      ],
      "metadata": {
        "id": "O4vLyRxA-QMk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = TE_prompt_runner(paper_zero_shot_prompt, SciERC_dataset_40samples, SciERC_list_of_relation, num_samples= 40)"
      ],
      "metadata": {
        "id": "AXXk8TQk-aqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-binary Evaluation\n",
        "nb_labels, nb_predictions = nonbinary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list )\n",
        "TE_metric_calculation(nb_labels, nb_predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkYg-YWg-dGz",
        "outputId": "a0634018-2887-4cb7-b943-0df961330230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 3.045685279187817\n",
            "Micro_Precision: 1.910828025477707\n",
            "Micro_Recall: 7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Evaluation\n",
        "labels, predictions = binary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list)\n",
        "TE_metric_calculation(labels, predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILljQtTH-20n",
        "outputId": "2162dbdd-9bbb-4ddc-88cd-5e7b404ccdb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 7.5\n",
            "Micro_Precision: 7.5\n",
            "Micro_Recall: 7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_output(output, SciERC_dataset_40samples, SciERC_gold_samples, nb_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgdnDUSb-2xH",
        "outputId": "744b43e3-fc1d-4f8f-ec11-ec2f1d37824d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate of unstructured or abstained output: 0.0%\n",
            "\n",
            "Sentence:  We present a novel method for discovering parallel sentences in comparable , non-parallel corpora .\n",
            "Gold ----->  ['method', 'USED-FOR', 'discovering parallel sentences']\n",
            "\n",
            "['method', 'FEATURE-OF', 'novel']                                                          \n",
            "['method', 'USED-FOR', 'discovering']                                                      \n",
            "['sentences', 'PART-OF', 'corpora']                                                        \n",
            "['corpora', 'CONJUNCTION', 'comparable']                                                   \n",
            "['corpora', 'CONJUNCTION', 'non-parallel']                                                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  During normal tracking conditions when the object is visible from frame to frame , local optimization is used to track the local mode of the similarity measure in a parameter space of translation , rotation and scale .\n",
            "Gold ----->  ['parameter space of translation , rotation and scale', 'USED-FOR', 'local mode of the similarity measure']\n",
            "\n",
            "['local optimization', 'USED-FOR', 'track the local mode of the similarity measure']       \n",
            "['local optimization', 'PART-OF', 'local mode of the similarity measure']                  \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  This paper describes a method for incorporating priming into an incremental probabilistic parser .\n",
            "Gold ----->  ['priming', 'USED-FOR', 'incremental probabilistic parser']\n",
            "\n",
            "['method', 'USED-FOR', 'incorporating priming']                                            \n",
            "['method', 'PART-OF', 'incremental probabilistic parser']                                  \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our technique is based on an improved , dynamic-programming , stereo algorithm for efficient novel-view generation .\n",
            "Gold ----->  ['dynamic-programming , stereo algorithm', 'USED-FOR', 'technique']\n",
            "\n",
            "['technique', 'FEATURE-OF', 'improved']                                                    \n",
            "['technique', 'FEATURE-OF', 'dynamic-programming']                                         \n",
            "['technique', 'FEATURE-OF', 'stereo']                                                      \n",
            "['technique', 'USED-FOR', 'efficient novel-view generation']                               \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  A demonstration -LRB- in UNIX -RRB- for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis -LRB- ICALL -RRB- , including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples .\n",
            "Gold ----->  ['lemmatized indexing', 'USED-FOR', 'aligned bilingual corpus']\n",
            "\n",
            "['demonstration', 'USED-FOR', 'Applied Natural Language Processing']                       \n",
            "['components', 'PART-OF', 'intelligent computer-assisted morphological analysis']          \n",
            "['morphological analysis', 'FEATURE-OF', 'intelligent computer-assisted morphological analysis'] \n",
            "['morphological analysis', 'EVALUATE-FOR', 'disambiguated']                                \n",
            "['indexing', 'FEATURE-OF', 'lemmatized indexing']                                          \n",
            "['indexing', 'USED-FOR', 'aligned bilingual corpus of word examples']                      \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We propose a multi-task end-to-end Joint Classification-Regression Recurrent Neural Network to better explore the action type and temporal localiza-tion information .\n",
            "Gold ----->  ['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'USED-FOR', 'action type']\n",
            "\n",
            "['We', 'FEATURE-OF', 'multi-task end-to-end Joint Classification-Regression Recurrent Neural Network'] \n",
            "['We', 'USED-FOR', 'explore the action type and temporal localiza-tion information']       \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .\n",
            "Gold ----->  ['Head Grammars', 'HYPONYM-OF', 'grammatical formalisms']\n",
            "\n",
            "['Tree Adjoining Grammars', 'FEATURE-OF', 'grammatical formalisms']                        \n",
            "['Head Grammars', 'FEATURE-OF', 'grammatical formalisms']                                  \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .\n",
            "Gold ----->  ['dependency grammars', 'HYPONYM-OF', 'grammar formalisms']\n",
            "\n",
            "['this formalism', 'FEATURE-OF', 'elementary']                                             \n",
            "['this formalism', 'FEATURE-OF', 'powerful']                                               \n",
            "['this formalism', 'USED-FOR', 'simulate']                                                 \n",
            "['this formalism', 'USED-FOR', 'strongly simulate']                                        \n",
            "['this formalism', 'USED-FOR', 'simulate many grammar formalisms']                         \n",
            "['this formalism', 'USED-FOR', 'simulate rewriting systems']                               \n",
            "['this formalism', 'USED-FOR', 'simulate dependency grammars']                             \n",
            "['this formalism', 'USED-FOR', 'simulate TAG']                                             \n",
            "['this formalism', 'USED-FOR', 'simulate HPSG']                                            \n",
            "['this formalism', 'USED-FOR', 'simulate LFG']                                             \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We have implemented a restricted domain parser called Plume .\n",
            "Gold ----->  ['Plume', 'HYPONYM-OF', 'restricted domain parser']\n",
            "\n",
            "['We', 'FEATURE-OF', 'restricted domain parser']                                           \n",
            "['We', 'USED-FOR', 'implemented']                                                          \n",
            "['Plume', 'PART-OF', 'restricted domain parser']                                           \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  There are four language pairs currently supported by GLOSSER : English-Bulgarian , English-Estonian , English-Hungarian and French-Dutch .\n",
            "Gold ----->  ['English-Estonian', 'HYPONYM-OF', 'language pairs']\n",
            "\n",
            "['English-Bulgarian', 'FEATURE-OF', 'GLOSSER']                                             \n",
            "['English-Estonian', 'FEATURE-OF', 'GLOSSER']                                              \n",
            "['English-Hungarian', 'FEATURE-OF', 'GLOSSER']                                             \n",
            "['French-Dutch', 'FEATURE-OF', 'GLOSSER']                                                  \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words .\n",
            "Gold ----->  ['Turkish', 'HYPONYM-OF', 'agglutinative language']\n",
            "\n",
            "['Turkish', 'FEATURE-OF', 'agglutinative language']                                        \n",
            "['word structures', 'PART-OF', 'Turkish']                                                  \n",
            "['affixations', 'USED-FOR', 'forming word structures']                                     \n",
            "['suffixes', 'PART-OF', 'affixations']                                                     \n",
            "['root words', 'PART-OF', 'affixations']                                                   \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Thus , in this paper , we study the problem of robust PCA with side information , where both prior structure and features of entities are exploited for recovery .\n",
            "Gold ----->  ['prior structure', 'CONJUNCTION', 'features of entities']\n",
            "\n",
            "['problem', 'FEATURE-OF', 'entities']                                                      \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and reconstruction , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .\n",
            "Gold ----->  ['Multi-view constraints', 'CONJUNCTION', 'normalized representation']\n",
            "\n",
            "['multi-view constraints', 'FEATURE-OF', 'groups of patches']                              \n",
            "['normalized representation', 'FEATURE-OF', 'appearance']                                  \n",
            "['multi-view constraints', 'USED-FOR', 'guide matching and reconstruction']                \n",
            "['multi-view constraints', 'USED-FOR', 'acquisition of true three-dimensional affine and Euclidean models'] \n",
            "['multi-view constraints', 'USED-FOR', 'recognition in a single photograph']               \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of Chinese text available in corpora and on the Web .\n",
            "Gold ----->  ['corpora', 'CONJUNCTION', 'Web']\n",
            "\n",
            "['method', 'FEATURE-OF', 'different way']                                                  \n",
            "['word senses', 'FEATURE-OF', 'lexicalised']                                               \n",
            "['method', 'USED-FOR', 'exploits']                                                         \n",
            "['method', 'USED-FOR', 'takes advantage']                                                  \n",
            "['Chinese text', 'PART-OF', 'corpora']                                                     \n",
            "['Chinese text', 'PART-OF', 'Web']                                                         \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Furthermore , this paper presents a novel algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts -LRB- flicker -RRB- ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .\n",
            "Gold ----->  ['cost aggregation algorithm', 'CONJUNCTION', 'algorithm']\n",
            "\n",
            "['algorithm', 'FEATURE-OF', 'temporal maintenance of a background model']                  \n",
            "['algorithm', 'USED-FOR', 'enhance the rendering of occlusions and reduce temporal artefacts'] \n",
            "['algorithm', 'USED-FOR', 'acts directly on our three-dimensional matching cost space']    \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , dags , graphs , and products of them .\n",
            "Gold ----->  ['strings', 'CONJUNCTION', 'trees']\n",
            "\n",
            "['paper', 'FEATURE-OF', 'generic mathematical formalism']                                  \n",
            "['paper', 'USED-FOR', 'combination']                                                       \n",
            "['paper', 'PART-OF', 'various structures']                                                 \n",
            "['strings', 'PART-OF', 'various structures']                                               \n",
            "['trees', 'PART-OF', 'various structures']                                                 \n",
            "['dags', 'PART-OF', 'various structures']                                                  \n",
            "['graphs', 'PART-OF', 'various structures']                                                \n",
            "['products', 'PART-OF', 'various structures']                                              \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases , referring expressions , and interruptions .\n",
            "Gold ----->  ['referring expressions', 'CONJUNCTION', 'interruptions']\n",
            "\n",
            "['distinction', 'PART-OF', 'components']                                                   \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing -LRB- -LSB- 1 -RSB- -LSB- 2 -RSB- -RRB- .\n",
            "Gold ----->  ['Recognition of proper nouns', 'PART-OF', 'morphological analysis']\n",
            "\n",
            "['Recognition of proper nouns in Japanese text', 'PART-OF', 'the more general problem of morphological analysis in Japanese text processing'] \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  In this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances -LRB- called the linguistic structure -RRB- , a structure of purposes -LRB- called the intentional structure -RRB- , and the state of focus of attention -LRB- called the attentional state -RRB- .\n",
            "Gold ----->  ['intentional structure', 'PART-OF', 'components']\n",
            "\n",
            "['discourse structure', 'PART-OF', 'this theory']                                          \n",
            "['structure of the sequence of utterances', 'PART-OF', 'discourse structure']              \n",
            "['structure of purposes', 'PART-OF', 'discourse structure']                                \n",
            "['state of focus of attention', 'PART-OF', 'discourse structure']                          \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We analyzed eye gaze , head nods and attentional focus in the context of a direction-giving task .\n",
            "Gold ----->  ['attentional focus', 'PART-OF', 'direction-giving task']\n",
            "\n",
            "['eye gaze', 'PART-OF', 'We']                                                              \n",
            "['head nods', 'PART-OF', 'We']                                                             \n",
            "['attentional focus', 'PART-OF', 'We']                                                     \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Amorph recognizes NE items in two stages : dictionary lookup and rule application .\n",
            "Gold ----->  ['dictionary lookup', 'PART-OF', 'Amorph']\n",
            "\n",
            "['Amorph', 'FEATURE-OF', 'NE items']                                                       \n",
            "['dictionary lookup', 'PART-OF', 'Amorph']                                                 *** Correct Prediction **** PART-OF\n",
            "['rule application', 'PART-OF', 'Amorph']                                                  \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  In the Object Recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .\n",
            "Gold ----->  ['estimating object pose', 'PART-OF', 'Object Recognition task']\n",
            "\n",
            "['object pose', 'FEATURE-OF', 'Object Recognition task']                                   \n",
            "['categorization of objects', 'PART-OF', 'Object Recognition task']                        \n",
            "['estimating object pose', 'PART-OF', 'Object Recognition task']                           *** Correct Prediction **** PART-OF\n",
            "['view-invariant representation', 'USED-FOR', 'categorization of objects']                 \n",
            "['representation', 'USED-FOR', 'estimating object pose']                                   \n",
            "['representation', 'FEATURE-OF', 'pose information']                                       \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Full digital resolution is maintained even with low-resolution analog-to-digital conversion , owing to random statistics in the analog summation of binary products .\n",
            "Gold ----->  ['random statistics', 'PART-OF', 'analog summation of binary products']\n",
            "\n",
            "['full digital resolution', 'FEATURE-OF', 'maintained']                                    \n",
            "['low-resolution analog-to-digital conversion', 'USED-FOR', 'maintained']                  \n",
            "['random statistics', 'PART-OF', 'analog summation of binary products']                    *** Correct Prediction **** PART-OF\n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches .\n",
            "Gold ----->  ['utterances', 'EVALUATE-FOR', 'trainable components']\n",
            "\n",
            "['techniques', 'FEATURE-OF', 'modules']                                                    \n",
            "['modules', 'PART-OF', 'natural language generator']                                       \n",
            "['concern', 'EVALUATE-FOR', 'quality']                                                     \n",
            "['quality', 'PART-OF', 'utterances']                                                       \n",
            "['utterances', 'PRODUCED-WITH', 'trainable components']                                    \n",
            "['approaches', 'CONJUNCTION', 'template-based']                                            \n",
            "['approaches', 'CONJUNCTION', 'rule-based']                                                \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  The experimental results show that the proposed histogram-based interest point detectors perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of repeatability and distinctiveness .\n",
            "Gold ----->  ['repeatability', 'EVALUATE-FOR', 'histogram-based interest point detectors']\n",
            "\n",
            "['experimental results', 'FEATURE-OF', 'histogram-based interest point detectors']         \n",
            "['histogram-based interest point detectors', 'USED-FOR', 'matching textured scenes under blur and illumination changes'] \n",
            "['matching textured scenes under blur and illumination changes', 'EVALUATE-FOR', 'repeatability and distinctiveness'] \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings .\n",
            "Gold ----->  ['summarization quality', 'EVALUATE-FOR', 'automatic parse-based evaluation']\n",
            "\n",
            "['experimental evaluation', 'EVALUATE-FOR', 'summarization quality']                       \n",
            "['automatic parse-based evaluation', 'CONJUNCTION', 'manual evaluation']                   \n",
            "['automatic parse-based evaluation', 'COMPARE', 'manual evaluation']                       \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods .\n",
            "Gold ----->  ['ACE corpora', 'EVALUATE-FOR', 'spectral clustering based approach']\n",
            "\n",
            "['experiment results', 'FEATURE-OF', 'ACE corpora']                                        \n",
            "['spectral clustering based approach', 'USED-FOR', 'clustering']                           \n",
            "['spectral clustering based approach', 'COMPARE', 'other clustering methods']              \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  In particular there are three areas of novelty : -LRB- i -RRB- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .\n",
            "Gold ----->  ['robustness', 'EVALUATE-FOR', \"video sequence '' reillumination '' algorithm\"]\n",
            "\n",
            "['photometric model of image formation', 'USED-FOR', 'generalize in the presence of extreme illumination changes'] \n",
            "['smoothness of geodesically local appearance manifold structure', 'USED-FOR', 'achieve invariance to unseen head poses'] \n",
            "[\"accurate video sequence '' reillumination '' algorithm\", 'USED-FOR', 'achieve robustness to face motion patterns in video'] \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  The results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our Horn clauses can be learnt automatically from these features .\n",
            "Gold ----->  ['features', 'FEATURE-OF', 'heuristic principles']\n",
            "\n",
            "['results', 'FEATURE-OF', 'features']                                                      \n",
            "['features', 'USED-FOR', 'formulate']                                                      \n",
            "['features', 'EVALUATE-FOR', 'predictive power']                                           \n",
            "['rules', 'PART-OF', 'Horn clauses']                                                       \n",
            "['rules', 'USED-FOR', 'learnt']                                                            \n",
            "['rules', 'FEATURE-OF', 'features']                                                        \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses .\n",
            "Gold ----->  ['probabilistic Horn clauses', 'FEATURE-OF', 'heuristic principles']\n",
            "\n",
            "['we', 'extract', 'a set of heuristic principles']                                         \n",
            "['we', 'formulate', 'them as probabilistic Horn clauses']                                  \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  A separation method is proposed that is nearly statistically efficient -LRB- approaching the corresponding Cramér-Rao lower bound -RRB- , if the separated signals obey the assumed model .\n",
            "Gold ----->  ['Cramér-Rao lower bound -RRB-', 'FEATURE-OF', 'separation method']\n",
            "\n",
            "['separation method', 'FEATURE-OF', 'statistically efficient']                             \n",
            "['separation method', 'FEATURE-OF', 'approaching the corresponding Cramér-Rao lower bound'] \n",
            "['separation method', 'USED-FOR', 'obey the assumed model']                                \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We introduce a method to accelerate the evaluation of object detection cascades with the help of a divide-and-conquer procedure in the space of candidate regions .\n",
            "Gold ----->  ['space of candidate regions', 'FEATURE-OF', 'divide-and-conquer procedure']\n",
            "\n",
            "['method', 'USED-FOR', 'accelerate']                                                       \n",
            "['method', 'PART-OF', 'object detection cascades']                                         \n",
            "['procedure', 'PART-OF', 'divide-and-conquer']                                             \n",
            "['procedure', 'USED-FOR', 'evaluate']                                                      \n",
            "['regions', 'PART-OF', 'candidate']                                                        \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Experimental results from a real telephone application on a natural number recognition task show an 50 % reduction in recognition errors with a moderate 12 % rejection rate of correct utterances and a low 1.5 % rate of false acceptance .\n",
            "Gold ----->  ['natural number recognition task', 'FEATURE-OF', 'telephone application']\n",
            "\n",
            "['experimental results', 'FEATURE-OF', 'real telephone application']                       \n",
            "['experimental results', 'USED-FOR', 'natural number recognition task']                    \n",
            "['reduction', 'EVALUATE-FOR', 'recognition errors']                                        \n",
            "['reduction', 'PART-OF', 'experimental results']                                           \n",
            "['rejection rate', 'EVALUATE-FOR', 'correct utterances']                                   \n",
            "['rejection rate', 'PART-OF', 'experimental results']                                      \n",
            "['false acceptance', 'EVALUATE-FOR', 'rate']                                               \n",
            "['false acceptance', 'PART-OF', 'experimental results']                                    \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Here , we leverage a logistic stick-breaking representation and recent innovations in Pólya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods , enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead .\n",
            "Gold ----->  ['minimal overhead', 'FEATURE-OF', 'Gaussian models']\n",
            "\n",
            "['multinomial distribution', 'FEATURE-OF', 'latent variables']                             \n",
            "['latent variables', 'PART-OF', 'Gaussian models']                                         \n",
            "['Bayesian inference techniques', 'USED-FOR', 'Gaussian models']                           \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our experiments on real data sets show that the resulting detector is more robust to the choice of training examples , and substantially improves both linear and kernel SVM when trained on 10 positive and 10 negative examples .\n",
            "Gold ----->  ['detector', 'COMPARE', 'linear and kernel SVM']\n",
            "\n",
            "['experiments', 'FEATURE-OF', 'real data sets']                                            \n",
            "['detector', 'FEATURE-OF', 'robustness']                                                   \n",
            "['detector', 'USED-FOR', 'training']                                                       \n",
            "['detector', 'EVALUATE-FOR', 'linear SVM']                                                 \n",
            "['detector', 'EVALUATE-FOR', 'kernel SVM']                                                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We consider the problem of computing the Kullback-Leibler distance , also called the relative entropy , between a probabilistic context-free grammar and a probabilistic finite automaton .\n",
            "Gold ----->  ['probabilistic context-free grammar', 'COMPARE', 'probabilistic finite automaton']\n",
            "\n",
            "['problem', 'FEATURE-OF', 'computing the Kullback-Leibler distance']                       \n",
            "['problem', 'FEATURE-OF', 'computing the relative entropy']                                \n",
            "['Kullback-Leibler distance', 'HYPONYM-OF', 'relative entropy']                            \n",
            "['problem', 'PART-OF', 'computing the Kullback-Leibler distance']                          \n",
            "['problem', 'PART-OF', 'computing the relative entropy']                                   \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments .\n",
            "Gold ----->  ['technique', 'COMPARE', 'models']\n",
            "\n",
            "['Our technique', 'FEATURE-OF', 'substantial improvement']                                 \n",
            "['Our technique', 'USED-FOR', 'paraphrase classification']                                 \n",
            "['Our technique', 'COMPARE', 'all of the other models']                                    \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representations .\n",
            "Gold ----->  ['this', 'COMPARE', 'object category representations']\n",
            "\n",
            "['layers', 'FEATURE-OF', 'CNN models']                                                     \n",
            "['layers', 'COMPARE', 'layers']                                                            \n",
            "['layers', 'REPRESENT', 'object pose information']                                         \n",
            "['layers', 'CONTRADICT', 'object category representations']                                \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We show that the trainable sentence planner performs better than the rule-based systems and the baselines , and as well as the hand-crafted system .\n",
            "Gold ----->  ['trainable sentence planner', 'COMPARE', 'baselines']\n",
            "\n",
            "['trainable sentence planner', 'FEATURE-OF', 'performs better than the rule-based systems and the baselines'] \n",
            "['trainable sentence planner', 'FEATURE-OF', 'as well as the hand-crafted system']         \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Compared to the exhaustive procedure that thus far is the state-of-the-art for cascade evaluation , the proposed method requires fewer evaluations of the classifier functions , thereby speeding up the search .\n",
            "Gold ----->  ['exhaustive procedure', 'COMPARE', 'method']\n",
            "\n",
            "['proposed method', 'COMPARE', 'exhaustive procedure']                                     \n",
            "['exhaustive procedure', 'PART-OF', 'cascade evaluation']                                  \n",
            "['proposed method', 'EVALUATE-FOR', 'classifier functions']                                \n",
            "\n",
            "***\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_one_shot_prompt = '''\n",
        "\n",
        "The list of predicates: {}.\n",
        "What Subject-Predicate-Object triples are included in the following sentence? Please return the possible answers according to the list above. Require the answer only in the form: [subject, predicate, object]\n",
        "\n",
        "Example:\n",
        "The given sentence is :  We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing .\n",
        "Triples: [lexical similarity , FEATURE-OF , discourse segments]\n",
        "\n",
        "The given sentence is : {}\n",
        "Triples: '''"
      ],
      "metadata": {
        "id": "zQ43Y1dRmnbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = TE_prompt_runner(paper_one_shot_prompt, SciERC_dataset_40samples, SciERC_list_of_relation, num_samples= 40)"
      ],
      "metadata": {
        "id": "crFlgCS24ScF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-binary Evaluation\n",
        "nb_labels, nb_predictions = nonbinary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list )\n",
        "TE_metric_calculation(nb_labels, nb_predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXipP0kY4XTL",
        "outputId": "0b7a6d1d-9eb2-412b-fff4-978518e1a428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 2.9197080291970803\n",
            "Micro_Precision: 2.0618556701030926\n",
            "Micro_Recall: 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Evaluation\n",
        "labels, predictions = binary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list)\n",
        "TE_metric_calculation(labels, predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27JeBeLa4XTS",
        "outputId": "01c3cf9a-744e-4d45-9adf-db39aef7c8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 5.0\n",
            "Micro_Precision: 5.0\n",
            "Micro_Recall: 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_output(output, SciERC_dataset_40samples, SciERC_gold_samples, nb_predictions)"
      ],
      "metadata": {
        "id": "s3o9xjB04XTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Designed Prompt"
      ],
      "metadata": {
        "id": "WOZW7-3rFJsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = '''\n",
        "\n",
        "Extract Subject-Predicate-Object triples from the given sentence.\n",
        "\n",
        "Subject:\n",
        "Identify the main entity or entities in the sentence that is/are the primary focus of the relationship described by the predicate. This can be a single word or a multi-word phrase.\n",
        "\n",
        "Predicate:\n",
        "Determine the specific semantic relationship between the subject and the object based on the context of the sentence. The predicate should accurately reflect one of the following relationships predicted by your dataset:\n",
        "list of Predicate: {}.\n",
        "COMPARE: Denotes a comparison between the subject and object.\n",
        "CONJUNCTION: Indicates that the subject and object are connected or combined.\n",
        "PART-OF: Specifies that the subject is a component or part of the object.\n",
        "USED-FOR: Describes the purpose or function of the subject in relation to the object.\n",
        "FEATURE-OF: Highlights a characteristic or attribute of the subject belonging to the object.\n",
        "HYPONYM-OF: Indicates that the subject is a more specific term or subtype of the object.\n",
        "EVALUATE-FOR: Suggests an assessment or evaluation of the subject with respect to the object.\n",
        "Object:\n",
        "Identify the entity or entities that are associated with the subject based on the chosen predicate. The object can represent the entity receiving the comparison, conjunction, part, purpose, feature, or evaluation described by the predicate. This can be a single word or a multi-word phrase indicating the recipient, target, or characteristic of the relationship.\n",
        "Ensure the correct order of subject, predicate, and object is maintained in your extracted triples.\n",
        "\n",
        "Submit your response in the following format: Triples: [subject, predicate, object]\n",
        "If there are multiple possible triples in a sentence, prioritize and return them based on the contextual relevance.\n",
        "\n",
        "Below are some examples to have a better understanding of the task.\n",
        "the given sentence: Unlike existing interest point detectors, which measure pixel-wise differences in image intensity, our detectors incorporate histogram-based representations and thus can find image regions that present a distinct distribution in the neighborhood.\n",
        "Triples:  [pixel-wise differences in image intensity, EVALUATE-FOR, interest point detectors]\n",
        "\n",
        "\n",
        "the given sentence: 'We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : SLIPPER , a rule-based learning algorithm , and TiMBL , a memory-based system.\n",
        "Triples:  [rule-based learning algorithm, COMPARE, memory-based system]\n",
        "\n",
        "\n",
        "the given sentence: However , when the object becomes partially or totally occluded , such local tracking is prone to failure , especially when common prediction techniques like the Kalman filter do not provide a good estimate of object parameters in future frames.\n",
        "Triples:  [Kalman filter, PART-OF, prediction techniques]\n",
        "\n",
        "\n",
        "the given sentence: With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the Prolog subset of logic.\n",
        "Triples:  [extraposition grammars, HYPONYM-OF, logic-based grammar formalism]\n",
        "\n",
        "\n",
        "The given sentence: Experimental results on our dataset and the public G3D dataset both demonstrate very promising performance of our scheme.\n",
        "Triples: [dataset, CONJUNCTION, G3D dataset]\n",
        "\n",
        "\n",
        "The given sentence:  We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing.\n",
        "Triples: [lexical similarity , FEATURE-OF , discourse segments]\n",
        "\n",
        "\n",
        "The given sentence: We propose a novel probabilistic framework for learning visual models of 3D object categories by combining appearance information and geometric constraints.\n",
        "Triples: [geometric constraints, USED-FOR, probabilistic framework]\n",
        "\n",
        "\n",
        "The given sentence: {}\n",
        "Triples:  '''"
      ],
      "metadata": {
        "id": "NGCIETmnpsB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = TE_prompt_runner(few_shot_prompt, SciERC_dataset_40samples, SciERC_list_of_relation, num_samples= 40)"
      ],
      "metadata": {
        "id": "BG0ZFxEOkyzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-binary Evaluation\n",
        "nb_labels, nb_predictions = nonbinary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list)\n",
        "TE_metric_calculation(nb_labels, nb_predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCc2oByx4Kry",
        "outputId": "551e1447-cf9f-4703-a332-ada826f6d97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 5.0\n",
            "Micro_Precision: 5.0\n",
            "Micro_Recall: 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Evaluation\n",
        "labels, predictions = binary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list )\n",
        "TE_metric_calculation(labels, predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruW09auV4Kd5",
        "outputId": "4f61fa50-48d6-4668-875f-60a882f13317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 5.0\n",
            "Micro_Precision: 5.0\n",
            "Micro_Recall: 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_output(output, SciERC_dataset_40samples, SciERC_gold_samples, nb_predictions)"
      ],
      "metadata": {
        "id": "XWcMXefn4RYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### COT Prompting"
      ],
      "metadata": {
        "id": "_tA_q2lJMtkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CoT_prompt = '''\n",
        "\n",
        "Extract Subject-Predicate-Object triples from the given sentence.\n",
        "\n",
        "Subject:\n",
        "Identify the main entity or entities in the sentence that is/are the primary focus of the relationship described by the predicate. This can be a single word or a multi-word phrase.\n",
        "\n",
        "Predicate:\n",
        "Determine the specific semantic relationship between the subject and the object based on the context of the sentence. The predicate should accurately reflect one of the following relationships predicted by your dataset:\n",
        "list of Predicate: {}.\n",
        "COMPARE: Denotes a comparison between the subject and object.\n",
        "CONJUNCTION: Indicates that the subject and object are connected or combined.\n",
        "PART-OF: Specifies that the subject is a component or part of the object.\n",
        "USED-FOR: Describes the purpose or function of the subject in relation to the object.\n",
        "FEATURE-OF: Highlights a characteristic or attribute of the subject belonging to the object.\n",
        "HYPONYM-OF: Indicates that the subject is a more specific term or subtype of the object.\n",
        "EVALUATE-FOR: Suggests an assessment or evaluation of the subject with respect to the object.\n",
        "Object:\n",
        "Identify the entity or entities that are associated with the subject based on the chosen predicate. The object can represent the entity receiving the comparison, conjunction, part, purpose, feature, or evaluation described by the predicate. This can be a single word or a multi-word phrase indicating the recipient, target, or characteristic of the relationship.\n",
        "Ensure the correct order of subject, predicate, and object is maintained in your extracted triples.\n",
        "\n",
        "Submit your response in the following format: Triples: [subject, predicate, object]\n",
        "If there are multiple possible triples in a sentence, prioritize and return them based on the contextual relevance.\n",
        "\n",
        "Below are some examples to have a better understanding of the task.\n",
        "the given sentence: Unlike existing interest point detectors, which measure pixel-wise differences in image intensity, our detectors incorporate histogram-based representations and thus can find image regions that present a distinct distribution in the neighborhood.\n",
        "Triples:  [pixel-wise differences in image intensity, EVALUATE-FOR, interest point detectors]\n",
        "\n",
        "\n",
        "the given sentence: 'We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : SLIPPER , a rule-based learning algorithm , and TiMBL , a memory-based system.\n",
        "Triples:  [rule-based learning algorithm, COMPARE, memory-based system]\n",
        "\n",
        "\n",
        "the given sentence: However , when the object becomes partially or totally occluded , such local tracking is prone to failure , especially when common prediction techniques like the Kalman filter do not provide a good estimate of object parameters in future frames.\n",
        "Triples:  [Kalman filter, PART-OF, prediction techniques]\n",
        "\n",
        "the given sentence: With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the Prolog subset of logic.\n",
        "Triples:  [extraposition grammars, HYPONYM-OF, logic-based grammar formalism]\n",
        "\n",
        "\n",
        "The given sentence: Experimental results on our dataset and the public G3D dataset both demonstrate very promising performance of our scheme.\n",
        "Triples: [dataset, CONJUNCTION, G3D dataset]\n",
        "\n",
        "\n",
        "The given sentence:  We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing.\n",
        "Triples: [lexical similarity , FEATURE-OF , discourse segments]\n",
        "\n",
        "\n",
        "The given sentence: We propose a novel probabilistic framework for learning visual models of 3D object categories by combining appearance information and geometric constraints.\n",
        "Triples: [geometric constraints, USED-FOR, probabilistic framework]\n",
        "\n",
        "Now, Let's extract triples step by step.\n",
        "The given sentence: {}\n",
        "Triples: '''"
      ],
      "metadata": {
        "id": "lq4JSxZFa9jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output, cot_reasoning_list = TE_prompt_runner(CoT_prompt, SciERC_dataset_40samples, SciERC_list_of_relation, num_samples= 40, CoT = True)"
      ],
      "metadata": {
        "id": "ao_37oQBbdqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-binary Evaluation\n",
        "nb_labels, nb_predictions = nonbinary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list)\n",
        "TE_metric_calculation(nb_labels, nb_predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eti0EtI9bh7Q",
        "outputId": "1a1aacd6-f5d6-4620-95ad-fcba79c80288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 5.59\n",
            "Micro_Precision: 3.88\n",
            "Micro_Recall: 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Evaluation\n",
        "labels, predictions = binary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list)\n",
        "TE_metric_calculation(labels, predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEkvaco6biKE",
        "outputId": "6df7fa0b-0c56-4381-bbdf-5e094a2c5af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 10.0\n",
            "Micro_Precision: 10.0\n",
            "Micro_Recall: 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_output(output, SciERC_dataset_40samples, SciERC_gold_samples, nb_predictions, cot_reasoning_list)"
      ],
      "metadata": {
        "id": "S2gx2o8gbmjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phase_two_cot_prompt = '''\n",
        "\n",
        "The list of predicates: {}.\n",
        "\n",
        "step (1): extract the entities from the given sentence. Try to remove adjective form an entity clause.\n",
        "step (2): Identify potential entities between each pair of extracted entities that can share one of the predicates in the given list of predicates. Ensure that the order of subject, predicate, and object is accurately preserved. consider that the order of entitis is matter based on the predicate meaning.\n",
        "step (3): return the triples in the exact format of [head, predicate, tail], separating them with commas.\n",
        "\n",
        "\n",
        "The given sentence is : This paper solves a specialized regression problem to obtain sampling probabilities for records in databases .\n",
        "\n",
        "\n",
        "step (1): Entities: 'paper', 'regression problem', 'sampling probabilities', 'records', 'databases'\n",
        "\n",
        "step (2):\n",
        "'paper' - 'solve' - 'regression problem'\n",
        "'regression problem' - 'obtain' - 'sampling probabilities'\n",
        "'sampling probabilities' - 'for' - 'records'\n",
        "'records' - 'in' - 'databases\n",
        "\n",
        "check possible realtipredicate on based on the give list of predicates:\n",
        "\n",
        "'regression problem' - 'USED-FOR' - 'sampling probabilities'\n",
        "'sampling probabilities' - 'USED-FOR' - 'records'\n",
        "'records' - 'PART-OF' - 'databases\n",
        "\n",
        "step (3):\n",
        "['regression problem', 'USED-FOR', 'sampling probabilities']\n",
        "['sampling probabilities', ' USED-FOR', 'records']\n",
        "['records', 'PART-OF', 'databases']\n",
        "\n",
        "The given sentence is : {}\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "8A1OZuRD3JID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output, cot_reasoning_list = TE_prompt_runner(phase_two_cot_prompt, SciERC_dataset_40samples, SciERC_list_of_relation, num_samples= 40, CoT = True)"
      ],
      "metadata": {
        "id": "lYgMIFiFccQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-binary Evaluation\n",
        "nb_labels, nb_predictions = nonbinary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list)\n",
        "TE_metric_calculation(nb_labels, nb_predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ0Ja8fi3rF9",
        "outputId": "813b2890-89e6-4842-f404-b81d8b2903e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 6.39\n",
            "Micro_Precision: 3.91\n",
            "Micro_Recall: 17.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Evaluation\n",
        "labels, predictions = binary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list)\n",
        "TE_metric_calculation(labels, predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umbpt2ks3vdI",
        "outputId": "d60025b0-f668-432f-cdaf-4fa4e5f2d4d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 17.5\n",
            "Micro_Precision: 17.5\n",
            "Micro_Recall: 17.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_output(output, SciERC_dataset_40samples, SciERC_gold_samples, nb_predictions, cot_reasoning_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGHl43uDhr0Y",
        "outputId": "a58edca0-4d94-4923-ca02-d34143ac9493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate of unstructured or abstained output: 0.0%\n",
            "\n",
            "Sentence:  We present a novel method for discovering parallel sentences in comparable , non-parallel corpora .\n",
            "Gold ----->  ['method', 'USED-FOR', 'discovering parallel sentences']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'method', 'parallel sentences', 'comparable corpora', 'non-parallel corpora'\n",
            "\n",
            "step (2):\n",
            "'method' - 'for' - 'discovering' - 'parallel sentences'\n",
            "'parallel sentences' - 'in' - 'comparable corpora'\n",
            "'comparable corpora' - 'and' - 'non-parallel corpora'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'method' - 'USED-FOR' - 'discovering'\n",
            "'parallel sentences' - 'PART-OF' - 'comparable corpora'\n",
            "'comparable corpora' - 'CONJUNCTION' - 'non-parallel corpora'\n",
            "\n",
            "step (3):\n",
            "['method', 'USED-FOR', 'discovering']\n",
            "['parallel sentences', 'PART-OF', 'comparable corpora']\n",
            "['comparable corpora', 'CONJUNCTION', 'non-parallel corpora']\n",
            "\n",
            "['method', 'USED-FOR', 'discovering']                                                      \n",
            "['parallel sentences', 'PART-OF', 'comparable corpora']                                    \n",
            "['comparable corpora', 'CONJUNCTION', 'non-parallel corpora']                              \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  During normal tracking conditions when the object is visible from frame to frame , local optimization is used to track the local mode of the similarity measure in a parameter space of translation , rotation and scale .\n",
            "Gold ----->  ['parameter space of translation , rotation and scale', 'USED-FOR', 'local mode of the similarity measure']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'normal tracking conditions', 'object', 'frame', 'local optimization', 'local mode', 'similarity measure', 'parameter space', 'translation', 'rotation', 'scale'\n",
            "\n",
            "step (2):\n",
            "'normal tracking conditions' - 'when' - 'object'\n",
            "'object' - 'visible' - 'frame'\n",
            "'local optimization' - 'used' - 'track'\n",
            "'local optimization' - 'of' - 'local mode'\n",
            "'local mode' - 'of' - 'similarity measure'\n",
            "'similarity measure' - 'in' - 'parameter space'\n",
            "'parameter space' - 'of' - 'translation'\n",
            "'parameter space' - 'of' - 'rotation'\n",
            "'parameter space' - 'of' - 'scale'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'local optimization' - 'USED-FOR' - 'track'\n",
            "'local mode' - 'PART-OF' - 'similarity measure'\n",
            "'similarity measure' - 'PART-OF' - 'parameter space'\n",
            "'translation' - 'PART-OF' - 'parameter space'\n",
            "'rotation' - 'PART-OF' - 'parameter space'\n",
            "'scale' - 'PART-OF' - 'parameter space'\n",
            "\n",
            "step (3):\n",
            "['local optimization', 'USED-FOR', 'track']\n",
            "['local mode', 'PART-OF', 'similarity measure']\n",
            "['similarity measure', 'PART-OF', 'parameter space']\n",
            "['translation', 'PART-OF', 'parameter space']\n",
            "['rotation', 'PART-OF', 'parameter space']\n",
            "['scale', 'PART-OF', 'parameter space']\n",
            "\n",
            "['local optimization', 'USED-FOR', 'track']                                                \n",
            "['local mode', 'PART-OF', 'similarity measure']                                            \n",
            "['similarity measure', 'PART-OF', 'parameter space']                                       \n",
            "['translation', 'PART-OF', 'parameter space']                                              \n",
            "['rotation', 'PART-OF', 'parameter space']                                                 \n",
            "['scale', 'PART-OF', 'parameter space']                                                    \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  This paper describes a method for incorporating priming into an incremental probabilistic parser .\n",
            "Gold ----->  ['priming', 'USED-FOR', 'incremental probabilistic parser']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'paper', 'method', 'priming', 'incremental probabilistic parser'\n",
            "\n",
            "step (2):\n",
            "'paper' - 'describe' - 'method'\n",
            "'method' - 'for' - 'priming'\n",
            "'priming' - 'into' - 'incremental probabilistic parser'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'method' - 'USED-FOR' - 'priming'\n",
            "'priming' - 'PART-OF' - 'incremental probabilistic parser'\n",
            "\n",
            "step (3):\n",
            "['method', 'USED-FOR', 'priming']\n",
            "['priming', 'PART-OF', 'incremental probabilistic parser']\n",
            "\n",
            "['method', 'USED-FOR', 'priming']                                                          \n",
            "['priming', 'PART-OF', 'incremental probabilistic parser']                                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our technique is based on an improved , dynamic-programming , stereo algorithm for efficient novel-view generation .\n",
            "Gold ----->  ['dynamic-programming , stereo algorithm', 'USED-FOR', 'technique']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'technique', 'stereo algorithm', 'novel-view generation'\n",
            "\n",
            "step (2):\n",
            "'technique' - 'based' - 'stereo algorithm'\n",
            "'stereo algorithm' - 'for' - 'novel-view generation'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'technique' - 'USED-FOR' - 'stereo algorithm'\n",
            "'stereo algorithm' - 'USED-FOR' - 'novel-view generation'\n",
            "\n",
            "step (3):\n",
            "['technique', 'USED-FOR', 'stereo algorithm']\n",
            "['stereo algorithm', 'USED-FOR', 'novel-view generation']\n",
            "\n",
            "['technique', 'USED-FOR', 'stereo algorithm']                                              \n",
            "['stereo algorithm', 'USED-FOR', 'novel-view generation']                                  \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  A demonstration -LRB- in UNIX -RRB- for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis -LRB- ICALL -RRB- , including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples .\n",
            "Gold ----->  ['lemmatized indexing', 'USED-FOR', 'aligned bilingual corpus']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'demonstration', 'UNIX', 'Applied Natural Language Processing', 'components', 'novel technical uses', 'intelligent computer-assisted morphological analysis', 'ICALL', 'disambiguated morphological analysis', 'lemmatized indexing', 'aligned bilingual corpus', 'word examples'\n",
            "\n",
            "step (2):\n",
            "'demonstration' - 'for' - 'Applied Natural Language Processing'\n",
            "'components' - 'put' - 'novel technical uses'\n",
            "'components' - 'in' - 'intelligent computer-assisted morphological analysis'\n",
            "'intelligent computer-assisted morphological analysis' - 'for' - 'ICALL'\n",
            "'disambiguated morphological analysis' - 'for' - 'aligned bilingual corpus'\n",
            "'lemmatized indexing' - 'for' - 'aligned bilingual corpus'\n",
            "'aligned bilingual corpus' - 'of' - 'word examples'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'components' - 'USED-FOR' - 'novel technical uses'\n",
            "'components' - 'PART-OF' - 'intelligent computer-assisted morphological analysis'\n",
            "'intelligent computer-assisted morphological analysis' - 'HYPONYM-OF' - 'ICALL'\n",
            "'disambiguated morphological analysis' - 'USED-FOR' - 'aligned bilingual corpus'\n",
            "'lemmatized indexing' - 'USED-FOR' - 'aligned bilingual corpus'\n",
            "'aligned bilingual corpus' - 'PART-OF' - 'word examples'\n",
            "\n",
            "step (3):\n",
            "['components', 'USED-FOR', 'novel technical uses']\n",
            "['components', 'PART-OF', 'intelligent computer-assisted morphological analysis']\n",
            "['intelligent computer-assisted morphological analysis', 'HYPONYM-OF', 'ICALL']\n",
            "['disambiguated morphological analysis', 'USED-FOR', 'aligned bilingual corpus']\n",
            "['lemmatized indexing', 'USED-FOR', 'aligned bilingual corpus']\n",
            "['aligned bilingual corpus', 'PART-OF', 'word examples']\n",
            "\n",
            "['components', 'USED-FOR', 'novel technical uses']                                         \n",
            "['components', 'PART-OF', 'intelligent computer-assisted morphological analysis']          \n",
            "['intelligent computer-assisted morphological analysis', 'HYPONYM-OF', 'ICALL']            \n",
            "['disambiguated morphological analysis', 'USED-FOR', 'aligned bilingual corpus']           \n",
            "['lemmatized indexing', 'USED-FOR', 'aligned bilingual corpus']                            *** Correct Prediction **** USED-FOR\n",
            "['aligned bilingual corpus', 'PART-OF', 'word examples']                                   \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We propose a multi-task end-to-end Joint Classification-Regression Recurrent Neural Network to better explore the action type and temporal localiza-tion information .\n",
            "Gold ----->  ['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'USED-FOR', 'action type']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'action type', 'temporal localization information'\n",
            "\n",
            "step (2):\n",
            "'multi-task end-to-end Joint Classification-Regression Recurrent Neural Network' - 'explore' - 'action type'\n",
            "'multi-task end-to-end Joint Classification-Regression Recurrent Neural Network' - 'explore' - 'temporal localization information'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'multi-task end-to-end Joint Classification-Regression Recurrent Neural Network' - 'USED-FOR' - 'action type'\n",
            "'multi-task end-to-end Joint Classification-Regression Recurrent Neural Network' - 'USED-FOR' - 'temporal localization information'\n",
            "\n",
            "step (3):\n",
            "['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'USED-FOR', 'action type']\n",
            "['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'USED-FOR', 'temporal localization information']\n",
            "\n",
            "['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'USED-FOR', 'action type'] *** Correct Prediction **** USED-FOR\n",
            "['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'USED-FOR', 'temporal localization information'] \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .\n",
            "Gold ----->  ['Head Grammars', 'HYPONYM-OF', 'grammatical formalisms']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'relationship', 'Tree Adjoining Grammars', 'Head Grammars'\n",
            "\n",
            "step (2):\n",
            "'relationship' - 'between' - 'Tree Adjoining Grammars'\n",
            "'relationship' - 'between' - 'Head Grammars'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'relationship' - 'COMPARE' - 'Tree Adjoining Grammars'\n",
            "'relationship' - 'COMPARE' - 'Head Grammars'\n",
            "\n",
            "step (3):\n",
            "['relationship', 'COMPARE', 'Tree Adjoining Grammars']\n",
            "['relationship', 'COMPARE', 'Head Grammars']\n",
            "\n",
            "['relationship', 'COMPARE', 'Tree Adjoining Grammars']                                     \n",
            "['relationship', 'COMPARE', 'Head Grammars']                                               \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .\n",
            "Gold ----->  ['dependency grammars', 'HYPONYM-OF', 'grammar formalisms']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'formalism', 'rewriting systems', 'dependency grammars', 'TAG', 'HPSG', 'LFG'\n",
            "\n",
            "step (2):\n",
            "'formalism' - 'simulate' - 'rewriting systems'\n",
            "'formalism' - 'simulate' - 'dependency grammars'\n",
            "'formalism' - 'simulate' - 'TAG'\n",
            "'formalism' - 'simulate' - 'HPSG'\n",
            "'formalism' - 'simulate' - 'LFG'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'formalism' - 'USED-FOR' - 'rewriting systems'\n",
            "'formalism' - 'USED-FOR' - 'dependency grammars'\n",
            "'formalism' - 'USED-FOR' - 'TAG'\n",
            "'formalism' - 'USED-FOR' - 'HPSG'\n",
            "'formalism' - 'USED-FOR' - 'LFG'\n",
            "\n",
            "step (3):\n",
            "['formalism', 'USED-FOR', 'rewriting systems']\n",
            "['formalism', 'USED-FOR', 'dependency grammars']\n",
            "['formalism', 'USED-FOR', 'TAG']\n",
            "['formalism', 'USED-FOR', 'HPSG']\n",
            "['formalism', 'USED-FOR', 'LFG']\n",
            "\n",
            "['formalism', 'USED-FOR', 'rewriting systems']                                             \n",
            "['formalism', 'USED-FOR', 'dependency grammars']                                           \n",
            "['formalism', 'USED-FOR', 'TAG']                                                           \n",
            "['formalism', 'USED-FOR', 'HPSG']                                                          \n",
            "['formalism', 'USED-FOR', 'LFG']                                                           \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We have implemented a restricted domain parser called Plume .\n",
            "Gold ----->  ['Plume', 'HYPONYM-OF', 'restricted domain parser']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'We', 'restricted domain parser', 'Plume'\n",
            "\n",
            "step (2):\n",
            "'We' - 'implement' - 'restricted domain parser'\n",
            "'restricted domain parser' - 'called' - 'Plume'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'We' - 'EVALUATE-FOR' - 'restricted domain parser'\n",
            "'restricted domain parser' - 'HYPONYM-OF' - 'Plume'\n",
            "\n",
            "step (3):\n",
            "['We', 'EVALUATE-FOR', 'restricted domain parser']\n",
            "['restricted domain parser', 'HYPONYM-OF', 'Plume']\n",
            "\n",
            "['We', 'EVALUATE-FOR', 'restricted domain parser']                                         \n",
            "['restricted domain parser', 'HYPONYM-OF', 'Plume']                                        *** Correct Prediction **** HYPONYM-OF\n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  There are four language pairs currently supported by GLOSSER : English-Bulgarian , English-Estonian , English-Hungarian and French-Dutch .\n",
            "Gold ----->  ['English-Estonian', 'HYPONYM-OF', 'language pairs']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'GLOSSER', 'language pairs', 'English', 'Bulgarian', 'Estonian', 'Hungarian', 'French', 'Dutch'\n",
            "\n",
            "step (2):\n",
            "'GLOSSER' - 'support' - 'language pairs'\n",
            "'language pairs' - 'include' - 'English'\n",
            "'language pairs' - 'include' - 'Bulgarian'\n",
            "'language pairs' - 'include' - 'Estonian'\n",
            "'language pairs' - 'include' - 'Hungarian'\n",
            "'language pairs' - 'include' - 'French'\n",
            "'language pairs' - 'include' - 'Dutch'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'GLOSSER' - 'USED-FOR' - 'language pairs'\n",
            "'language pairs' - 'PART-OF' - 'English'\n",
            "'language pairs' - 'PART-OF' - 'Bulgarian'\n",
            "'language pairs' - 'PART-OF' - 'Estonian'\n",
            "'language pairs' - 'PART-OF' - 'Hungarian'\n",
            "'language pairs' - 'PART-OF' - 'French'\n",
            "'language pairs' - 'PART-OF' - 'Dutch'\n",
            "\n",
            "step (3):\n",
            "['GLOSSER', 'USED-FOR', 'language pairs']\n",
            "['language pairs', 'PART-OF', 'English']\n",
            "['language pairs', 'PART-OF', 'Bulgarian']\n",
            "['language pairs', 'PART-OF', 'Estonian']\n",
            "['language pairs', 'PART-OF', 'Hungarian']\n",
            "['language pairs', 'PART-OF', 'French']\n",
            "['language pairs', 'PART-OF', 'Dutch']\n",
            "\n",
            "['GLOSSER', 'USED-FOR', 'language pairs']                                                  \n",
            "['language pairs', 'PART-OF', 'English']                                                   \n",
            "['language pairs', 'PART-OF', 'Bulgarian']                                                 \n",
            "['language pairs', 'PART-OF', 'Estonian']                                                  \n",
            "['language pairs', 'PART-OF', 'Hungarian']                                                 \n",
            "['language pairs', 'PART-OF', 'French']                                                    \n",
            "['language pairs', 'PART-OF', 'Dutch']                                                     \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words .\n",
            "Gold ----->  ['Turkish', 'HYPONYM-OF', 'agglutinative language']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'Turkish', 'agglutinative language', 'word structures', 'productive affixations', 'derivational suffixes', 'inflectional suffixes', 'root words'\n",
            "\n",
            "step (2):\n",
            "'Turkish' - 'is' - 'agglutinative language'\n",
            "'word structures' - 'formed' - 'productive affixations'\n",
            "'productive affixations' - 'of' - 'derivational suffixes'\n",
            "'productive affixations' - 'of' - 'inflectional suffixes'\n",
            "'productive affixations' - 'to' - 'root words'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'Turkish' - 'FEATURE-OF' - 'agglutinative language'\n",
            "'word structures' - 'USED-FOR' - 'productive affixations'\n",
            "'derivational suffixes' - 'HYPONYM-OF' - 'productive affixations'\n",
            "'inflectional suffixes' - 'HYPONYM-OF' - 'productive affixations'\n",
            "'productive affixations' - 'PART-OF' - 'word structures'\n",
            "\n",
            "step (3):\n",
            "['Turkish', 'FEATURE-OF', 'agglutinative language']\n",
            "['word structures', 'USED-FOR', 'productive affixations']\n",
            "['derivational suffixes', 'HYPONYM-OF', 'productive affixations']\n",
            "['inflectional suffixes', 'HYPONYM-OF', 'productive affixations']\n",
            "['productive affixations', 'PART-OF', 'word structures']\n",
            "\n",
            "['Turkish', 'FEATURE-OF', 'agglutinative language']                                        \n",
            "['word structures', 'USED-FOR', 'productive affixations']                                  \n",
            "['derivational suffixes', 'HYPONYM-OF', 'productive affixations']                          \n",
            "['inflectional suffixes', 'HYPONYM-OF', 'productive affixations']                          \n",
            "['productive affixations', 'PART-OF', 'word structures']                                   \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Thus , in this paper , we study the problem of robust PCA with side information , where both prior structure and features of entities are exploited for recovery .\n",
            "Gold ----->  ['prior structure', 'CONJUNCTION', 'features of entities']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'paper', 'problem', 'robust PCA', 'side information', 'prior structure', 'features', 'entities', 'recovery'\n",
            "\n",
            "step (2):\n",
            "'paper' - 'study' - 'problem'\n",
            "'problem' - 'of' - 'robust PCA'\n",
            "'robust PCA' - 'with' - 'side information'\n",
            "'side information' - 'exploited' - 'prior structure'\n",
            "'side information' - 'exploited' - 'features'\n",
            "'features' - 'of' - 'entities'\n",
            "'features' - 'exploited' - 'recovery'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'problem' - 'HYPONYM-OF' - 'robust PCA'\n",
            "'robust PCA' - 'USED-FOR' - 'recovery'\n",
            "'side information' - 'USED-FOR' - 'prior structure'\n",
            "'side information' - 'USED-FOR' - 'features'\n",
            "'features' - 'FEATURE-OF' - 'entities'\n",
            "\n",
            "step (3):\n",
            "['problem', 'HYPONYM-OF', 'robust PCA']\n",
            "['robust PCA', 'USED-FOR', 'recovery']\n",
            "['side information', 'USED-FOR', 'prior structure']\n",
            "['side information', 'USED-FOR', 'features']\n",
            "['features', 'FEATURE-OF', 'entities']\n",
            "\n",
            "['problem', 'HYPONYM-OF', 'robust PCA']                                                    \n",
            "['robust PCA', 'USED-FOR', 'recovery']                                                     \n",
            "['side information', 'USED-FOR', 'prior structure']                                        \n",
            "['side information', 'USED-FOR', 'features']                                               \n",
            "['features', 'FEATURE-OF', 'entities']                                                     \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and reconstruction , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .\n",
            "Gold ----->  ['Multi-view constraints', 'CONJUNCTION', 'normalized representation']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'multi-view constraints', 'groups of patches', 'normalized representation', 'appearance', 'matching', 'reconstruction', 'true three-dimensional affine models', 'Euclidean models', 'multiple images', 'single photograph', 'arbitrary viewpoint'\n",
            "\n",
            "step (2):\n",
            "'multi-view constraints' - 'associated with' - 'groups of patches'\n",
            "'groups of patches' - 'combined with' - 'normalized representation'\n",
            "'normalized representation' - 'of' - 'appearance'\n",
            "'appearance' - 'guide' - 'matching'\n",
            "'matching' - 'and' - 'reconstruction'\n",
            "'reconstruction' - 'of' - 'true three-dimensional affine models'\n",
            "'true three-dimensional affine models' - 'and' - 'Euclidean models'\n",
            "'Euclidean models' - 'from' - 'multiple images'\n",
            "'multiple images' - 'and' - 'single photograph'\n",
            "'single photograph' - 'taken from' - 'arbitrary viewpoint'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'multi-view constraints' - 'FEATURE-OF' - 'groups of patches'\n",
            "'groups of patches' - 'USED-FOR' - 'normalized representation'\n",
            "'normalized representation' - 'FEATURE-OF' - 'appearance'\n",
            "'appearance' - 'USED-FOR' - 'matching'\n",
            "'matching' - 'CONJUNCTION' - 'reconstruction'\n",
            "'reconstruction' - 'EVALUATE-FOR' - 'true three-dimensional affine models'\n",
            "'true three-dimensional affine models' - 'CONJUNCTION' - 'Euclidean models'\n",
            "'Euclidean models' - 'OBTAINED-FROM' - 'multiple images'\n",
            "'multiple images' - 'CONJUNCTION' - 'single photograph'\n",
            "'single photograph' - 'FEATURE-OF' - 'arbitrary viewpoint'\n",
            "\n",
            "step (3):\n",
            "['multi-view constraints', 'FEATURE-OF', 'groups of patches']\n",
            "['groups of patches', 'USED-FOR', 'normalized representation']\n",
            "['normalized representation', 'FEATURE-OF', 'appearance']\n",
            "['appearance', 'USED-FOR', 'matching']\n",
            "['matching', 'CONJUNCTION', 'reconstruction']\n",
            "['reconstruction', 'EVALUATE-FOR', 'true three-dimensional affine models']\n",
            "['true three-dimensional affine models', 'CONJUNCTION', 'Euclidean models']\n",
            "['Euclidean models', 'OBTAINED-FROM', 'multiple images']\n",
            "['multiple images', 'CONJUNCTION', 'single photograph']\n",
            "['single photograph', 'FEATURE-OF', 'arbitrary viewpoint']\n",
            "\n",
            "['multi-view constraints', 'FEATURE-OF', 'groups of patches']                              \n",
            "['groups of patches', 'USED-FOR', 'normalized representation']                             \n",
            "['normalized representation', 'FEATURE-OF', 'appearance']                                  \n",
            "['appearance', 'USED-FOR', 'matching']                                                     \n",
            "['matching', 'CONJUNCTION', 'reconstruction']                                              \n",
            "['reconstruction', 'EVALUATE-FOR', 'true three-dimensional affine models']                 \n",
            "['true three-dimensional affine models', 'CONJUNCTION', 'Euclidean models']                \n",
            "['Euclidean models', 'OBTAINED-FROM', 'multiple images']                                   \n",
            "['multiple images', 'CONJUNCTION', 'single photograph']                                    \n",
            "['single photograph', 'FEATURE-OF', 'arbitrary viewpoint']                                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of Chinese text available in corpora and on the Web .\n",
            "Gold ----->  ['corpora', 'CONJUNCTION', 'Web']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'method', 'word senses', 'English', 'Chinese', 'corpora', 'Web'\n",
            "\n",
            "step (2):\n",
            "'method' - 'take advantage of' - 'word senses'\n",
            "'word senses' - 'lexicalised in' - 'English'\n",
            "'word senses' - 'lexicalised in' - 'Chinese'\n",
            "'method' - 'exploit' - 'corpora'\n",
            "'method' - 'exploit' - 'Web'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'word senses' - 'FEATURE-OF' - 'English'\n",
            "'word senses' - 'FEATURE-OF' - 'Chinese'\n",
            "'method' - 'USED-FOR' - 'corpora'\n",
            "'method' - 'USED-FOR' - 'Web'\n",
            "\n",
            "step (3):\n",
            "['word senses', 'FEATURE-OF', 'English']\n",
            "['word senses', 'FEATURE-OF', 'Chinese']\n",
            "['method', 'USED-FOR', 'corpora']\n",
            "['method', 'USED-FOR', 'Web']\n",
            "\n",
            "['word senses', 'FEATURE-OF', 'English']                                                   \n",
            "['word senses', 'FEATURE-OF', 'Chinese']                                                   \n",
            "['method', 'USED-FOR', 'corpora']                                                          \n",
            "['method', 'USED-FOR', 'Web']                                                              \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Furthermore , this paper presents a novel algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts -LRB- flicker -RRB- ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .\n",
            "Gold ----->  ['cost aggregation algorithm', 'CONJUNCTION', 'algorithm']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'paper', 'algorithm', 'temporal maintenance', 'background model', 'rendering', 'occlusions', 'temporal artefacts', 'flicker', 'cost aggregation algorithm', 'three-dimensional matching cost space'\n",
            "\n",
            "step (2):\n",
            "'paper' - 'present' - 'algorithm'\n",
            "'algorithm' - 'for' - 'temporal maintenance'\n",
            "'temporal maintenance' - 'of' - 'background model'\n",
            "'background model' - 'to' - 'enhance' - 'rendering'\n",
            "'rendering' - 'of' - 'occlusions'\n",
            "'occlusions' - 'and' - 'reduce' - 'temporal artefacts'\n",
            "'temporal artefacts' - 'flicker'\n",
            "'cost aggregation algorithm' - 'act' - 'on' - 'three-dimensional matching cost space'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'algorithm' - 'USED-FOR' - 'temporal maintenance'\n",
            "'temporal maintenance' - 'PART-OF' - 'background model'\n",
            "'background model' - 'USED-FOR' - 'rendering'\n",
            "'rendering' - 'USED-FOR' - 'occlusions'\n",
            "'occlusions' - 'CONJUNCTION' - 'temporal artefacts'\n",
            "'cost aggregation algorithm' - 'USED-FOR' - 'three-dimensional matching cost space'\n",
            "\n",
            "step (3):\n",
            "['algorithm', 'USED-FOR', 'temporal maintenance']\n",
            "['temporal maintenance', 'PART-OF', 'background model']\n",
            "['background model', 'USED-FOR', 'rendering']\n",
            "['rendering', 'USED-FOR', 'occlusions']\n",
            "['occlusions', 'CONJUNCTION', 'temporal artefacts']\n",
            "['cost aggregation algorithm', 'USED-FOR', 'three-dimensional matching cost space']\n",
            "\n",
            "['algorithm', 'USED-FOR', 'temporal maintenance']                                          \n",
            "['temporal maintenance', 'PART-OF', 'background model']                                    \n",
            "['background model', 'USED-FOR', 'rendering']                                              \n",
            "['rendering', 'USED-FOR', 'occlusions']                                                    \n",
            "['occlusions', 'CONJUNCTION', 'temporal artefacts']                                        \n",
            "['cost aggregation algorithm', 'USED-FOR', 'three-dimensional matching cost space']        \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , dags , graphs , and products of them .\n",
            "Gold ----->  ['strings', 'CONJUNCTION', 'trees']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'paper', 'mathematical formalism', 'strings', 'trees', 'dags', 'graphs', 'products'\n",
            "\n",
            "step (2):\n",
            "'paper' - 'propose' - 'mathematical formalism'\n",
            "'mathematical formalism' - 'for' - 'combination'\n",
            "'combination' - 'of' - 'strings'\n",
            "'combination' - 'of' - 'trees'\n",
            "'combination' - 'of' - 'dags'\n",
            "'combination' - 'of' - 'graphs'\n",
            "'combination' - 'of' - 'products'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'mathematical formalism' - 'USED-FOR' - 'combination'\n",
            "'combination' - 'FEATURE-OF' - 'strings'\n",
            "'combination' - 'FEATURE-OF' - 'trees'\n",
            "'combination' - 'FEATURE-OF' - 'dags'\n",
            "'combination' - 'FEATURE-OF' - 'graphs'\n",
            "'combination' - 'FEATURE-OF' - 'products'\n",
            "\n",
            "step (3):\n",
            "['mathematical formalism', 'USED-FOR', 'combination']\n",
            "['combination', 'FEATURE-OF', 'strings']\n",
            "['combination', 'FEATURE-OF', 'trees']\n",
            "['combination', 'FEATURE-OF', 'dags']\n",
            "['combination', 'FEATURE-OF', 'graphs']\n",
            "['combination', 'FEATURE-OF', 'products']\n",
            "\n",
            "['mathematical formalism', 'USED-FOR', 'combination']                                      \n",
            "['combination', 'FEATURE-OF', 'strings']                                                   \n",
            "['combination', 'FEATURE-OF', 'trees']                                                     \n",
            "['combination', 'FEATURE-OF', 'dags']                                                      \n",
            "['combination', 'FEATURE-OF', 'graphs']                                                    \n",
            "['combination', 'FEATURE-OF', 'products']                                                  \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases , referring expressions , and interruptions .\n",
            "Gold ----->  ['referring expressions', 'CONJUNCTION', 'interruptions']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'distinction', 'components', 'cue phrases', 'referring expressions', 'interruptions', 'explanation', 'discourse phenomena'\n",
            "\n",
            "step (2):\n",
            "'distinction' - 'among' - 'components'\n",
            "'distinction' - 'provide' - 'explanation'\n",
            "'explanation' - 'of' - 'discourse phenomena'\n",
            "'discourse phenomena' - 'include' - 'cue phrases'\n",
            "'discourse phenomena' - 'include' - 'referring expressions'\n",
            "'discourse phenomena' - 'include' - 'interruptions'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'distinction' - 'FEATURE-OF' - 'components'\n",
            "'distinction' - 'USED-FOR' - 'explanation'\n",
            "'explanation' - 'EVALUATE-FOR' - 'discourse phenomena'\n",
            "'discourse phenomena' - 'CONJUNCTION' - 'cue phrases'\n",
            "'discourse phenomena' - 'CONJUNCTION' - 'referring expressions'\n",
            "'discourse phenomena' - 'CONJUNCTION' - 'interruptions'\n",
            "\n",
            "step (3):\n",
            "['distinction', 'FEATURE-OF', 'components']\n",
            "['distinction', 'USED-FOR', 'explanation']\n",
            "['explanation', 'EVALUATE-FOR', 'discourse phenomena']\n",
            "['discourse phenomena', 'CONJUNCTION', 'cue phrases']\n",
            "['discourse phenomena', 'CONJUNCTION', 'referring expressions']\n",
            "['discourse phenomena', 'CONJUNCTION', 'interruptions']\n",
            "\n",
            "['distinction', 'FEATURE-OF', 'components']                                                \n",
            "['distinction', 'USED-FOR', 'explanation']                                                 \n",
            "['explanation', 'EVALUATE-FOR', 'discourse phenomena']                                     \n",
            "['discourse phenomena', 'CONJUNCTION', 'cue phrases']                                      \n",
            "['discourse phenomena', 'CONJUNCTION', 'referring expressions']                            \n",
            "['discourse phenomena', 'CONJUNCTION', 'interruptions']                                    \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing -LRB- -LSB- 1 -RSB- -LSB- 2 -RSB- -RRB- .\n",
            "Gold ----->  ['Recognition of proper nouns', 'PART-OF', 'morphological analysis']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'Recognition', 'proper nouns', 'Japanese text', 'part', 'general problem', 'morphological analysis', 'Japanese text processing'\n",
            "\n",
            "step (2):\n",
            "'Recognition' - 'of' - 'proper nouns'\n",
            "'proper nouns' - 'in' - 'Japanese text'\n",
            "'Recognition' - 'as' - 'part'\n",
            "'part' - 'of' - 'general problem'\n",
            "'general problem' - 'of' - 'morphological analysis'\n",
            "'morphological analysis' - 'in' - 'Japanese text processing'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'Recognition' - 'FEATURE-OF' - 'proper nouns'\n",
            "'proper nouns' - 'PART-OF' - 'Japanese text'\n",
            "'Recognition' - 'PART-OF' - 'general problem'\n",
            "'general problem' - 'HYPONYM-OF' - 'morphological analysis'\n",
            "'morphological analysis' - 'PART-OF' - 'Japanese text processing'\n",
            "\n",
            "step (3):\n",
            "['Recognition', 'FEATURE-OF', 'proper nouns']\n",
            "['proper nouns', 'PART-OF', 'Japanese text']\n",
            "['Recognition', 'PART-OF', 'general problem']\n",
            "['general problem', 'HYPONYM-OF', 'morphological analysis']\n",
            "['morphological analysis', 'PART-OF', 'Japanese text processing']\n",
            "\n",
            "['Recognition', 'FEATURE-OF', 'proper nouns']                                              \n",
            "['proper nouns', 'PART-OF', 'Japanese text']                                               \n",
            "['Recognition', 'PART-OF', 'general problem']                                              \n",
            "['general problem', 'HYPONYM-OF', 'morphological analysis']                                \n",
            "['morphological analysis', 'PART-OF', 'Japanese text processing']                          \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  In this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances -LRB- called the linguistic structure -RRB- , a structure of purposes -LRB- called the intentional structure -RRB- , and the state of focus of attention -LRB- called the attentional state -RRB- .\n",
            "Gold ----->  ['intentional structure', 'PART-OF', 'components']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'discourse structure', 'sequence of utterances', 'linguistic structure', 'structure of purposes', 'intentional structure', 'state of focus of attention', 'attentional state'\n",
            "\n",
            "step (2):\n",
            "'discourse structure' - 'composed of' - 'sequence of utterances'\n",
            "'sequence of utterances' - 'called' - 'linguistic structure'\n",
            "'discourse structure' - 'composed of' - 'structure of purposes'\n",
            "'structure of purposes' - 'called' - 'intentional structure'\n",
            "'discourse structure' - 'composed of' - 'state of focus of attention'\n",
            "'state of focus of attention' - 'called' - 'attentional state'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'sequence of utterances' - 'FEATURE-OF' - 'discourse structure'\n",
            "'linguistic structure' - 'HYPONYM-OF' - 'sequence of utterances'\n",
            "'structure of purposes' - 'FEATURE-OF' - 'discourse structure'\n",
            "'intentional structure' - 'HYPONYM-OF' - 'structure of purposes'\n",
            "'state of focus of attention' - 'FEATURE-OF' - 'discourse structure'\n",
            "'attentional state' - 'HYPONYM-OF' - 'state of focus of attention'\n",
            "\n",
            "step (3):\n",
            "['sequence of utterances', 'FEATURE-OF', 'discourse structure']\n",
            "['linguistic structure', 'HYPONYM-OF', 'sequence of utterances']\n",
            "['structure of purposes', 'FEATURE-OF', 'discourse structure']\n",
            "['intentional structure', 'HYPONYM-OF', 'structure of purposes']\n",
            "['state of focus of attention', 'FEATURE-OF', 'discourse structure']\n",
            "['attentional state', 'HYPONYM-OF', 'state of focus of attention']\n",
            "\n",
            "['sequence of utterances', 'FEATURE-OF', 'discourse structure']                            \n",
            "['linguistic structure', 'HYPONYM-OF', 'sequence of utterances']                           \n",
            "['structure of purposes', 'FEATURE-OF', 'discourse structure']                             \n",
            "['intentional structure', 'HYPONYM-OF', 'structure of purposes']                           \n",
            "['state of focus of attention', 'FEATURE-OF', 'discourse structure']                       \n",
            "['attentional state', 'HYPONYM-OF', 'state of focus of attention']                         \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We analyzed eye gaze , head nods and attentional focus in the context of a direction-giving task .\n",
            "Gold ----->  ['attentional focus', 'PART-OF', 'direction-giving task']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'eye gaze', 'head nods', 'attentional focus', 'direction-giving task'\n",
            "\n",
            "step (2):\n",
            "'eye gaze' - 'and' - 'head nods'\n",
            "'head nods' - 'and' - 'attentional focus'\n",
            "'eye gaze' - 'in' - 'direction-giving task'\n",
            "'head nods' - 'in' - 'direction-giving task'\n",
            "'attentional focus' - 'in' - 'direction-giving task'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'eye gaze' - 'CONJUNCTION' - 'head nods'\n",
            "'head nods' - 'CONJUNCTION' - 'attentional focus'\n",
            "'eye gaze' - 'PART-OF' - 'direction-giving task'\n",
            "'head nods' - 'PART-OF' - 'direction-giving task'\n",
            "'attentional focus' - 'PART-OF' - 'direction-giving task'\n",
            "\n",
            "step (3):\n",
            "['eye gaze', 'CONJUNCTION', 'head nods']\n",
            "['head nods', 'CONJUNCTION', 'attentional focus']\n",
            "['eye gaze', 'PART-OF', 'direction-giving task']\n",
            "['head nods', 'PART-OF', 'direction-giving task']\n",
            "['attentional focus', 'PART-OF', 'direction-giving task']\n",
            "\n",
            "['eye gaze', 'CONJUNCTION', 'head nods']                                                   \n",
            "['head nods', 'CONJUNCTION', 'attentional focus']                                          \n",
            "['eye gaze', 'PART-OF', 'direction-giving task']                                           \n",
            "['head nods', 'PART-OF', 'direction-giving task']                                          \n",
            "['attentional focus', 'PART-OF', 'direction-giving task']                                  *** Correct Prediction **** PART-OF\n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Amorph recognizes NE items in two stages : dictionary lookup and rule application .\n",
            "Gold ----->  ['dictionary lookup', 'PART-OF', 'Amorph']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'Amorph', 'NE items', 'dictionary lookup', 'rule application'\n",
            "\n",
            "step (2):\n",
            "'Amorph' - 'recognize' - 'NE items'\n",
            "'Amorph' - 'use' - 'dictionary lookup'\n",
            "'Amorph' - 'use' - 'rule application'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'Amorph' - 'USED-FOR' - 'dictionary lookup'\n",
            "'Amorph' - 'USED-FOR' - 'rule application'\n",
            "\n",
            "step (3):\n",
            "['Amorph', 'USED-FOR', 'dictionary lookup']\n",
            "['Amorph', 'USED-FOR', 'rule application']\n",
            "\n",
            "['Amorph', 'USED-FOR', 'dictionary lookup']                                                \n",
            "['Amorph', 'USED-FOR', 'rule application']                                                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  In the Object Recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .\n",
            "Gold ----->  ['estimating object pose', 'PART-OF', 'Object Recognition task']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'Object Recognition task', 'categorization of objects', 'estimating object pose', 'view-invariant representation', 'representation', 'pose information', 'categories of objects'\n",
            "\n",
            "step (2):\n",
            "'Object Recognition task' - 'exists' - 'di-chotomy'\n",
            "'di-chotomy' - 'between' - 'categorization of objects'\n",
            "'di-chotomy' - 'between' - 'estimating object pose'\n",
            "'categorization of objects' - 'necessitates' - 'view-invariant representation'\n",
            "'estimating object pose' - 'requires' - 'representation'\n",
            "'representation' - 'capturing' - 'pose information'\n",
            "'pose information' - 'over' - 'categories of objects'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'categorization of objects' - 'COMPARE' - 'estimating object pose'\n",
            "'categorization of objects' - 'USED-FOR' - 'view-invariant representation'\n",
            "'estimating object pose' - 'USED-FOR' - 'representation'\n",
            "'representation' - 'FEATURE-OF' - 'pose information'\n",
            "'pose information' - 'PART-OF' - 'categories of objects'\n",
            "\n",
            "step (3):\n",
            "['categorization of objects', 'COMPARE', 'estimating object pose']\n",
            "['categorization of objects', 'USED-FOR', 'view-invariant representation']\n",
            "['estimating object pose', 'USED-FOR', 'representation']\n",
            "['representation', 'FEATURE-OF', 'pose information']\n",
            "['pose information', 'PART-OF', 'categories of objects']\n",
            "\n",
            "['categorization of objects', 'COMPARE', 'estimating object pose']                         \n",
            "['categorization of objects', 'USED-FOR', 'view-invariant representation']                 \n",
            "['estimating object pose', 'USED-FOR', 'representation']                                   \n",
            "['representation', 'FEATURE-OF', 'pose information']                                       \n",
            "['pose information', 'PART-OF', 'categories of objects']                                   \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Full digital resolution is maintained even with low-resolution analog-to-digital conversion , owing to random statistics in the analog summation of binary products .\n",
            "Gold ----->  ['random statistics', 'PART-OF', 'analog summation of binary products']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'full digital resolution', 'low-resolution analog-to-digital conversion', 'random statistics', 'analog summation', 'binary products'\n",
            "\n",
            "step (2):\n",
            "'full digital resolution' - 'maintained' - 'low-resolution analog-to-digital conversion'\n",
            "'low-resolution analog-to-digital conversion' - 'owing' - 'random statistics'\n",
            "'random statistics' - 'in' - 'analog summation'\n",
            "'analog summation' - 'of' - 'binary products'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'full digital resolution' - 'COMPARE' - 'low-resolution analog-to-digital conversion'\n",
            "'low-resolution analog-to-digital conversion' - 'EVALUATE-FOR' - 'random statistics'\n",
            "'random statistics' - 'PART-OF' - 'analog summation'\n",
            "'analog summation' - 'FEATURE-OF' - 'binary products'\n",
            "\n",
            "step (3):\n",
            "['full digital resolution', 'COMPARE', 'low-resolution analog-to-digital conversion']\n",
            "['low-resolution analog-to-digital conversion', 'EVALUATE-FOR', 'random statistics']\n",
            "['random statistics', 'PART-OF', 'analog summation']\n",
            "['analog summation', 'FEATURE-OF', 'binary products']\n",
            "\n",
            "['full digital resolution', 'COMPARE', 'low-resolution analog-to-digital conversion']      \n",
            "['low-resolution analog-to-digital conversion', 'EVALUATE-FOR', 'random statistics']       \n",
            "['random statistics', 'PART-OF', 'analog summation']                                       \n",
            "['analog summation', 'FEATURE-OF', 'binary products']                                      \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches .\n",
            "Gold ----->  ['utterances', 'EVALUATE-FOR', 'trainable components']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'techniques', 'modules', 'natural language generator', 'utterances', 'trainable components', 'hand-crafted template-based approaches', 'rule-based approaches'\n",
            "\n",
            "step (2):\n",
            "'techniques' - 'for' - 'training modules'\n",
            "'modules' - 'of' - 'natural language generator'\n",
            "'utterances' - 'produced with' - 'trainable components'\n",
            "'utterances' - 'can compete with' - 'hand-crafted template-based approaches'\n",
            "'utterances' - 'can compete with' - 'rule-based approaches'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'techniques' - 'USED-FOR' - 'training modules'\n",
            "'modules' - 'PART-OF' - 'natural language generator'\n",
            "'utterances' - 'EVALUATE-FOR' - 'trainable components'\n",
            "'utterances' - 'COMPARE' - 'hand-crafted template-based approaches'\n",
            "'utterances' - 'COMPARE' - 'rule-based approaches'\n",
            "\n",
            "step (3):\n",
            "['techniques', 'USED-FOR', 'training modules']\n",
            "['modules', 'PART-OF', 'natural language generator']\n",
            "['utterances', 'EVALUATE-FOR', 'trainable components']\n",
            "['utterances', 'COMPARE', 'hand-crafted template-based approaches']\n",
            "['utterances', 'COMPARE', 'rule-based approaches']\n",
            "\n",
            "['techniques', 'USED-FOR', 'training modules']                                             \n",
            "['modules', 'PART-OF', 'natural language generator']                                       \n",
            "['utterances', 'EVALUATE-FOR', 'trainable components']                                     *** Correct Prediction **** EVALUATE-FOR\n",
            "['utterances', 'COMPARE', 'hand-crafted template-based approaches']                        \n",
            "['utterances', 'COMPARE', 'rule-based approaches']                                         \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  The experimental results show that the proposed histogram-based interest point detectors perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of repeatability and distinctiveness .\n",
            "Gold ----->  ['repeatability', 'EVALUATE-FOR', 'histogram-based interest point detectors']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'experimental results', 'histogram-based interest point detectors', 'tasks', 'matching textured scenes', 'blur', 'illumination changes', 'repeatability', 'distinctiveness'\n",
            "\n",
            "step (2):\n",
            "'experimental results' - 'show' - 'histogram-based interest point detectors'\n",
            "'histogram-based interest point detectors' - 'perform' - 'tasks'\n",
            "'tasks' - 'of' - 'matching textured scenes'\n",
            "'matching textured scenes' - 'under' - 'blur'\n",
            "'matching textured scenes' - 'under' - 'illumination changes'\n",
            "'matching textured scenes' - 'in terms of' - 'repeatability'\n",
            "'matching textured scenes' - 'in terms of' - 'distinctiveness'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'histogram-based interest point detectors' - 'USED-FOR' - 'tasks'\n",
            "'matching textured scenes' - 'PART-OF' - 'tasks'\n",
            "'blur' - 'FEATURE-OF' - 'matching textured scenes'\n",
            "'illumination changes' - 'FEATURE-OF' - 'matching textured scenes'\n",
            "'repeatability' - 'FEATURE-OF' - 'matching textured scenes'\n",
            "'distinctiveness' - 'FEATURE-OF' - 'matching textured scenes'\n",
            "\n",
            "step (3):\n",
            "['histogram-based interest point detectors', 'USED-FOR', 'tasks']\n",
            "['matching textured scenes', 'PART-OF', 'tasks']\n",
            "['blur', 'FEATURE-OF', 'matching textured scenes']\n",
            "['illumination changes', 'FEATURE-OF', 'matching textured scenes']\n",
            "['repeatability', 'FEATURE-OF', 'matching textured scenes']\n",
            "['distinctiveness', 'FEATURE-OF', 'matching textured scenes']\n",
            "\n",
            "['histogram-based interest point detectors', 'USED-FOR', 'tasks']                          \n",
            "['matching textured scenes', 'PART-OF', 'tasks']                                           \n",
            "['blur', 'FEATURE-OF', 'matching textured scenes']                                         \n",
            "['illumination changes', 'FEATURE-OF', 'matching textured scenes']                         \n",
            "['repeatability', 'FEATURE-OF', 'matching textured scenes']                                \n",
            "['distinctiveness', 'FEATURE-OF', 'matching textured scenes']                              \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings .\n",
            "Gold ----->  ['summarization quality', 'EVALUATE-FOR', 'automatic parse-based evaluation']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'experimental evaluation', 'summarization quality', 'correlation', 'automatic parse-based evaluation', 'manual evaluation', 'generated strings'\n",
            "\n",
            "step (2):\n",
            "'experimental evaluation' - 'of' - 'summarization quality'\n",
            "'correlation' - 'between' - 'automatic parse-based evaluation'\n",
            "'correlation' - 'between' - 'manual evaluation'\n",
            "'manual evaluation' - 'of' - 'generated strings'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'experimental evaluation' - 'EVALUATE-FOR' - 'summarization quality'\n",
            "'correlation' - 'COMPARE' - 'automatic parse-based evaluation'\n",
            "'correlation' - 'COMPARE' - 'manual evaluation'\n",
            "'manual evaluation' - 'EVALUATE-FOR' - 'generated strings'\n",
            "\n",
            "step (3):\n",
            "['experimental evaluation', 'EVALUATE-FOR', 'summarization quality']\n",
            "['correlation', 'COMPARE', 'automatic parse-based evaluation']\n",
            "['correlation', 'COMPARE', 'manual evaluation']\n",
            "['manual evaluation', 'EVALUATE-FOR', 'generated strings']\n",
            "\n",
            "['experimental evaluation', 'EVALUATE-FOR', 'summarization quality']                       \n",
            "['correlation', 'COMPARE', 'automatic parse-based evaluation']                             \n",
            "['correlation', 'COMPARE', 'manual evaluation']                                            \n",
            "['manual evaluation', 'EVALUATE-FOR', 'generated strings']                                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods .\n",
            "Gold ----->  ['ACE corpora', 'EVALUATE-FOR', 'spectral clustering based approach']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'experiment results', 'ACE corpora', 'spectral clustering based approach', 'other clustering methods'\n",
            "\n",
            "step (2):\n",
            "'experiment results' - 'on' - 'ACE corpora'\n",
            "'spectral clustering based approach' - 'outperforms' - 'other clustering methods'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'experiment results' - 'EVALUATE-FOR' - 'ACE corpora'\n",
            "'spectral clustering based approach' - 'COMPARE' - 'other clustering methods'\n",
            "\n",
            "step (3):\n",
            "['experiment results', 'EVALUATE-FOR', 'ACE corpora']\n",
            "['spectral clustering based approach', 'COMPARE', 'other clustering methods']\n",
            "\n",
            "['experiment results', 'EVALUATE-FOR', 'ACE corpora']                                      \n",
            "['spectral clustering based approach', 'COMPARE', 'other clustering methods']              \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  In particular there are three areas of novelty : -LRB- i -RRB- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .\n",
            "Gold ----->  ['robustness', 'EVALUATE-FOR', \"video sequence '' reillumination '' algorithm\"]\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'photometric model', 'image formation', 'statistical model', 'generic face appearance variation', 'extreme illumination changes', 'smoothness', 'geodesically local appearance manifold structure', 'robust same-identity likelihood', 'unseen head poses', 'video sequence', 'reillumination algorithm', 'face motion patterns', 'video'\n",
            "\n",
            "step (2):\n",
            "'photometric model' - 'of' - 'image formation'\n",
            "'statistical model' - 'of' - 'generic face appearance variation'\n",
            "'smoothness' - 'of' - 'geodesically local appearance manifold structure'\n",
            "'robust same-identity likelihood' - 'to' - 'unseen head poses'\n",
            "'video sequence' - 'of' - 'reillumination algorithm'\n",
            "'reillumination algorithm' - 'to' - 'face motion patterns'\n",
            "'face motion patterns' - 'in' - 'video'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'photometric model' - 'PART-OF' - 'image formation'\n",
            "'statistical model' - 'PART-OF' - 'generic face appearance variation'\n",
            "'smoothness' - 'FEATURE-OF' - 'geodesically local appearance manifold structure'\n",
            "'robust same-identity likelihood' - 'EVALUATE-FOR' - 'unseen head poses'\n",
            "'video sequence' - 'PART-OF' - 'reillumination algorithm'\n",
            "'reillumination algorithm' - 'EVALUATE-FOR' - 'face motion patterns'\n",
            "'face motion patterns' - 'PART-OF' - 'video'\n",
            "\n",
            "step (3):\n",
            "['photometric model', 'PART-OF', 'image formation']\n",
            "['statistical model', 'PART-OF', 'generic face appearance variation']\n",
            "['smoothness', 'FEATURE-OF', 'geodesically local appearance manifold structure']\n",
            "['robust same-identity likelihood', 'EVALUATE-FOR', 'unseen head poses']\n",
            "['video sequence', 'PART-OF', 'reillumination algorithm']\n",
            "['reillumination algorithm', 'EVALUATE-FOR', 'face motion patterns']\n",
            "['face motion patterns', 'PART-OF', 'video']\n",
            "\n",
            "['photometric model', 'PART-OF', 'image formation']                                        \n",
            "['statistical model', 'PART-OF', 'generic face appearance variation']                      \n",
            "['smoothness', 'FEATURE-OF', 'geodesically local appearance manifold structure']           \n",
            "['robust same-identity likelihood', 'EVALUATE-FOR', 'unseen head poses']                   \n",
            "['video sequence', 'PART-OF', 'reillumination algorithm']                                  \n",
            "['reillumination algorithm', 'EVALUATE-FOR', 'face motion patterns']                       \n",
            "['face motion patterns', 'PART-OF', 'video']                                               \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  The results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our Horn clauses can be learnt automatically from these features .\n",
            "Gold ----->  ['features', 'FEATURE-OF', 'heuristic principles']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'results', 'features', 'heuristic principles', 'predictive power', 'rules', 'Horn clauses'\n",
            "\n",
            "step (2):\n",
            "'results' - 'show' - 'features'\n",
            "'features' - 'formulate' - 'heuristic principles'\n",
            "'heuristic principles' - 'have' - 'predictive power'\n",
            "'rules' - 'resemble' - 'Horn clauses'\n",
            "'Horn clauses' - 'learnt' - 'features'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'features' - 'FEATURE-OF' - 'heuristic principles'\n",
            "'heuristic principles' - 'EVALUATE-FOR' - 'predictive power'\n",
            "'rules' - 'COMPARE' - 'Horn clauses'\n",
            "'Horn clauses' - 'USED-FOR' - 'features'\n",
            "\n",
            "step (3):\n",
            "['features', 'FEATURE-OF', 'heuristic principles']\n",
            "['heuristic principles', 'EVALUATE-FOR', 'predictive power']\n",
            "['rules', 'COMPARE', 'Horn clauses']\n",
            "['Horn clauses', 'USED-FOR', 'features']\n",
            "\n",
            "['features', 'FEATURE-OF', 'heuristic principles']                                         *** Correct Prediction **** FEATURE-OF\n",
            "['heuristic principles', 'EVALUATE-FOR', 'predictive power']                               \n",
            "['rules', 'COMPARE', 'Horn clauses']                                                       \n",
            "['Horn clauses', 'USED-FOR', 'features']                                                   \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses .\n",
            "Gold ----->  ['probabilistic Horn clauses', 'FEATURE-OF', 'heuristic principles']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'heuristic principles', 'corpus-based sample', 'probabilistic Horn clauses'\n",
            "\n",
            "step (2):\n",
            "'heuristic principles' - 'extract' - 'corpus-based sample'\n",
            "'heuristic principles' - 'formulate' - 'probabilistic Horn clauses'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'heuristic principles' - 'FEATURE-OF' - 'corpus-based sample'\n",
            "'heuristic principles' - 'USED-FOR' - 'probabilistic Horn clauses'\n",
            "\n",
            "step (3):\n",
            "['heuristic principles', 'FEATURE-OF', 'corpus-based sample']\n",
            "['heuristic principles', 'USED-FOR', 'probabilistic Horn clauses']\n",
            "\n",
            "['heuristic principles', 'FEATURE-OF', 'corpus-based sample']                              \n",
            "['heuristic principles', 'USED-FOR', 'probabilistic Horn clauses']                         \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  A separation method is proposed that is nearly statistically efficient -LRB- approaching the corresponding Cramér-Rao lower bound -RRB- , if the separated signals obey the assumed model .\n",
            "Gold ----->  ['Cramér-Rao lower bound -RRB-', 'FEATURE-OF', 'separation method']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'separation method', 'statistically efficient', 'Cramér-Rao lower bound', 'separated signals', 'assumed model'\n",
            "\n",
            "step (2):\n",
            "'separation method' - 'is' - 'statistically efficient'\n",
            "'separation method' - 'is' - 'approaching the corresponding Cramér-Rao lower bound'\n",
            "'separated signals' - 'obey' - 'assumed model'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'separation method' - 'FEATURE-OF' - 'statistically efficient'\n",
            "'separation method' - 'FEATURE-OF' - 'approaching the corresponding Cramér-Rao lower bound'\n",
            "'separated signals' - 'USED-FOR' - 'assumed model'\n",
            "\n",
            "step (3):\n",
            "['separation method', 'FEATURE-OF', 'statistically efficient']\n",
            "['separation method', 'FEATURE-OF', 'approaching the corresponding Cramér-Rao lower bound']\n",
            "['separated signals', 'USED-FOR', 'assumed model']\n",
            "\n",
            "['separation method', 'FEATURE-OF', 'statistically efficient']                             \n",
            "['separation method', 'FEATURE-OF', 'approaching the corresponding Cramér-Rao lower bound'] \n",
            "['separated signals', 'USED-FOR', 'assumed model']                                         \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We introduce a method to accelerate the evaluation of object detection cascades with the help of a divide-and-conquer procedure in the space of candidate regions .\n",
            "Gold ----->  ['space of candidate regions', 'FEATURE-OF', 'divide-and-conquer procedure']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'method', 'evaluation', 'object detection cascades', 'divide-and-conquer procedure', 'space', 'candidate regions'\n",
            "\n",
            "step (2):\n",
            "'method' - 'accelerate' - 'evaluation'\n",
            "'evaluation' - 'of' - 'object detection cascades'\n",
            "'object detection cascades' - 'with' - 'divide-and-conquer procedure'\n",
            "'divide-and-conquer procedure' - 'in' - 'space'\n",
            "'space' - 'of' - 'candidate regions'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'method' - 'USED-FOR' - 'evaluation'\n",
            "'evaluation' - 'OF' - 'object detection cascades'\n",
            "'object detection cascades' - 'USED-FOR' - 'divide-and-conquer procedure'\n",
            "'divide-and-conquer procedure' - 'PART-OF' - 'space'\n",
            "'space' - 'OF' - 'candidate regions'\n",
            "\n",
            "step (3):\n",
            "['method', 'USED-FOR', 'evaluation']\n",
            "['evaluation', 'OF', 'object detection cascades']\n",
            "['object detection cascades', 'USED-FOR', 'divide-and-conquer procedure']\n",
            "['divide-and-conquer procedure', 'PART-OF', 'space']\n",
            "['space', 'OF', 'candidate regions']\n",
            "\n",
            "['method', 'USED-FOR', 'evaluation']                                                       \n",
            "['evaluation', 'OF', 'object detection cascades']                                          \n",
            "['object detection cascades', 'USED-FOR', 'divide-and-conquer procedure']                  \n",
            "['divide-and-conquer procedure', 'PART-OF', 'space']                                       \n",
            "['space', 'OF', 'candidate regions']                                                       \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Experimental results from a real telephone application on a natural number recognition task show an 50 % reduction in recognition errors with a moderate 12 % rejection rate of correct utterances and a low 1.5 % rate of false acceptance .\n",
            "Gold ----->  ['natural number recognition task', 'FEATURE-OF', 'telephone application']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'experimental results', 'telephone application', 'natural number recognition task', 'recognition errors', 'rejection rate', 'correct utterances', 'false acceptance'\n",
            "\n",
            "step (2):\n",
            "'experimental results' - 'from' - 'telephone application'\n",
            "'telephone application' - 'on' - 'natural number recognition task'\n",
            "'experimental results' - 'show' - 'reduction'\n",
            "'reduction' - 'in' - 'recognition errors'\n",
            "'rejection rate' - 'of' - 'correct utterances'\n",
            "'false acceptance' - 'rate' - 'of' - 'correct utterances'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'telephone application' - 'USED-FOR' - 'natural number recognition task'\n",
            "'experimental results' - 'EVALUATE-FOR' - 'reduction'\n",
            "'reduction' - 'COMPARE' - 'recognition errors'\n",
            "'rejection rate' - 'EVALUATE-FOR' - 'correct utterances'\n",
            "'false acceptance' - 'EVALUATE-FOR' - 'correct utterances'\n",
            "\n",
            "step (3):\n",
            "['telephone application', 'USED-FOR', 'natural number recognition task']\n",
            "['experimental results', 'EVALUATE-FOR', 'reduction']\n",
            "['reduction', 'COMPARE', 'recognition errors']\n",
            "['rejection rate', 'EVALUATE-FOR', 'correct utterances']\n",
            "['false acceptance', 'EVALUATE-FOR', 'correct utterances']\n",
            "\n",
            "['telephone application', 'USED-FOR', 'natural number recognition task']                   \n",
            "['experimental results', 'EVALUATE-FOR', 'reduction']                                      \n",
            "['reduction', 'COMPARE', 'recognition errors']                                             \n",
            "['rejection rate', 'EVALUATE-FOR', 'correct utterances']                                   \n",
            "['false acceptance', 'EVALUATE-FOR', 'correct utterances']                                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Here , we leverage a logistic stick-breaking representation and recent innovations in Pólya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods , enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead .\n",
            "Gold ----->  ['minimal overhead', 'FEATURE-OF', 'Gaussian models']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'logistic stick-breaking representation', 'Pólya-gamma augmentation', 'multinomial distribution', 'latent variables', 'Gaussian likelihoods', 'Bayesian inference techniques', 'Gaussian models'\n",
            "\n",
            "step (2):\n",
            "'we' - 'leverage' - 'logistic stick-breaking representation'\n",
            "'we' - 'leverage' - 'Pólya-gamma augmentation'\n",
            "'we' - 'reformulate' - 'multinomial distribution'\n",
            "'we' - 'take advantage of' - 'Bayesian inference techniques'\n",
            "'latent variables' - 'with' - 'Gaussian likelihoods'\n",
            "'Bayesian inference techniques' - 'for' - 'Gaussian models'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'we' - 'USED-FOR' - 'logistic stick-breaking representation'\n",
            "'we' - 'USED-FOR' - 'Pólya-gamma augmentation'\n",
            "'we' - 'USED-FOR' - 'multinomial distribution'\n",
            "'we' - 'USED-FOR' - 'Bayesian inference techniques'\n",
            "'latent variables' - 'FEATURE-OF' - 'Gaussian likelihoods'\n",
            "'Bayesian inference techniques' - 'USED-FOR' - 'Gaussian models'\n",
            "\n",
            "step (3):\n",
            "['we', 'USED-FOR', 'logistic stick-breaking representation']\n",
            "['we', 'USED-FOR', 'Pólya-gamma augmentation']\n",
            "['we', 'USED-FOR', 'multinomial distribution']\n",
            "['we', 'USED-FOR', 'Bayesian inference techniques']\n",
            "['latent variables', 'FEATURE-OF', 'Gaussian likelihoods']\n",
            "['Bayesian inference techniques', 'USED-FOR', 'Gaussian models']\n",
            "\n",
            "['we', 'USED-FOR', 'logistic stick-breaking representation']                               \n",
            "['we', 'USED-FOR', 'Pólya-gamma augmentation']                                             \n",
            "['we', 'USED-FOR', 'multinomial distribution']                                             \n",
            "['we', 'USED-FOR', 'Bayesian inference techniques']                                        \n",
            "['latent variables', 'FEATURE-OF', 'Gaussian likelihoods']                                 \n",
            "['Bayesian inference techniques', 'USED-FOR', 'Gaussian models']                           \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our experiments on real data sets show that the resulting detector is more robust to the choice of training examples , and substantially improves both linear and kernel SVM when trained on 10 positive and 10 negative examples .\n",
            "Gold ----->  ['detector', 'COMPARE', 'linear and kernel SVM']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'experiments', 'real data sets', 'resulting detector', 'choice of training examples', 'linear SVM', 'kernel SVM', 'positive examples', 'negative examples'\n",
            "\n",
            "step (2):\n",
            "'experiments' - 'on' - 'real data sets'\n",
            "'experiments' - 'show' - 'resulting detector'\n",
            "'resulting detector' - 'robust' - 'choice of training examples'\n",
            "'resulting detector' - 'improves' - 'linear SVM'\n",
            "'resulting detector' - 'improves' - 'kernel SVM'\n",
            "'linear SVM' - 'trained' - 'positive examples'\n",
            "'kernel SVM' - 'trained' - 'negative examples'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'experiments' - 'EVALUATE-FOR' - 'real data sets'\n",
            "'resulting detector' - 'USED-FOR' - 'choice of training examples'\n",
            "'resulting detector' - 'USED-FOR' - 'linear SVM'\n",
            "'resulting detector' - 'USED-FOR' - 'kernel SVM'\n",
            "'linear SVM' - 'PART-OF' - 'positive examples'\n",
            "'kernel SVM' - 'PART-OF' - 'negative examples'\n",
            "\n",
            "step (3):\n",
            "['experiments', 'EVALUATE-FOR', 'real data sets']\n",
            "['resulting detector', 'USED-FOR', 'choice of training examples']\n",
            "['resulting detector', 'USED-FOR', 'linear SVM']\n",
            "['resulting detector', 'USED-FOR', 'kernel SVM']\n",
            "['linear SVM', 'PART-OF', 'positive examples']\n",
            "['kernel SVM', 'PART-OF', 'negative examples']\n",
            "\n",
            "['experiments', 'EVALUATE-FOR', 'real data sets']                                          \n",
            "['resulting detector', 'USED-FOR', 'choice of training examples']                          \n",
            "['resulting detector', 'USED-FOR', 'linear SVM']                                           \n",
            "['resulting detector', 'USED-FOR', 'kernel SVM']                                           \n",
            "['linear SVM', 'PART-OF', 'positive examples']                                             \n",
            "['kernel SVM', 'PART-OF', 'negative examples']                                             \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We consider the problem of computing the Kullback-Leibler distance , also called the relative entropy , between a probabilistic context-free grammar and a probabilistic finite automaton .\n",
            "Gold ----->  ['probabilistic context-free grammar', 'COMPARE', 'probabilistic finite automaton']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'problem', 'Kullback-Leibler distance', 'relative entropy', 'probabilistic context-free grammar', 'probabilistic finite automaton'\n",
            "\n",
            "step (2):\n",
            "'problem' - 'of' - 'computing'\n",
            "'Kullback-Leibler distance' - 'called' - 'relative entropy'\n",
            "'relative entropy' - 'between' - 'probabilistic context-free grammar'\n",
            "'probabilistic context-free grammar' - 'and' - 'probabilistic finite automaton'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'Kullback-Leibler distance' - 'COMPARE' - 'relative entropy'\n",
            "'relative entropy' - 'FEATURE-OF' - 'probabilistic context-free grammar'\n",
            "'probabilistic context-free grammar' - 'CONJUNCTION' - 'probabilistic finite automaton'\n",
            "\n",
            "step (3):\n",
            "['Kullback-Leibler distance', 'COMPARE', 'relative entropy']\n",
            "['relative entropy', 'FEATURE-OF', 'probabilistic context-free grammar']\n",
            "['probabilistic context-free grammar', 'CONJUNCTION', 'probabilistic finite automaton']\n",
            "\n",
            "['Kullback-Leibler distance', 'COMPARE', 'relative entropy']                               \n",
            "['relative entropy', 'FEATURE-OF', 'probabilistic context-free grammar']                   \n",
            "['probabilistic context-free grammar', 'CONJUNCTION', 'probabilistic finite automaton']    \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments .\n",
            "Gold ----->  ['technique', 'COMPARE', 'models']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'technique', 'improvement', 'paraphrase classification accuracy', 'models', 'experiments'\n",
            "\n",
            "step (2):\n",
            "'technique' - 'give' - 'improvement'\n",
            "'improvement' - 'in' - 'paraphrase classification accuracy'\n",
            "'paraphrase classification accuracy' - 'over' - 'models'\n",
            "'models' - 'used' - 'experiments'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'technique' - 'EVALUATE-FOR' - 'improvement'\n",
            "'improvement' - 'FEATURE-OF' - 'paraphrase classification accuracy'\n",
            "'paraphrase classification accuracy' - 'COMPARE' - 'models'\n",
            "'models' - 'USED-FOR' - 'experiments'\n",
            "\n",
            "step (3):\n",
            "['technique', 'EVALUATE-FOR', 'improvement']\n",
            "['improvement', 'FEATURE-OF', 'paraphrase classification accuracy']\n",
            "['paraphrase classification accuracy', 'COMPARE', 'models']\n",
            "['models', 'USED-FOR', 'experiments']\n",
            "\n",
            "['technique', 'EVALUATE-FOR', 'improvement']                                               \n",
            "['improvement', 'FEATURE-OF', 'paraphrase classification accuracy']                        \n",
            "['paraphrase classification accuracy', 'COMPARE', 'models']                                \n",
            "['models', 'USED-FOR', 'experiments']                                                      \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representations .\n",
            "Gold ----->  ['this', 'COMPARE', 'object category representations']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'layers', 'CNN models', 'object pose information', 'object category representations'\n",
            "\n",
            "step (2):\n",
            "'We' - 'investigate' - 'layers'\n",
            "'We' - 'analyze' - 'layers'\n",
            "'layers' - 'of' - 'CNN models'\n",
            "'layers' - 'compare' - 'layers'\n",
            "'layers' - 'represent' - 'object pose information'\n",
            "'layers' - 'contradict' - 'object category representations'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'layers' - 'PART-OF' - 'CNN models'\n",
            "'layers' - 'COMPARE' - 'layers'\n",
            "'layers' - 'USED-FOR' - 'object pose information'\n",
            "'layers' - 'COMPARE' - 'object category representations'\n",
            "\n",
            "step (3):\n",
            "['layers', 'PART-OF', 'CNN models']\n",
            "['layers', 'COMPARE', 'layers']\n",
            "['layers', 'USED-FOR', 'object pose information']\n",
            "['layers', 'COMPARE', 'object category representations']\n",
            "\n",
            "['layers', 'PART-OF', 'CNN models']                                                        \n",
            "['layers', 'COMPARE', 'layers']                                                            \n",
            "['layers', 'USED-FOR', 'object pose information']                                          \n",
            "['layers', 'COMPARE', 'object category representations']                                   \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We show that the trainable sentence planner performs better than the rule-based systems and the baselines , and as well as the hand-crafted system .\n",
            "Gold ----->  ['trainable sentence planner', 'COMPARE', 'baselines']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'trainable sentence planner', 'rule-based systems', 'baselines', 'hand-crafted system'\n",
            "\n",
            "step (2):\n",
            "'trainable sentence planner' - 'perform' - 'rule-based systems'\n",
            "'trainable sentence planner' - 'perform' - 'baselines'\n",
            "'trainable sentence planner' - 'perform' - 'hand-crafted system'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'trainable sentence planner' - 'COMPARE' - 'rule-based systems'\n",
            "'trainable sentence planner' - 'COMPARE' - 'baselines'\n",
            "'trainable sentence planner' - 'COMPARE' - 'hand-crafted system'\n",
            "\n",
            "step (3):\n",
            "['trainable sentence planner', 'COMPARE', 'rule-based systems']\n",
            "['trainable sentence planner', 'COMPARE', 'baselines']\n",
            "['trainable sentence planner', 'COMPARE', 'hand-crafted system']\n",
            "\n",
            "['trainable sentence planner', 'COMPARE', 'rule-based systems']                            \n",
            "['trainable sentence planner', 'COMPARE', 'baselines']                                     *** Correct Prediction **** COMPARE\n",
            "['trainable sentence planner', 'COMPARE', 'hand-crafted system']                           \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Compared to the exhaustive procedure that thus far is the state-of-the-art for cascade evaluation , the proposed method requires fewer evaluations of the classifier functions , thereby speeding up the search .\n",
            "Gold ----->  ['exhaustive procedure', 'COMPARE', 'method']\n",
            "\n",
            "Reasoning --->  step (1): Entities: 'exhaustive procedure', 'cascade evaluation', 'proposed method', 'classifier functions', 'search'\n",
            "\n",
            "step (2):\n",
            "'exhaustive procedure' - 'for' - 'cascade evaluation'\n",
            "'proposed method' - 'requires' - 'evaluations'\n",
            "'evaluations' - 'of' - 'classifier functions'\n",
            "'classifier functions' - 'for' - 'search'\n",
            "\n",
            "check possible realtipredicate on based on the give list of predicates:\n",
            "\n",
            "'exhaustive procedure' - 'USED-FOR' - 'cascade evaluation'\n",
            "'proposed method' - 'USED-FOR' - 'evaluations'\n",
            "'evaluations' - 'PART-OF' - 'classifier functions'\n",
            "'classifier functions' - 'USED-FOR' - 'search'\n",
            "\n",
            "step (3):\n",
            "['exhaustive procedure', 'USED-FOR', 'cascade evaluation']\n",
            "['proposed method', 'USED-FOR', 'evaluations']\n",
            "['evaluations', 'PART-OF', 'classifier functions']\n",
            "['classifier functions', 'USED-FOR', 'search']\n",
            "\n",
            "['exhaustive procedure', 'USED-FOR', 'cascade evaluation']                                 \n",
            "['proposed method', 'USED-FOR', 'evaluations']                                             \n",
            "['evaluations', 'PART-OF', 'classifier functions']                                         \n",
            "['classifier functions', 'USED-FOR', 'search']                                             \n",
            "\n",
            "***\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phase_three_cot_prompt = '''\n",
        "\n",
        "Given a sentence, identify and extract relevant triples of entities connected by a predicate from the provided list.\n",
        "The format of each triple should be [head, predicate, tail], separated by commas.\n",
        "\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Step (1): Entity Extraction\n",
        "- Extract the entities from the given sentence.\n",
        "- Group sequences of words that indicate the same role in the sentence as a single entity.\n",
        "- Exclude <adjectives> from entities to focus on core nouns.\n",
        "\n",
        "Step (2): Triple Identification\n",
        "- Identify a single relevant predicate from the given list that best describes the relationship between the entities. List of predicates: {}.\n",
        "- Ensure that the order of subject, predicate, and object is maintained based on the predicate's meaning.\n",
        "- Consider the semantic relationship between entities when selecting a predicate.\n",
        "\n",
        "Step (3): Triple Formation\n",
        "- Construct triples in the format [head, predicate, tail].\n",
        "- Prioritize the triples based on your confidence in their relevance and accuracy.\n",
        "- Return up to three of the most confident triples. If there is only one confident triple, return that singular triple. Ensure that the predicates in each triple are distinct.\n",
        "\n",
        "\n",
        "Example 1:\n",
        "Given Sentence: = \"With the aid of a logic-based grammar formalism called extraposition grammars, Chat-80 translates English questions into the Prolog subset of logic.\"\n",
        "\n",
        "Step (1): Entity Extraction\n",
        "entities = 'extraposition grammars', 'logic-based grammar formalism', 'English questions', 'Prolog subset', 'logic'\n",
        "\n",
        "Step (2): Triple Identification\n",
        "potential_triples:\n",
        "- 'extraposition grammars', 'HYPONYM-OF', 'logic-based grammar formalism'\n",
        "- 'Prolog subset', 'PART-OF', 'logic'\n",
        "\n",
        "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
        "['extraposition grammars', 'HYPONYM-OF', 'logic-based grammar formalism']\n",
        "['Prolog subset', 'PART-OF', 'logic']\n",
        "\n",
        "\n",
        "Example 2:\n",
        "Given Sentence: \"We propose a novel probabilistic framework for learning visual models of 3D object categories by combining appearance information and geometric constraints.\"\n",
        "\n",
        "Step (1):\n",
        "Entities: We, novel probabilistic framework, learning, visual models, 3D object categories, appearance information, geometric constraints\n",
        "\n",
        "Step (2):\n",
        "Potential Triples:\n",
        "- 'probabilistic framework' 'USED-FOR', 'learning visual models'\n",
        "- 'geometric constraints', 'USED-FOR', 'probabilistic framework'\n",
        "- '3D object categories', 'PART-OF', 'visual models'\n",
        "\n",
        "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
        "['geometric constraints', 'USED-FOR', 'probabilistic framework']\n",
        "['3D object categories', 'PART-OF', 'visual models']\n",
        "\n",
        "\n",
        "Example 3:\n",
        "Given Sentence: \"Experimental results on our dataset and the public G3D dataset both demonstrate very promising performance of our scheme.\"\n",
        "\n",
        "Step (1):\n",
        "Entities: Experimental results, our dataset, public G3D dataset, performance, our scheme\n",
        "\n",
        "Step (2):\n",
        "Potential Triples:\n",
        "- 'our dataset', 'CONJUNCTION', 'public G3D dataset'\n",
        "- 'Experimental results', 'COMPARE', 'performance'\n",
        "\n",
        "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
        "['our dataset', 'CONJUNCTION', 'public G3D dataset']\n",
        "['Experimental results', 'COMPARE', 'performance']\n",
        "\n",
        "\n",
        "The given sentence is : {}\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "FHEYHMDOIhPL"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output, cot_reasoning_list = TE_prompt_runner(phase_three_cot_prompt, SciERC_dataset_40samples, SciERC_list_of_relation, num_samples= 40, CoT = True, model = 'GPT')"
      ],
      "metadata": {
        "id": "0MViyt-kwULz"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# non-Binary Evaluation\n",
        "nb_labels, nb_predictions = nonbinary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list)\n",
        "TE_metric_calculation(nb_labels, nb_predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "id": "qJbu_ZkcwUJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb42de34-473a-41ec-b72c-9882f72aa4d1"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 9.27\n",
            "Micro_Precision: 6.36\n",
            "Micro_Recall: 17.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Evaluation\n",
        "labels, predictions = binary_evaluation(SciERC_gold_samples, output, SciERC_orderless_list)\n",
        "TE_metric_calculation(labels, predictions, SciERC_list_of_relation)"
      ],
      "metadata": {
        "id": "GopNBppm4CtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27dc6a76-fb5a-42bc-89dc-fbb7354d3380"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 15.0\n",
            "Micro_Precision: 15.0\n",
            "Micro_Recall: 15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_output(output, SciERC_dataset_40samples, SciERC_gold_samples, nb_predictions, cot_reasoning_list)"
      ],
      "metadata": {
        "id": "fbo8zyNK4Cwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "debcb54d-50fd-44b0-c756-0b916fdd8d5a"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate of unstructured or abstained output: 0.0%\n",
            "\n",
            "Sentence:  We present a novel method for discovering parallel sentences in comparable , non-parallel corpora .\n",
            "Gold ----->  ['method', 'USED-FOR', 'discovering parallel sentences']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: We, novel method, discovering parallel sentences, comparable, non-parallel corpora\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- ['novel method', 'USED-FOR', 'discovering parallel sentences']\n",
            "- ['discovering parallel sentences', 'PART-OF', 'non-parallel corpora']\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the most confident triples:\n",
            "Return: ['novel method', 'USED-FOR', 'discovering parallel sentences']\n",
            "\n",
            "['novel method', 'USED-FOR', 'discovering parallel sentences']                             \n",
            "['discovering parallel sentences', 'PART-OF', 'non-parallel corpora']                      \n",
            "['novel method', 'USED-FOR', 'discovering parallel sentences']                             \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  During normal tracking conditions when the object is visible from frame to frame , local optimization is used to track the local mode of the similarity measure in a parameter space of translation , rotation and scale .\n",
            "Gold ----->  ['parameter space of translation , rotation and scale', 'USED-FOR', 'local mode of the similarity measure']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: normal tracking conditions, object, frame, local optimization, local mode, similarity measure, parameter space, translation, rotation, scale\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'local optimization', 'USED-FOR', 'track the local mode'\n",
            "- 'translation , rotation and scale', 'PART-OF', 'parameter space'\n",
            "- 'local mode', 'PART-OF', 'similarity measure'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the three most confident triples:\n",
            "['local optimization', 'USED-FOR', 'track the local mode']\n",
            "['translation , rotation and scale', 'PART-OF', 'parameter space']\n",
            "['local mode', 'PART-OF', 'similarity measure']\n",
            "\n",
            "['local optimization', 'USED-FOR', 'track the local mode']                                 \n",
            "['translation', 'rotation and scale', 'PART-OF', 'parameter space']                        \n",
            "['local mode', 'PART-OF', 'similarity measure']                                            \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  This paper describes a method for incorporating priming into an incremental probabilistic parser .\n",
            "Gold ----->  ['priming', 'USED-FOR', 'incremental probabilistic parser']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'paper', 'method', 'priming', 'incremental probabilistic parser'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'method', 'USED-FOR', 'incorporating priming'\n",
            "- 'method', 'PART-OF', 'incremental probabilistic parser'\n",
            "- 'priming', 'FEATURE-OF', 'method'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['method', 'USED-FOR', 'incorporating priming']\n",
            "['priming', 'FEATURE-OF', 'method']\n",
            "\n",
            "['method', 'USED-FOR', 'incorporating priming']                                            \n",
            "['priming', 'FEATURE-OF', 'method']                                                        \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our technique is based on an improved , dynamic-programming , stereo algorithm for efficient novel-view generation .\n",
            "Gold ----->  ['dynamic-programming , stereo algorithm', 'USED-FOR', 'technique']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'Our technique', 'improved dynamic-programming stereo algorithm', 'efficient novel-view generation'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'Our technique', 'BASED-ON', 'improved dynamic-programming stereo algorithm'\n",
            "- 'improved dynamic-programming stereo algorithm', 'USED-FOR', 'efficient novel-view generation'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['Our technique', 'BASED-ON', 'improved dynamic-programming stereo algorithm']\n",
            "['improved dynamic-programming stereo algorithm', 'USED-FOR', 'efficient novel-view generation']\n",
            "\n",
            "['Our technique', 'BASED-ON', 'improved dynamic-programming stereo algorithm']             \n",
            "['improved dynamic-programming stereo algorithm', 'USED-FOR', 'efficient novel-view generation'] \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  A demonstration -LRB- in UNIX -RRB- for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis -LRB- ICALL -RRB- , including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples .\n",
            "Gold ----->  ['lemmatized indexing', 'USED-FOR', 'aligned bilingual corpus']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: A demonstration, UNIX, Applied Natural Language Processing, components, novel technical uses, intelligent computer-assisted morphological analysis, ICALL, disambiguated morphological analysis, lemmatized indexing, aligned bilingual corpus, word examples\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'A demonstration', 'PART-OF', 'UNIX'\n",
            "- 'components', 'USED-FOR', 'novel technical uses'\n",
            "- 'disambiguated morphological analysis', 'PART-OF', 'intelligent computer-assisted morphological analysis'\n",
            "- 'lemmatized indexing', 'USED-FOR', 'aligned bilingual corpus'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['components', 'USED-FOR', 'novel technical uses']\n",
            "['lemmatized indexing', 'USED-FOR', 'aligned bilingual corpus']\n",
            "\n",
            "['components', 'USED-FOR', 'novel technical uses']                                         \n",
            "['lemmatized indexing', 'USED-FOR', 'aligned bilingual corpus']                            *** Correct Prediction **** USED-FOR\n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We propose a multi-task end-to-end Joint Classification-Regression Recurrent Neural Network to better explore the action type and temporal localiza-tion information .\n",
            "Gold ----->  ['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'USED-FOR', 'action type']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'We', 'multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'action type', 'temporal localization information'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'multi-task end-to-end Joint Classification-Regression Recurrent Neural Network' 'USED-FOR', 'exploring action type'\n",
            "- 'multi-task end-to-end Joint Classification-Regression Recurrent Neural Network' 'USED-FOR', 'temporal localization information'\n",
            "\n",
            "Step (3): Triple Formation\n",
            "Most relevant triples based on confidence:\n",
            "- ['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'USED-FOR', 'exploring action type']\n",
            "- ['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'USED-FOR', 'temporal localization information']\n",
            "\n",
            "['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'USED-FOR', 'exploring action type'] \n",
            "['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'USED-FOR', 'temporal localization information'] \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .\n",
            "Gold ----->  ['Head Grammars', 'HYPONYM-OF', 'grammatical formalisms']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'We', 'relationship', 'two grammatical formalisms', 'Tree Adjoining Grammars', 'Head Grammars'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'Tree Adjoining Grammars', 'CONJUNCTION', 'Head Grammars'\n",
            "- 'relationship', 'COMPARE', 'two grammatical formalisms'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['Tree Adjoining Grammars', 'CONJUNCTION', 'Head Grammars']\n",
            "['relationship', 'COMPARE', 'two grammatical formalisms']\n",
            "\n",
            "['Tree Adjoining Grammars', 'CONJUNCTION', 'Head Grammars']                                \n",
            "['relationship', 'COMPARE', 'two grammatical formalisms']                                  \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .\n",
            "Gold ----->  ['dependency grammars', 'HYPONYM-OF', 'grammar formalisms']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "- Entities: 'This formalism', 'elementary', 'powerful', 'many grammar formalisms', 'rewriting systems', 'dependency grammars', 'TAG', 'HPSG', 'LFG'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "- Potential Triples:\n",
            "  - 'This formalism', 'COMPARE', 'many grammar formalisms'\n",
            "  - 'rewriting systems', 'HYPONYM-OF', 'many grammar formalisms'\n",
            "  - 'dependency grammars', 'HYPONYM-OF', 'many grammar formalisms'\n",
            "  - 'TAG', 'HYPONYM-OF', 'many grammar formalisms'\n",
            "  - 'HPSG', 'HYPONYM-OF', 'many grammar formalisms'\n",
            "  - 'LFG', 'HYPONYM-OF', 'many grammar formalisms'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the three most confident triples:\n",
            "- ['This formalism', 'COMPARE', 'many grammar formalisms']\n",
            "- ['rewriting systems', 'HYPONYM-OF', 'many grammar formalisms']\n",
            "- ['dependency grammars', 'HYPONYM-OF', 'many grammar formalisms']\n",
            "\n",
            "['This formalism', 'COMPARE', 'many grammar formalisms']                                   \n",
            "['rewriting systems', 'HYPONYM-OF', 'many grammar formalisms']                             \n",
            "['dependency grammars', 'HYPONYM-OF', 'many grammar formalisms']                           \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We have implemented a restricted domain parser called Plume .\n",
            "Gold ----->  ['Plume', 'HYPONYM-OF', 'restricted domain parser']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: We, restricted domain parser, Plume\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triple:\n",
            "- 'Plume', 'HYPONYM-OF', 'restricted domain parser'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the most confident triple:\n",
            "['Plume', 'HYPONYM-OF', 'restricted domain parser']\n",
            "\n",
            "['Plume', 'HYPONYM-OF', 'restricted domain parser']                                        *** Correct Prediction **** HYPONYM-OF\n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  There are four language pairs currently supported by GLOSSER : English-Bulgarian , English-Estonian , English-Hungarian and French-Dutch .\n",
            "Gold ----->  ['English-Estonian', 'HYPONYM-OF', 'language pairs']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'four language pairs', 'GLOSSER', 'English-Bulgarian', 'English-Estonian', 'English-Hungarian', 'French-Dutch'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'English-Bulgarian', 'PART-OF', 'four language pairs'\n",
            "- 'English-Estonian', 'PART-OF', 'four language pairs'\n",
            "- 'English-Hungarian', 'PART-OF', 'four language pairs'\n",
            "- 'French-Dutch', 'PART-OF', 'four language pairs'\n",
            "- 'four language pairs', 'USED-FOR', 'GLOSSER'\n",
            "\n",
            "Step (3): Triple Formation\n",
            "Priority is given based on the importance and relevance of the entities related. Most confident triples are as follows:\n",
            "['four language pairs', 'USED-FOR', 'GLOSSER']\n",
            "['English-Bulgarian', 'PART-OF', 'four language pairs']\n",
            "['French-Dutch', 'PART-OF', 'four language pairs']\n",
            "\n",
            "['four language pairs', 'USED-FOR', 'GLOSSER']                                             \n",
            "['English-Bulgarian', 'PART-OF', 'four language pairs']                                    \n",
            "['French-Dutch', 'PART-OF', 'four language pairs']                                         \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words .\n",
            "Gold ----->  ['Turkish', 'HYPONYM-OF', 'agglutinative language']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: Turkish, agglutinative language, word structures, productive affixations, derivational suffixes, inflectional suffixes, root words\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'Turkish', 'HYPONYM-OF', 'agglutinative language'\n",
            "- 'productive affixations', 'USED-FOR', 'word structures'\n",
            "- 'derivational suffixes', 'PART-OF', 'productive affixations'\n",
            "- 'inflectional suffixes', 'PART-OF', 'productive affixations'\n",
            "- 'root words', 'EVALUATE-FOR', 'word structures'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the three most confident triples:\n",
            "['Turkish', 'HYPONYM-OF', 'agglutinative language']\n",
            "['productive affixations', 'USED-FOR', 'word structures']\n",
            "['root words', 'EVALUATE-FOR', 'word structures']\n",
            "\n",
            "['Turkish', 'HYPONYM-OF', 'agglutinative language']                                        *** Correct Prediction **** HYPONYM-OF\n",
            "['productive affixations', 'USED-FOR', 'word structures']                                  \n",
            "['root words', 'EVALUATE-FOR', 'word structures']                                          \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Thus , in this paper , we study the problem of robust PCA with side information , where both prior structure and features of entities are exploited for recovery .\n",
            "Gold ----->  ['prior structure', 'CONJUNCTION', 'features of entities']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'this paper', 'we', 'problem of robust PCA with side information', 'prior structure', 'features of entities', 'recovery'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'problem of robust PCA with side information', 'FEATURE-OF', 'this paper'\n",
            "- 'prior structure', 'USED-FOR', 'recovery'\n",
            "- 'features of entities', 'USED-FOR', 'recovery'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the three most confident triples:\n",
            "- ['problem of robust PCA with side information', 'FEATURE-OF', 'this paper']\n",
            "- ['prior structure', 'USED-FOR', 'recovery']\n",
            "- ['features of entities', 'USED-FOR', 'recovery']\n",
            "\n",
            "['problem of robust PCA with side information', 'FEATURE-OF', 'this paper']                \n",
            "['prior structure', 'USED-FOR', 'recovery']                                                \n",
            "['features of entities', 'USED-FOR', 'recovery']                                           \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and reconstruction , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .\n",
            "Gold ----->  ['Multi-view constraints', 'CONJUNCTION', 'normalized representation']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'Multi-view constraints', 'groups of patches', 'normalized representation', 'appearance', 'matching and reconstruction', 'acquisition', 'true three-dimensional affine and Euclidean models', 'multiple images', 'recognition', 'single photograph', 'arbitrary viewpoint'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'Multi-view constraints', 'USED-FOR', 'matching and reconstruction'\n",
            "- 'normalized representation', 'PART-OF', 'appearance'\n",
            "- 'true three-dimensional affine and Euclidean models', 'CONJUNCTION', 'multiple images'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the three most confident triples:\n",
            "['Multi-view constraints', 'USED-FOR', 'matching and reconstruction']\n",
            "['normalized representation', 'PART-OF', 'appearance']\n",
            "['true three-dimensional affine and Euclidean models', 'CONJUNCTION', 'multiple images']\n",
            "\n",
            "['Multi-view constraints', 'USED-FOR', 'matching and reconstruction']                      \n",
            "['normalized representation', 'PART-OF', 'appearance']                                     \n",
            "['true three-dimensional affine and Euclidean models', 'CONJUNCTION', 'multiple images']   \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of Chinese text available in corpora and on the Web .\n",
            "Gold ----->  ['corpora', 'CONJUNCTION', 'Web']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: Our method, different way, word senses, English, Chinese, large amount of Chinese text, corpora, the Web\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'different way', 'USED-FOR', 'word senses'\n",
            "- 'large amount of Chinese text', 'PART-OF', 'corpora and the Web'\n",
            "- 'English', 'CONJUNCTION', 'Chinese'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the three most confident triples:\n",
            "['different way', 'USED-FOR', 'word senses']\n",
            "['large amount of Chinese text', 'PART-OF', 'corpora and the Web']\n",
            "['English', 'CONJUNCTION', 'Chinese']\n",
            "\n",
            "['different way', 'USED-FOR', 'word senses']                                               \n",
            "['large amount of Chinese text', 'PART-OF', 'corpora and the Web']                         \n",
            "['English', 'CONJUNCTION', 'Chinese']                                                      \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Furthermore , this paper presents a novel algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts -LRB- flicker -RRB- ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .\n",
            "Gold ----->  ['cost aggregation algorithm', 'CONJUNCTION', 'algorithm']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: this paper, novel algorithm, temporal maintenance, background model, rendering of occlusions, temporal artefacts, flicker, cost aggregation algorithm, our three-dimensional matching cost space\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'novel algorithm', 'USED-FOR', 'temporal maintenance of background model'\n",
            "- 'novel algorithm', 'FEATURE-OF', 'reduce temporal artefacts'\n",
            "- 'cost aggregation algorithm', 'COMPARE', 'novel algorithm'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['novel algorithm', 'USED-FOR', 'temporal maintenance of background model']\n",
            "['novel algorithm', 'FEATURE-OF', 'reduce temporal artefacts']\n",
            "\n",
            "['novel algorithm', 'USED-FOR', 'temporal maintenance of background model']                \n",
            "['novel algorithm', 'FEATURE-OF', 'reduce temporal artefacts']                             \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , dags , graphs , and products of them .\n",
            "Gold ----->  ['strings', 'CONJUNCTION', 'trees']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'paper', 'generic mathematical formalism', 'combination', 'various structures', 'strings', 'trees', 'dags', 'graphs', 'products of them'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'generic mathematical formalism', 'USED-FOR', 'combination'\n",
            "- 'combination', 'CONJUNCTION', 'various structures'\n",
            "- 'products of them', 'HYPONYM-OF', 'various structures'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['generic mathematical formalism', 'USED-FOR', 'combination']\n",
            "['products of them', 'HYPONYM-OF', 'various structures']\n",
            "\n",
            "['generic mathematical formalism', 'USED-FOR', 'combination']                              \n",
            "['products of them', 'HYPONYM-OF', 'various structures']                                   \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases , referring expressions , and interruptions .\n",
            "Gold ----->  ['referring expressions', 'CONJUNCTION', 'interruptions']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'distinction', 'these components', 'adequate explanation', 'discourse phenomena', 'cue phrases', 'referring expressions', 'interruptions'.\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'distinction', 'FEATURE-OF', 'these components'\n",
            "- 'discourse phenomena', 'PART-OF', 'adequate explanation'\n",
            "- 'cue phrases', 'CONJUNCTION', 'referring expressions'\n",
            "- 'referring expressions', 'CONJUNCTION', 'interruptions'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the three most confident triples:\n",
            "['distinction', 'FEATURE-OF', 'these components']\n",
            "['discourse phenomena', 'PART-OF', 'adequate explanation']\n",
            "['cue phrases', 'CONJUNCTION', 'referring expressions']\n",
            "\n",
            "['distinction', 'FEATURE-OF', 'these components']                                          \n",
            "['discourse phenomena', 'PART-OF', 'adequate explanation']                                 \n",
            "['cue phrases', 'CONJUNCTION', 'referring expressions']                                    \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing -LRB- -LSB- 1 -RSB- -LSB- 2 -RSB- -RRB- .\n",
            "Gold ----->  ['Recognition of proper nouns', 'PART-OF', 'morphological analysis']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'Recognition of proper nouns', 'Japanese text', 'more general problem', 'morphological analysis', 'Japanese text processing'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'Recognition of proper nouns', 'PART-OF', 'morphological analysis'\n",
            "- 'morphological analysis', 'PART-OF', 'Japanese text processing'\n",
            "\n",
            "Step (3): Triple Formation. Prioritizing and selecting the two most confident triples:\n",
            "['Recognition of proper nouns', 'PART-OF', 'morphological analysis']\n",
            "['morphological analysis', 'PART-OF', 'Japanese text processing']\n",
            "\n",
            "['Recognition of proper nouns', 'PART-OF', 'morphological analysis']                       *** Correct Prediction **** PART-OF\n",
            "['morphological analysis', 'PART-OF', 'Japanese text processing']                          \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  In this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances -LRB- called the linguistic structure -RRB- , a structure of purposes -LRB- called the intentional structure -RRB- , and the state of focus of attention -LRB- called the attentional state -RRB- .\n",
            "Gold ----->  ['intentional structure', 'PART-OF', 'components']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'theory', 'discourse structure', 'three separate but interrelated components', 'structure of the sequence of utterances', 'linguistic structure', 'structure of purposes', 'intentional structure', 'state of focus of attention', 'the attentional state'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'discourse structure', 'PART-OF', 'theory'\n",
            "- 'linguistic structure', 'HYPONYM-OF', 'structure of the sequence of utterances'\n",
            "- 'intentional structure', 'HYPONYM-OF', 'structure of purposes'\n",
            "- 'the attentional state', 'HYPONYM-OF', 'state of focus of attention'\n",
            "\n",
            "Step (3): Triple Formation\n",
            "Based on importance and confidence, the three triples are:\n",
            "['discourse structure', 'PART-OF', 'theory']\n",
            "['linguistic structure', 'HYPONYM-OF', 'structure of the sequence of utterances']\n",
            "['intentional structure', 'HYPONYM-OF', 'structure of purposes']\n",
            "\n",
            "['discourse structure', 'PART-OF', 'theory']                                               \n",
            "['linguistic structure', 'HYPONYM-OF', 'structure of the sequence of utterances']          \n",
            "['intentional structure', 'HYPONYM-OF', 'structure of purposes']                           \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We analyzed eye gaze , head nods and attentional focus in the context of a direction-giving task .\n",
            "Gold ----->  ['attentional focus', 'PART-OF', 'direction-giving task']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'eye gaze', 'head nods', 'attentional focus', 'direction-giving task'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- ['eye gaze', 'PART-OF', 'direction-giving task']\n",
            "- ['head nods', 'PART-OF', 'direction-giving task']\n",
            "- ['attentional focus', 'PART-OF', 'direction-giving task']\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the most confident triple:\n",
            "['eye gaze', 'PART-OF', 'direction-giving task'],\n",
            "['head nods', 'PART-OF', 'direction-giving task'],\n",
            "['attentional focus', 'PART-OF', 'direction-giving task']\n",
            "\n",
            "['eye gaze', 'PART-OF', 'direction-giving task']                                           \n",
            "['head nods', 'PART-OF', 'direction-giving task']                                          \n",
            "['attentional focus', 'PART-OF', 'direction-giving task']                                  *** Correct Prediction **** PART-OF\n",
            "['eye gaze', 'PART-OF', 'direction-giving task']                                           \n",
            "['head nods', 'PART-OF', 'direction-giving task']                                          \n",
            "['attentional focus', 'PART-OF', 'direction-giving task']                                  *** Correct Prediction **** PART-OF\n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Amorph recognizes NE items in two stages : dictionary lookup and rule application .\n",
            "Gold ----->  ['dictionary lookup', 'PART-OF', 'Amorph']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: Amorph, NE items, two stages, dictionary lookup, rule application\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'dictionary lookup', 'PART-OF', 'two stages'\n",
            "- 'rule application', 'PART-OF', 'two stages'\n",
            "- 'Amorph', 'USED-FOR', 'NE items'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the most confident triples:\n",
            "['dictionary lookup', 'PART-OF', 'two stages']\n",
            "['rule application', 'PART-OF', 'two stages']\n",
            "['Amorph', 'USED-FOR', 'NE items']\n",
            "\n",
            "['dictionary lookup', 'PART-OF', 'two stages']                                             \n",
            "['rule application', 'PART-OF', 'two stages']                                              \n",
            "['Amorph', 'USED-FOR', 'NE items']                                                         \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  In the Object Recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .\n",
            "Gold ----->  ['estimating object pose', 'PART-OF', 'Object Recognition task']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'Object Recognition task', 'di-chotomy', 'categorization of objects', 'estimating object pose', 'view-invariant representation', 'representation', 'capturing pose information', 'different categories of objects'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential triples:\n",
            "- 'categorization of objects', 'CONJUNCTION', 'estimating object pose'\n",
            "- 'view-invariant representation', 'USED-FOR', 'categorization of objects'\n",
            "- 'representation', 'USED-FOR', 'capturing pose information over different categories of objects'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['categorization of objects', 'CONJUNCTION', 'estimating object pose']\n",
            "['representation', 'USED-FOR', 'capturing pose information over different categories of objects']\n",
            "\n",
            "['categorization of objects', 'CONJUNCTION', 'estimating object pose']                     \n",
            "['representation', 'USED-FOR', 'capturing pose information over different categories of objects'] \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Full digital resolution is maintained even with low-resolution analog-to-digital conversion , owing to random statistics in the analog summation of binary products .\n",
            "Gold ----->  ['random statistics', 'PART-OF', 'analog summation of binary products']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'Full digital resolution', 'low-resolution analog-to-digital conversion', 'random statistics', 'analog summation', 'binary products'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- ['Full digital resolution', 'MAINTAINED-THROUGH', 'low-resolution analog-to-digital conversion']\n",
            "- ['analog summation', 'RESULTS-FROM', 'binary products']\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['Full digital resolution', 'MAINTAINED-THROUGH', 'low-resolution analog-to-digital conversion']\n",
            "['analog summation', 'RESULTS-FROM', 'binary products']\n",
            "\n",
            "['Full digital resolution', 'MAINTAINED-THROUGH', 'low-resolution analog-to-digital conversion'] \n",
            "['analog summation', 'RESULTS-FROM', 'binary products']                                    \n",
            "['Full digital resolution', 'MAINTAINED-THROUGH', 'low-resolution analog-to-digital conversion'] \n",
            "['analog summation', 'RESULTS-FROM', 'binary products']                                    \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches .\n",
            "Gold ----->  ['utterances', 'EVALUATE-FOR', 'trainable components']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: Techniques, automatically training modules, natural language generator, proposed, quality of utterances, trainable components, hand-crafted template-based approaches, rule-based approaches\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'automatically training modules', 'PART-OF', 'natural language generator'\n",
            "- 'quality of utterances', 'COMPARE', 'hand-crafted template-based approaches'\n",
            "- 'quality of utterances', 'COMPARE', 'rule-based approaches'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing, and selecting the two most confident triples:\n",
            "['automatically training modules', 'PART-OF', 'natural language generator']\n",
            "['quality of utterances', 'COMPARE', 'hand-crafted template-based approaches']\n",
            "\n",
            "['automatically training modules', 'PART-OF', 'natural language generator']                \n",
            "['quality of utterances', 'COMPARE', 'hand-crafted template-based approaches']             \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  The experimental results show that the proposed histogram-based interest point detectors perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of repeatability and distinctiveness .\n",
            "Gold ----->  ['repeatability', 'EVALUATE-FOR', 'histogram-based interest point detectors']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'experimental results', 'proposed histogram-based interest point detectors', 'tasks of matching textured scenes', 'blur and illumination changes', 'terms of repeatability and distinctiveness'.\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential triples:\n",
            "- 'proposed histogram-based interest point detectors', 'USED-FOR', 'tasks of matching textured scenes'\n",
            "- 'terms of repeatability and distinctiveness', 'EVALUATE-FOR', 'proposed histogram-based interest point detectors'\n",
            "- 'blur and illumination changes', 'FEATURE-OF', 'tasks of matching textured scenes'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['proposed histogram-based interest point detectors', 'USED-FOR', 'tasks of matching textured scenes']\n",
            "['terms of repeatability and distinctiveness', 'EVALUATE-FOR', 'proposed histogram-based interest point detectors']\n",
            "\n",
            "['proposed histogram-based interest point detectors', 'USED-FOR', 'tasks of matching textured scenes'] \n",
            "['terms of repeatability and distinctiveness', 'EVALUATE-FOR', 'proposed histogram-based interest point detectors'] \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings .\n",
            "Gold ----->  ['summarization quality', 'EVALUATE-FOR', 'automatic parse-based evaluation']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'experimental evaluation', 'summarization quality', 'close correlation', 'automatic parse-based evaluation', 'manual evaluation', 'generated strings'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'experimental evaluation', 'EVALUATE-FOR', 'summarization quality'\n",
            "- 'close correlation', 'COMPARE', 'automatic parse-based evaluation' and 'manual evaluation'\n",
            "- 'automatic parse-based evaluation', 'PART-OF', 'generated strings'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['experimental evaluation', 'EVALUATE-FOR', 'summarization quality']\n",
            "['close correlation', 'COMPARE', 'automatic parse-based evaluation']\n",
            "['automatic parse-based evaluation', 'PART-OF', 'generated strings']\n",
            "\n",
            "['experimental evaluation', 'EVALUATE-FOR', 'summarization quality']                       \n",
            "['close correlation', 'COMPARE', 'automatic parse-based evaluation']                       \n",
            "['automatic parse-based evaluation', 'PART-OF', 'generated strings']                       \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods .\n",
            "Gold ----->  ['ACE corpora', 'EVALUATE-FOR', 'spectral clustering based approach']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'Experiment results', 'ACE corpora', 'spectral clustering based approach', 'other clustering methods'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'Experiment results', 'EVALUATE-FOR', 'ACE corpora'\n",
            "- 'spectral clustering based approach', 'COMPARE', 'other clustering methods'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            " ['Experiment results', 'EVALUATE-FOR', 'ACE corpora']\n",
            " ['spectral clustering based approach', 'COMPARE', 'other clustering methods']\n",
            "\n",
            "['Experiment results', 'EVALUATE-FOR', 'ACE corpora']                                      \n",
            "['spectral clustering based approach', 'COMPARE', 'other clustering methods']              \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  In particular there are three areas of novelty : -LRB- i -RRB- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .\n",
            "Gold ----->  ['robustness', 'EVALUATE-FOR', \"video sequence '' reillumination '' algorithm\"]\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'three areas of novelty', 'photometric model of image formation', 'statistical model of generic face appearance variation', 'extreme illumination changes', 'smoothness of geodesically local appearance manifold structure', 'robust same-identity likelihood', 'unseen head poses', 'accurate video sequence', 'reillumination algorithm', 'face motion patterns in video'.\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- ['photometric model of image formation', 'COMBINED-WITH', 'statistical model of generic face appearance variation']\n",
            "- ['statistical model of generic face appearance variation', 'USED-FOR', 'generalize in the presence of extreme illumination changes']\n",
            "- ['smoothness of geodesically local appearance manifold structure', 'USED-FOR', 'achieve invariance to unseen head poses']\n",
            "- ['reillumination algorithm', 'USED-FOR', 'achieve robustness to face motion patterns in video']\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the three most confident triples:\n",
            "- ['photometric model of image formation', 'COMBINED-WITH', 'statistical model of generic face appearance variation']\n",
            "- ['smoothness of geodesically local appearance manifold structure', 'USED-FOR', 'achieve invariance to unseen head poses']\n",
            "- ['reillumination algorithm', 'USED-FOR', 'achieve robustness to face motion patterns in video']\n",
            "\n",
            "['photometric model of image formation', 'COMBINED-WITH', 'statistical model of generic face appearance variation'] \n",
            "['statistical model of generic face appearance variation', 'USED-FOR', 'generalize in the presence of extreme illumination changes'] \n",
            "['smoothness of geodesically local appearance manifold structure', 'USED-FOR', 'achieve invariance to unseen head poses'] \n",
            "['reillumination algorithm', 'USED-FOR', 'achieve robustness to face motion patterns in video'] \n",
            "['photometric model of image formation', 'COMBINED-WITH', 'statistical model of generic face appearance variation'] \n",
            "['smoothness of geodesically local appearance manifold structure', 'USED-FOR', 'achieve invariance to unseen head poses'] \n",
            "['reillumination algorithm', 'USED-FOR', 'achieve robustness to face motion patterns in video'] \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  The results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our Horn clauses can be learnt automatically from these features .\n",
            "Gold ----->  ['features', 'FEATURE-OF', 'heuristic principles']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'results', 'features', 'heuristic principles', 'predictive power', 'rules', 'Horn clauses'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'features', 'USED-FOR', 'heuristic principles'\n",
            "- 'rules', 'HYPONYM-OF', 'Horn clauses'\n",
            "- 'features', 'EVALUATE-FOR', 'predictive power'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the most confident triples:\n",
            "['features', 'USED-FOR', 'heuristic principles'],\n",
            "['rules', 'HYPONYM-OF', 'Horn clauses'],\n",
            "['features', 'EVALUATE-FOR', 'predictive power']\n",
            "\n",
            "['features', 'USED-FOR', 'heuristic principles']                                           \n",
            "['rules', 'HYPONYM-OF', 'Horn clauses']                                                    \n",
            "['features', 'EVALUATE-FOR', 'predictive power']                                           \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses .\n",
            "Gold ----->  ['probabilistic Horn clauses', 'FEATURE-OF', 'heuristic principles']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'We', 'set of heuristic principles', 'corpus-based sample', 'probabilistic Horn clauses'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- ['set of heuristic principles', 'HYPONYM-OF', 'corpus-based sample']\n",
            "- ['set of heuristic principles', 'USED-FOR', 'probabilistic Horn clauses']\n",
            "\n",
            "Step (3): Triple Formation\n",
            "The most confident triples are:\n",
            "['set of heuristic principles', 'HYPONYM-OF', 'corpus-based sample']\n",
            "['set of heuristic principles', 'USED-FOR', 'probabilistic Horn clauses']\n",
            "\n",
            "['set of heuristic principles', 'HYPONYM-OF', 'corpus-based sample']                       \n",
            "['set of heuristic principles', 'USED-FOR', 'probabilistic Horn clauses']                  \n",
            "['set of heuristic principles', 'HYPONYM-OF', 'corpus-based sample']                       \n",
            "['set of heuristic principles', 'USED-FOR', 'probabilistic Horn clauses']                  \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  A separation method is proposed that is nearly statistically efficient -LRB- approaching the corresponding Cramér-Rao lower bound -RRB- , if the separated signals obey the assumed model .\n",
            "Gold ----->  ['Cramér-Rao lower bound -RRB-', 'FEATURE-OF', 'separation method']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'separation method', 'statistically efficient', 'Cramér-Rao lower bound', 'separated signals', 'assumed model'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'separation method', 'EVALUATE-FOR', 'statistically efficient'\n",
            "- 'statistically efficient', 'COMPARE', 'Cramér-Rao lower bound'\n",
            "- 'separated signals', 'FEATURE-OF', 'assumed model'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['separation method', 'EVALUATE-FOR', 'statistically efficient']\n",
            "['separated signals', 'FEATURE-OF', 'assumed model']\n",
            "\n",
            "['separation method', 'EVALUATE-FOR', 'statistically efficient']                           \n",
            "['separated signals', 'FEATURE-OF', 'assumed model']                                       \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We introduce a method to accelerate the evaluation of object detection cascades with the help of a divide-and-conquer procedure in the space of candidate regions .\n",
            "Gold ----->  ['space of candidate regions', 'FEATURE-OF', 'divide-and-conquer procedure']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'method', 'accelerate', 'evaluation', 'object detection cascades', 'divide-and-conquer procedure', 'space', 'candidate regions' \n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'method', 'USED-FOR', 'accelerate'\n",
            "- 'divide-and-conquer procedure', 'USED-FOR', 'accelerate'\n",
            "- 'candidate regions', 'PART-OF', 'space'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['method', 'USED-FOR', 'accelerate']\n",
            "['divide-and-conquer procedure', 'USED-FOR', 'accelerate']\n",
            "\n",
            "['method', 'USED-FOR', 'accelerate']                                                       \n",
            "['divide-and-conquer procedure', 'USED-FOR', 'accelerate']                                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Experimental results from a real telephone application on a natural number recognition task show an 50 % reduction in recognition errors with a moderate 12 % rejection rate of correct utterances and a low 1.5 % rate of false acceptance .\n",
            "Gold ----->  ['natural number recognition task', 'FEATURE-OF', 'telephone application']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: Experimental results, real telephone application, natural number recognition task, 50 % reduction, recognition errors, moderate 12 % rejection rate, correct utterances, low 1.5 % rate, false acceptance\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- '50 % reduction', 'FEATURE-OF', 'recognition errors'\n",
            "- 'moderate 12 % rejection rate', 'FEATURE-OF', 'correct utterances'\n",
            "- 'low 1.5 % rate', 'FEATURE-OF', 'false acceptance'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the three most confident triples:\n",
            "['50 % reduction', 'FEATURE-OF', 'recognition errors']\n",
            "['moderate 12 % rejection rate', 'FEATURE-OF', 'correct utterances']\n",
            "['low 1.5 % rate', 'FEATURE-OF', 'false acceptance']\n",
            "\n",
            "['50 % reduction', 'FEATURE-OF', 'recognition errors']                                     \n",
            "['moderate 12 % rejection rate', 'FEATURE-OF', 'correct utterances']                       \n",
            "['low 1.5 % rate', 'FEATURE-OF', 'false acceptance']                                       \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Here , we leverage a logistic stick-breaking representation and recent innovations in Pólya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods , enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead .\n",
            "Gold ----->  ['minimal overhead', 'FEATURE-OF', 'Gaussian models']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'we', 'logistic stick-breaking representation', 'recent innovations', 'Pólya-gamma augmentation', 'multinomial distribution', 'latent variables', 'jointly Gaussian likelihoods', 'host of Bayesian inference techniques', 'Gaussian models', 'minimal overhead'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'logistic stick-breaking representation', 'CONJUNCTION', 'Pólya-gamma augmentation'\n",
            "- 'latent variables', 'FEATURE-OF', 'jointly Gaussian likelihoods'\n",
            "- 'host of Bayesian inference techniques', 'USED-FOR', 'Gaussian models'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['logistic stick-breaking representation', 'CONJUNCTION', 'Pólya-gamma augmentation']\n",
            "['latent variables', 'FEATURE-OF', 'jointly Gaussian likelihoods']\n",
            "\n",
            "['logistic stick-breaking representation', 'CONJUNCTION', 'Pólya-gamma augmentation']      \n",
            "['latent variables', 'FEATURE-OF', 'jointly Gaussian likelihoods']                         \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our experiments on real data sets show that the resulting detector is more robust to the choice of training examples , and substantially improves both linear and kernel SVM when trained on 10 positive and 10 negative examples .\n",
            "Gold ----->  ['detector', 'COMPARE', 'linear and kernel SVM']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'Our experiments', 'real data sets', 'resulting detector', 'choice of training examples', 'linear and kernel SVM', '10 positive and 10 negative examples'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'Our experiments', 'EVALUATE-FOR', 'resulting detector'\n",
            "- 'resulting detector', 'COMPARE', 'linear and kernel SVM'\n",
            "- 'choice of training examples', 'FEATURE-OF', 'Our experiments'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['Our experiments', 'EVALUATE-FOR', 'resulting detector']\n",
            "['resulting detector', 'COMPARE', 'linear and kernel SVM']\n",
            "\n",
            "['Our experiments', 'EVALUATE-FOR', 'resulting detector']                                  \n",
            "['resulting detector', 'COMPARE', 'linear and kernel SVM']                                 \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We consider the problem of computing the Kullback-Leibler distance , also called the relative entropy , between a probabilistic context-free grammar and a probabilistic finite automaton .\n",
            "Gold ----->  ['probabilistic context-free grammar', 'COMPARE', 'probabilistic finite automaton']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: We, the problem of computing, Kullback-Leibler distance, relative entropy, probabilistic context-free grammar, probabilistic finite automaton\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'Kullback-Leibler distance', 'COMPARE', 'relative entropy'\n",
            "- 'probabilistic context-free grammar', 'CONJUNCTION', 'probabilistic finite automaton'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['Kullback-Leibler distance', 'COMPARE', 'relative entropy']\n",
            "['probabilistic context-free grammar', 'CONJUNCTION', 'probabilistic finite automaton']\n",
            "\n",
            "['Kullback-Leibler distance', 'COMPARE', 'relative entropy']                               \n",
            "['probabilistic context-free grammar', 'CONJUNCTION', 'probabilistic finite automaton']    \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments .\n",
            "Gold ----->  ['technique', 'COMPARE', 'models']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'Our technique', 'substantial improvement', 'paraphrase classification accuracy', 'other models', 'experiments'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- ['Our technique', 'EVALUATE-FOR', 'paraphrase classification accuracy']\n",
            "- ['substantial improvement', 'COMPARE', 'other models']\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['Our technique', 'EVALUATE-FOR', 'paraphrase classification accuracy']\n",
            "['substantial improvement', 'COMPARE', 'other models']\n",
            "\n",
            "['Our technique', 'EVALUATE-FOR', 'paraphrase classification accuracy']                    \n",
            "['substantial improvement', 'COMPARE', 'other models']                                     \n",
            "['Our technique', 'EVALUATE-FOR', 'paraphrase classification accuracy']                    \n",
            "['substantial improvement', 'COMPARE', 'other models']                                     \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representations .\n",
            "Gold ----->  ['this', 'COMPARE', 'object category representations']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: 'layers of various CNN models', 'object pose information', 'object category representations'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'layers of various CNN models', 'EVALUATE-FOR', 'object pose information'\n",
            "- 'layers of various CNN models', 'EVALUATE-FOR', 'object category representations'\n",
            "- 'object pose information', 'COMPARE', 'object category representations'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
            "['layers of various CNN models', 'EVALUATE-FOR', 'object pose information']\n",
            "['object pose information', 'COMPARE', 'object category representations']\n",
            "\n",
            "['layers of various CNN models', 'EVALUATE-FOR', 'object pose information']                \n",
            "['object pose information', 'COMPARE', 'object category representations']                  \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  We show that the trainable sentence planner performs better than the rule-based systems and the baselines , and as well as the hand-crafted system .\n",
            "Gold ----->  ['trainable sentence planner', 'COMPARE', 'baselines']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "entities = 'trainable sentence planner', 'rule-based systems', 'baselines', 'hand-crafted system'\n",
            "\n",
            "Step (2): Triple Identification\n",
            "potential_triples:\n",
            "- 'trainable sentence planner', 'COMPARE', 'rule-based systems'\n",
            "- 'trainable sentence planner', 'COMPARE', 'baselines'\n",
            "- 'trainable sentence planner', 'COMPARE', 'hand-crafted system'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the three most confident triples:\n",
            "['trainable sentence planner', 'COMPARE', 'rule-based systems']\n",
            "['trainable sentence planner', 'COMPARE', 'baselines']\n",
            "['trainable sentence planner', 'COMPARE', 'hand-crafted system']\n",
            "\n",
            "['trainable sentence planner', 'COMPARE', 'rule-based systems']                            \n",
            "['trainable sentence planner', 'COMPARE', 'baselines']                                     *** Correct Prediction **** COMPARE\n",
            "['trainable sentence planner', 'COMPARE', 'hand-crafted system']                           \n",
            "\n",
            "***\n",
            "\n",
            "Sentence:  Compared to the exhaustive procedure that thus far is the state-of-the-art for cascade evaluation , the proposed method requires fewer evaluations of the classifier functions , thereby speeding up the search .\n",
            "Gold ----->  ['exhaustive procedure', 'COMPARE', 'method']\n",
            "\n",
            "Reasoning --->  Step (1): Entity Extraction\n",
            "Entities: exhaustive procedure, state-of-the-art, cascade evaluation, proposed method, evaluations, classifier functions, search\n",
            "\n",
            "Step (2): Triple Identification\n",
            "Potential Triples:\n",
            "- 'exhaustive procedure', 'COMPARE', 'proposed method'\n",
            "- 'state-of-the-art', 'EVALUATE-FOR', 'cascade evaluation'\n",
            "- 'proposed method', 'FEATURE-OF', 'fewer evaluations'\n",
            "- 'classifier functions', 'PART-OF', 'proposed method'\n",
            "\n",
            "Step (3): Triple Formation, Prioritizing and selecting the most confident triples:\n",
            "['exhaustive procedure', 'COMPARE', 'proposed method']\n",
            "['state-of-the-art', 'EVALUATE-FOR', 'cascade evaluation']\n",
            "['classifier functions', 'PART-OF', 'proposed method']\n",
            "\n",
            "['exhaustive procedure', 'COMPARE', 'proposed method']                                     \n",
            "['state-of-the-art', 'EVALUATE-FOR', 'cascade evaluation']                                 \n",
            "['classifier functions', 'PART-OF', 'proposed method']                                     \n",
            "\n",
            "***\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uzWvcS2Q4Czu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cA4-JbQ04C2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1UJBSBbX4C5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E5O16xcE4C7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phase_three_cot_prompt = '''\n",
        "\n",
        "Given a sentence, identify and extract relevant triples of entities connected by a predicate from the provided list.\n",
        "The format of each triple should be [head, predicate, tail], separated by commas.\n",
        "\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Step (1): Entity Extraction\n",
        "- Extract the entities from the given sentence.\n",
        "- Group sequences of words that indicate the same role in the sentence as a single entity.\n",
        "- Exclude adjectives from entities to focus on core nouns.\n",
        "\n",
        "Step (2): Triple Identification\n",
        "- Identify a single relevant predicate from the given list that best describes the relationship between the entities. List of predicates: {}.\n",
        "- Ensure that the order of subject, predicate, and object is maintained based on the predicate's meaning.\n",
        "- Consider the semantic relationship between entities when selecting a predicate.\n",
        "\n",
        "Step (3): Triple Formation\n",
        "- Construct triples in the format [head, predicate, tail].\n",
        "- Prioritize the triples based on your confidence in their relevance and accuracy.\n",
        "- Return at most two of the most confident triples. If there is only one confident reiple, only return that.\n",
        "\n",
        "\n",
        "\n",
        "Example 1:\n",
        "Given Sentence: \"This paper solves a specialized regression problem to obtain sampling probabilities for records in databases.\"\n",
        "\n",
        "Step (1):\n",
        "Entities: 'paper', 'regression problem', 'sampling probabilities', 'records', 'databases'\n",
        "\n",
        "Step (2):\n",
        "Potential Triples:\n",
        "- 'regression problem', 'USED-FOR', 'sampling probabilities'\n",
        "- 'sampling probabilities', 'USED-FOR', 'records'\n",
        "- 'records', 'PART-OF', 'databases'\n",
        "\n",
        "\n",
        "step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
        "['method', 'USED-FOR', 'discovering parallel sentences']\n",
        "['sampling probabilities', 'USED-FOR', 'records']\n",
        "\n",
        "\n",
        "\n",
        "Example 2:\n",
        "Given Sentence: = \"With the aid of a logic-based grammar formalism called extraposition grammars, Chat-80 translates English questions into the Prolog subset of logic.\"\n",
        "\n",
        "Step (1): Entity Extraction\n",
        "entities = 'extraposition grammars', 'logic-based grammar formalism', 'English questions', 'Prolog subset', 'logic'\n",
        "\n",
        "Step (2): Triple Identification\n",
        "potential_triples:\n",
        "- 'extraposition grammars', 'HYPONYM-OF', 'logic-based grammar formalism'\n",
        "- 'Prolog subset', 'PART-OF', 'logic'\n",
        "\n",
        "Step (3): Triple Formation, Prioritizing and selecting the two most confident triples:\n",
        "['extraposition grammars', 'HYPONYM-OF', 'logic-based grammar formalism']\n",
        "['Prolog subset', 'PART-OF', 'logic']\n",
        "\n",
        "\n",
        "The given sentence is : {}\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "IDhxju704C_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8C-fsgXB4DCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra code for checking"
      ],
      "metadata": {
        "id": "2DIQuVSo3_hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TE_metric_calculation(labels, predictions, event_labels):\n",
        "\n",
        "  # when predict an event which is not in gournd truth, we had wrong prediction which decreases our P.\n",
        "  # when we did not predict any event and there is an even in groundtruth, it deacreses our R.\n",
        "  # Thats why we put zeor both in ground truth and pred list to handle this inconsitency of number of extracted events.\n",
        "  micro_p = precision_score(labels,predictions, average='micro')*100.0\n",
        "  micro_r = recall_score(labels,predictions, labels= event_labels, average='micro')*100.0\n",
        "  micro_f1 = (2 * micro_p * micro_r)/(micro_p + micro_r)\n",
        "  # f1_score(labels,predictions, average='micro')*100.0\n",
        "\n",
        "\n",
        "  print(\"Micro_F1:\",micro_f1)\n",
        "  print(\"Micro_Precision:\",micro_p)\n",
        "  print(\"Micro_Recall:\",micro_r)"
      ],
      "metadata": {
        "id": "2CJzbEnN4BBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['a', 'b', 'b', 'c', 'c', 'c', 'd']"
      ],
      "metadata": {
        "id": "u_6PEFnM4CXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['a', 'b', 'c', 'd']\n",
        "pred = ['a', 'f', 'f', 'f']"
      ],
      "metadata": {
        "id": "qMkVoVAI4L81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TE_metric_calculation(labels, pred, ['a', 'b', 'c', 'd'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ2Zquwe4Td5",
        "outputId": "93061271-ff59-4af4-8ce0-94f3379d5a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 25.0\n",
            "Micro_Precision: 25.0\n",
            "Micro_Recall: 25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['a','k','k','w', 'b', 'c', 'd']\n",
        "pred = ['a','g','g','z', 'f', 'f', 'f']"
      ],
      "metadata": {
        "id": "AHhv44HR5Qrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TE_metric_calculation(labels, pred,['a', 'b', 'c', 'd'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjpZ3GE742pc",
        "outputId": "f06b2de3-affe-493d-f672-1cdb6f06dc73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro_F1: 18.18181818181818\n",
            "Micro_Precision: 14.285714285714285\n",
            "Micro_Recall: 25.0\n"
          ]
        }
      ]
    }
  ]
}